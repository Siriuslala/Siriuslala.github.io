<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.147.6"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Siriuslala's Blog!</title><meta name=keywords content="Blog,Portfolio,PaperMod"><meta name=description content="ExampleSite description"><meta name=author content="Me"><link rel=canonical href=https://Siriuslala.github.io/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.77204cabb0aad976cb8e435a19c859198b86d7206bfe4216c7b47d84f771a685.css integrity="sha256-dyBMq7Cq2XbLjkNaGchZGYuG1yBr/kIWx7R9hPdxpoU=" rel="preload stylesheet" as=style><link rel=icon href=https://Siriuslala.github.io/pig.svg><link rel=icon type=image/png sizes=16x16 href=https://Siriuslala.github.io/pig.svg><link rel=icon type=image/png sizes=32x32 href=https://Siriuslala.github.io/pig.svg><link rel=apple-touch-icon href=https://Siriuslala.github.io/pig.svg><link rel=mask-icon href=https://Siriuslala.github.io/pig.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://Siriuslala.github.io/index.xml><link rel=alternate type=application/json href=https://Siriuslala.github.io/index.json><link rel=alternate hreflang=en href=https://Siriuslala.github.io/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}.formula{width:100%;overflow-x:auto}</style><meta property="og:title" content="Siriuslala's Blog!"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://Siriuslala.github.io/"><meta property="og:image" content="https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Siriuslala's Blog!"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Siriuslala's Blog!","url":"https://Siriuslala.github.io/","description":"ExampleSite description","thumbnailUrl":"https://Siriuslala.github.io/pig.svg","sameAs":["https://github.com/Siriuslala","https://x.com/Siriuslala007","https://www.douban.com/people/150419254/?_i=9191575fWMDhmD","https://space.bilibili.com/354740423?spm_id_from=333.1007.0.0","https://steamcommunity.com/profiles/76561199087827194/"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Siriuslala.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://Siriuslala.github.io/pig.svg alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Siriuslala.github.io/ title=Posts><span class=active>Posts</span></a></li><li><a href=https://Siriuslala.github.io/about/ title=About><span>About</span></a></li><li><a href=https://Siriuslala.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://Siriuslala.github.io/faq/ title=FAQ><span>FAQ</span></a></li><li><a href=https://Siriuslala.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://Siriuslala.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class="first-entry home-info"><header class=entry-header><h1>Welcome to Siriuslala&rsquo;s Blog! ğŸ‘‹</h1></header><div class=entry-content><div style=text-align:center><b><em>It is the responsibility of intellectuals to speak the truth and expose lies.</b></em></div><div style=text-align:right><b><em>-- Noam Chomsky</b></em></div><div style=text-align:justify>Hi, I'm Yueyan Li, just call me Sirius! I am a reseacher on machine learning, deep learning and anything cool with that. My interest focuses on <b>interpretability and its applications</b>, <b>unified generation and understanding</b>, <b>cognitive science</b> and <b>linguistics</b>. The blog is a place where I share my thoughts, ideas and some cool stuff I found. Hope you'll enjoy it!</div><div style=text-align:justify>- Publications can be found <a href=/posts/publications style=color:blue>here</a>.</div><div style=text-align:justify>- Details about my basic information can be found in the <a href=/about style=color:blue>About</a> page.</div><div style=text-align:justify>If you have any questions or suggestions about my blogs or simply want for a communication, feel free to comment below my blogs or contact me via <a href=mailto:almightygod007@163.com>almightygod007@163.com</a>.</div></div><footer class=entry-footer><div class=social-icons><a href=https://github.com/Siriuslala target=_blank rel="noopener noreferrer me" title=Github><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a href=https://x.com/Siriuslala007 target=_blank rel="noopener noreferrer me" title=X><svg viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"/></svg>
</a><a href="https://www.douban.com/people/150419254/?_i=9191575fWMDhmD" target=_blank rel="noopener noreferrer me" title=Douban><svg viewBox="0 0 24 24" fill="currentColor" stroke="none" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="m2.42288 2.08086h19.15349v2.15751H2.42288V2.08086z"/><path d="m19.88879 14.92347V6.48903H4.06528v8.43444h15.82351zM6.3599 8.64505h11.25739v4.1235H6.3599v-4.1235z"/><path d="m16.48864 19.78508c.6885-1.05398 1.33827-2.27636 1.94031-3.66377l-2.30206-.83906c-.59872 1.64418-1.29579 3.14745-2.08899 4.50283h-4.00578c-.66389-1.75663-1.41312-3.25884-2.25363-4.50283l-2.11727.83906c.87327 1.30991 1.57742 2.52932 2.11727 3.66377H1.88116v2.13406h20.23769v-2.13406h-5.63021z"/></svg>
</a><a href="https://space.bilibili.com/354740423?spm_id_from=333.1007.0.0" target=_blank rel="noopener noreferrer me" title=Bilibili><svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"><rect x="1.3333" y="6" width="21.333" height="15.333" rx="4" ry="4"/><path d="m8 12.4v1.2"/><path d="m16 12.4v1.2"/><path d="m5.8853 2.6667L8.552 5.3334"/><path d="m18.115 2.6667-2.6667 2.6667"/></svg>
</a><a href=https://steamcommunity.com/profiles/76561199087827194/ target=_blank rel="noopener noreferrer me" title=Steam><svg viewBox="0 0 24 24" fill="currentColor" stroke="none"><path d="M-24.6 20.8c-1.4-.8-2.7-1.5-4.1-2.3-2.4-1.5-3.6-3.6-3.3-6.5.3-3 2.8-5.2 5.8-5.3h11c2.6.0 4.6 2.2 4.7 4.7.0 2.6-2 4.8-4.6 4.9h-2.8v.2c.2.2.5.3.7.5 1.2.7 2.4 1.3 3.6 2 2.5 1.5 3.7 4.5 2.9 7.2s-3.1 4.3-6.1 4.4h-10.6c-2.4.0-4.3-1.9-4.6-4.3-.3-2.3 1.2-4.5 3.5-5.1.6-.1 1.2-.1 1.7-.2h2.1c.1.0.1-.1.1-.2zm4.3-3.8c-.1.3-.2.3-.2.4v3.9c0 1.1-.1 1.2-1.2 1.2h-5.1c-.4.0-.9.0-1.3.1-1.6.4-2.5 1.9-2.4 3.6.2 1.6 1.6 2.9 3.3 2.9h10.5c2.1.0 3.8-1.2 4.5-3.1.7-1.8.2-4.1-1.5-5.3-2.1-1.3-4.3-2.4-6.6-3.7zm-1.7 3.4v-4c0-1.4.1-1.5 1.4-1.5h5.3c1.5.0 2.7-1 3.1-2.4.6-2.2-1-4.2-3.4-4.3h-10.1c-2.4.0-4.3 1.3-4.9 3.4-.6 2.1.4 4.4 2.5 5.5 1.7 1 3.5 1.9 5.2 2.9.4.2.6.3.9.4z"/><path d="M53.3 6.9c-.2-1-1-1.7-1.9-2-1.7-.4-8.6-.4-8.6-.4s-6.9.0-8.6.5c-1 .3-1.7 1-1.9 2-.3 1.7-.5 3.5-.5 5.3s.1 3.6.5 5.3c.3.9 1 1.7 1.9 1.9 1.7.5 8.6.5 8.6.5s6.9.0 8.6-.5c1-.3 1.7-1 1.9-2 .3-1.7.5-3.5.5-5.3s-.1-3.6-.5-5.3z"/><path d="m40.6 15.5 5.7-3.3-5.7-3.3z"/><path d="M72.4-9.9c5.5.0 10 4 10 8.9.0 1.8-.8 3.8-2.1 5.4-.9 1-1.5 2.3-1.6 3.7.0-.1-.1-.1-.2-.2-.4-.4-1.1-.7-1.7-.7-.3.0-.5.0-.8.1-1.2.5-2.5.7-3.6.7-5.5.0-10-4-10-8.9s4.5-9 10-9m0-2c-6.6.0-12 4.9-12 10.9s5.4 11 12 11c1.4.0 2.8-.2 4.2-.7h.1c.1.0.2.0.3.1 1 1.3 2.5 2.3 4.2 2.7l.2-.1v-.3c-.7-.9-1-1.9-1-3s.4-2.1 1.2-2.9c1.5-1.8 2.6-4.2 2.6-6.7.1-6.1-5.3-11-11.8-11z"/><path d="M72.3-6.5c.1.0.3.1.3.2l1.2 3.4 3.7.1c.1.0.3.1.3.2s0 .3-.1.4l-3 2.2 1.1 3.5c0 .1.0.3-.1.4h-.4l-3-2.1-3 2.1h-.4c-.1-.1-.2-.2-.1-.4L69.9.0l-3-2.2c-.1-.1-.2-.2-.1-.4.0-.1.2-.2.3-.2l3.7-.1L72-6.3c0-.1.2-.2.3-.2zM46.8-20.8c2 0 4 .6 5.6 1.6-.5-.1-1.1-.2-1.6-.2-3.2.0-5.8 2.5-6 5.6l-2.2 3.2c-.5.1-.9.2-1.4.4l-4.7-1.9c.8-5 5.2-8.7 10.3-8.7m9.9 7.2c.3 1 .5 2.1.5 3.3C57.2-4.5 52.5.2 46.7.2c-1.8.0-3.6-.5-5.1-1.3.5.2 1 .2 1.5.2 2.5.0 4.5-1.9 4.8-4.3L51-7.4c3.1-.2 5.6-2.8 5.6-6 .1-.1.1-.1.1-.2M38.3-4.2l.3.2c.1.2.2.5.3.7l-.6-.9m8.5-18.1c-6.3.0-11.5 4.9-12 11l6.4 2.7c.5-.4 1.2-.6 1.9-.6h.2l2.9-4.1v-.1c0-2.5 2-4.5 4.5-4.5s4.5 2 4.5 4.5-2 4.5-4.5 4.5h-.1L46.5-6v.2c0 1.9-1.5 3.4-3.4 3.4-1.6.0-3-1.2-3.3-2.7L35.2-7c1.4 5 6 8.7 11.5 8.7 6.6.0 12-5.4 12-12s-5.3-12-11.9-12z"/><path d="M42.6-4.6 41.5-5c.2.4.5.7 1 .9 1 .4 2.1-.1 2.5-1 .2-.5.2-1 0-1.4-.2-.5-.6-.8-1-1-.5-.2-1-.2-1.4.0l1.1.5c.7.3 1 1.1.7 1.8-.3.6-1.1.9-1.8.6zm8.1-11.5c-1.5.0-2.7 1.2-2.7 2.7s1.2 2.7 2.7 2.7 2.7-1.2 2.7-2.7-1.2-2.7-2.7-2.7zm0 4.7c-1.1.0-2-.9-2-2s.9-2 2-2 2 .9 2 2c.1 1.1-.9 2-2 2zM12 0C5.4.0.0 5.4.0 12s5.4 12 12 12 12-5.4 12-12S18.6.0 12 0zm0 22c-4.3.0-8-2.7-9.4-6.5L6.2 17c.3 1.3 1.5 2.4 2.9 2.4 1.6.0 2.9-1.3 2.9-2.9v-.1l3.5-2.5h.1c2.1.0 3.9-1.8 3.9-3.9s-1.8-3.9-3.9-3.9c-2.2.0-3.9 1.8-3.9 3.9v.1l-2.5 3.6H9c-.6.0-1.2.2-1.7.5L2 11.7C2.2 6.3 6.6 2 12 2c5.5.0 10 4.5 10 10s-4.5 10-10 10zm-2.4-7.1-1.3-.5c.5-.2 1.1-.2 1.6.0s1 .7 1.2 1.2.2 1.1.0 1.7c-.5 1.1-1.8 1.7-2.9 1.2-.5-.2-.9-.6-1.1-1.1l1.3.5c.2.0.4.1.6.1.6.0 1.2-.4 1.5-1 .4-.9.0-1.8-.9-2.1zM13 9.8c0-1.5 1.2-2.6 2.6-2.6 1.5.0 2.6 1.2 2.6 2.6.0 1.5-1.2 2.6-2.6 2.6S13 11.2 13 9.8z"/><path d="M13.7 9.8c0-1.1.9-2 2-2s2 .9 2 2-.9 2-2 2c-1.1-.1-2-.9-2-2z"/></svg></a></div></footer></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>A Collection of My Publications</h2></header><div class=entry-content><p>* Equal contribution âœ‰ Corresponding author ICLR 2026 Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models Yueyan Li*, Chenggong Zhao, Zeyuan Zhang, Caixia Yuan, Xiaojie Wangâœ‰ International Conference on Learning Representations (ICLR), 2026 PDF Code arxiv Sparse Model Diffing via Dynamic Circuits Yueyan Li*, Wenhao Gap, Caixia Yuan, Xiaojie Wangâœ‰ ArXiv, 2026 PDF Code Technical Report AutoGLM: Autonomous Foundation Agents for GUIs Team AutoGLM Arxiv, 2024 PDF Code EMNLP 2024 ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline Yifan Xu*, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Zhao Wenyi, Jie Tang, Yuxiao Dongâœ‰ Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024 PDF Code Technical Report ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools Team GLM Arxiv, 2024 PDF Code arxiv EntroKV: An Entropy-aware Memory Manager for KV cache Compression Wenhao Gao*, Haoran Cao, Yueyan Li, Caixia Yuan, Xiaojie Wangâœ‰ ArXiv, 2026 Code</p></div><footer class=entry-footer><span title='2026-01-23 21:21:39 +0800 CST'>January 23, 2026</span>&nbsp;Â·&nbsp;1 min&nbsp;Â·&nbsp;164 words&nbsp;Â·&nbsp;Sirius</footer><a class=entry-link aria-label="post link to A Collection of My Publications" href=https://Siriuslala.github.io/posts/publications/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Thinking and Reasoning</h2></header><div class=entry-content><p>The Purpose I Write This Blog Thinking models are crazily popualr nowadays. The first time I delved in this area was in September, 2023. Later I gradually forgetted this area, until Deepseek came to life. I want to keep to collect information about LLM reasoning and share my thoughts here.
Thinking Models text-based explicit reasoning DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning Kimi k1.5: Scaling Reinforcement Learning with LLMs GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models Skywork Open Reasoner 1 Technical Report implicit reasoning (Coconut) Training Large Language Models to Reason in a Continuous Latent Space others ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline blogs è‡ªé¡¶å‘ä¸‹æ–¹å¼æ·±åº¦è§£è¯» DeepSeek-R1ï¼Œå†…å«å¤§é‡ç»†èŠ‚ MLA(1)ï¼šä»ä»£ç è§’åº¦å­¦ä¹ å’Œå½»åº•ç†è§£ DeepSeek MLA ç®—æ³• ä»å¤´ç†è§£æ€è€ƒæ¨¡å‹ï¼ˆLLM based Reasoning Modelï¼‰ï¼ŒO1ï¼ŒDeepSeek R1ï¼ŒKimi K1.5 overthinking survey Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models (repo) Awesome-Efficient-Reasoning-LLMs papers Qwen3 Technical Report AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning AdaptThink: Reasoning Models Can Learn When to Think blogs è‡ªé€‚åº”å¿«æ…¢æ€è€ƒæ¨ç†æ¨¡å‹ï¼ˆAdaptive Reasoning Modelï¼‰ï¼šQwen3æ··åˆæ€è€ƒ->å­—èŠ‚AdaCoT->æ¸…åAdaptThinking parallel thinking Deep Think with Confidence visual reasoning survey Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers papers $V^{*}$: Guided Visual Search as a Core Mechanism in Multimodal LLMs active perception DeepEyes: Incentivizing â€œThinking with Imagesâ€ via Reinforcement Learning Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL GRIT: Teaching MLLMs to Think with Images tool use VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models imagination Thinking with Generated Images Visual Planning: Letâ€™s Think Only with Images blogs Thinking with Images å°ç»“ others [è’™ç‰¹å¡æ´›æœç´¢æ ‘] MCT Self-Refine (MCTSr)çš„ç®—æ³•ï¼ˆåŒ…å«ä»£ç ç†è§£ï¼‰ èŠèŠæ¨ç†æ¨¡å‹ä¸­çš„PRMsä¸MCTS Evaluation dataset Analyses implicit reasoning Emergence of Superposition: Unveiling the Training Dynamics of Chain of Continuous Thought interpretability How Reinforcement Learning After Next-Token Prediction Facilitates Learning Base Models Know How to Reason, Thinking Models Learn When Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning Thought Anchors: Which LLM Reasoning Steps Matter? Understanding Reasoning in Thinking Language Models via Steering Vectors Chain-of-Thought Is Not Explainability Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization How Do LLMs Perform Two-Hop Reasoning in Context? theories Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought Reinforcement Learning RL algorithms (GAE) High-Dimensional Continuous Control Using Generalized Advantage Estimation (DPO) Direct preference optimization: Your language model is secretly a reward model From r to qâˆ—: Your language model is secretly a q-function DPOæ–°ä½œYour Language Model is Secretly a Q-Functionè§£è¯»ï¼Œä¸OPENAI Q* çš„è”ç³»ï¼Ÿ (PPO) Proximal Policy Optimization Algorithms (REINFORCE++) REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt and Reward Models (GRPO) DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (DAPO) DAPO: An Open-Source LLM Reinforcement Learning System at Scale (GSPO) Group Sequence Policy Optimization (Cispo) MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention Blogs algorithms äººäººéƒ½èƒ½çœ‹æ‡‚çš„RL-PPOç†è®ºçŸ¥è¯† Reasoning LLMï¼ˆä¸‰ï¼‰ï¼šLLM+RL RLHF å¸¸è§çš„æ€ç»´è¯¯åŒº reward modeling text (PRM) Letâ€™s verify step by step (POLAR) Pre-Trained Policy Discriminators are General Reward Models reward model for generative models Improving Video Generation with Human Feedback VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation Black-Box Prompt Optimization: Aligning Large Language Models without Model Training analyses RL training Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning entropy Reasoning LLMï¼ˆäº”ï¼‰ï¼šç†µç¼©è¿‡ç¨‹ä¸èƒ½åŠ›è¾¹ç•Œ LLMxRLã€‘ç†µåç¼©ä¸ç¼“è§£ç­–ç•¥ (clip/kl-cov) The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models (forking tokens) Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning RL v.s. SFT Sft memorizes, rl generalizes: A comparative study of foundation model post-training RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs 3.2 ç»Ÿä¸€è§†è§’ç†è§£ä» SFT åˆ° RL All Roads Lead to Likelihood: The Value of Reinforcement Learning in Fine-Tuning Generalist Reward Models: Found Inside Large Language Models (DFT) On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification ä» SFT åˆ° RLï¼šä¸€æ­¥æ­¥çœ‹æ¸…å®ƒä»¬çš„è”ç³» (NFT) Bridging Supervised Learning and Reinforcement Learning in Math Reasoning Resource RL infra (verl) HybridFlow: A Flexible and Efficient RLHF Framework doc repo slime RL Scaling æ—¶ä»£ï¼Œæˆ‘ä»¬éœ€è¦ä»€ä¹ˆæ ·çš„ RL æ¡†æ¶å‘¢ï¼Ÿ blogs training æµ…èŠRLæ¡†æ¶çš„å‹ƒå‹ƒç”Ÿæœºã€ä¸‡ç‰©ç«å‘ How we built our multi-agent research system verl [AI Infra] VeRL æ¡†æ¶å…¥é—¨&ä»£ç å¸¦è¯» ä»é›¶å¼€å§‹çš„verlæ¡†æ¶è§£æ verl RLæ”¯æŒè®­ç»ƒdeepseek-v3 671Bå®ä¹ å¤ç›˜(ä¸ªäººç‰ˆ) OpenRLHF&amp;Verlå‚æ•°è½¬æ¢æŒ‡å— verlå°ç™½è§£è¯» ä¸€æ–‡æ·±åº¦å…¨é¢è§£æå¤§æ¨¡å‹åˆ†å¸ƒå¼å¹¶è¡Œç­–ç•¥ï¼šDP/TP/PP/CP/EP/SP æ·±å…¥ç†è§£ Megatron-LMï¼ˆ2ï¼‰åŸç†ä»‹ç» DeepSpeed zero1ï¼Œzero2ï¼Œzero3å’ŒFSDPåŒºåˆ«è¯¦è§£ inference SGLangï¼šLLMæ¨ç†å¼•æ“å‘å±•æ–°æ–¹å‘ å›¾è§£å¤§æ¨¡å‹è®¡ç®—åŠ é€Ÿç³»åˆ—ï¼šFlashAttention V1ï¼Œä»ç¡¬ä»¶åˆ°è®¡ç®—é€»è¾‘</p></div><footer class=entry-footer><span title='2025-06-30 10:50:00 +0800 CST'>June 30, 2025</span>&nbsp;Â·&nbsp;3 min&nbsp;Â·&nbsp;1310 words&nbsp;Â·&nbsp;Sirius</footer><a class=entry-link aria-label="post link to Thinking and Reasoning" href=https://Siriuslala.github.io/posts/thinking_and_reasoning/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>LLM Agents</h2></header><div class=entry-content><p>The Purpose I Write This Blog LLM-based agent is gonna change the world. Amazing agent systems have been created to change our life. Since I was once in a team that aimed to build advanced agents for the control of digital devices and for which I was impressed, I want to keep to collect information about LLM agents and share my thoughts here.
Resource GUI Agents survey Large Language Model-Brained GUI Agents: A Survey GUI Agentç»¼è¿° : æ­ç§˜GUIæ™ºèƒ½ä½“çš„å‰ä¸–ä»Šç”Ÿ-1 : æ€»è§ˆç¯‡-å¯ç¨‹ models autoglm ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents MobileRL: Advancing Mobile Use Agents With Adaptive Online Reinforcement Learning ANDROIDGEN: Building an Android Language Agent under Data Scarcity Autoglm: Autonomous foundation agents for guis WebRL:Training llm web agents via self-evolving online curriculum reinforcement learning AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents others DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents Appagent: Multimodal agents as smartphone users (SeeAct) GPT-4V(ision) is a Generalist Web Agent, if Grounded Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V benchmarks web WebArena: A Realistic Web Environment for Building Autonomous Agents Mind2web: Towards a generalist agent for the web Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments (MiniWob) World of Bits: An Open-Domain Platform for Web-Based Agents android Android in the Wild: A Large-Scale Dataset for Android Device Control (AndroidArena) Understanding the weakness of large language model agents within a complex android environment DeepResearch survey Deep Research Agents: A Systematic Examination And Roadmap Towards AI Search Paradigm models Search-o1: Agentic search-enhanced large reasoning models Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning R1-searcher: Incentivizing the search capability in llms via reinforcement learning repo (Jina) node-DeepResearch Public Kimi-Researcher: End-to-End RL Training for Emerging Agentic Capabilities Language Modeling by Language Models Agentic RL papers
...</p></div><footer class=entry-footer><span title='2025-06-28 13:01:09 +0800 CST'>June 28, 2025</span>&nbsp;Â·&nbsp;1 min&nbsp;Â·&nbsp;364 words&nbsp;Â·&nbsp;Sirius</footer><a class=entry-link aria-label="post link to LLM Agents" href=https://Siriuslala.github.io/posts/llm-agent/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Interpretability (& other areas) for Multimodal Models</h2></header><div class=entry-content><p>ğŸ’¡ This post isÂ initially focused on interpretability for multimodal models, while later a lot of papers in other fields are included, just for convenience.
Resource Interpretability for MLLMs survey A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models Sparks of Explainability Recent Advancements in Explaining Large Vision Models Awesome LMMs Mechanistic Interpretability probing Probing Multimodal Large Language Models for Global and Local Semantic Representations representation Zoom in: An introduction to circuits Multimodal Neurons in Artificial Neural Networks Interpreting CLIPâ€™s Image Representation via Text-Based Decomposition Interpreting the Second-Order Effects of Neurons in CLIP CLIPä¸åŒå±‚ Multimodal Neurons in Pretrained Text-Only Transformers Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers? circuit **(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models Automatic Discovery of Visual Circuits Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP SAE Case study: Interpreting, manipulating, and controlling clip with sparse autoencoders Towards multimodal interpretability: Learning sparse interpretable features in vision transformers Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery visualization Visualizerï¼ç®€åŒ–ä½ çš„Vision Transformerå¯è§†åŒ–ï¼ (DVT) Denoising Vision Transformers Token Activation Map to Visually Explain Multimodal LLMs LVLM-Intrepret: An Interpretability Tool for Large Vision Language Models Transformer Interpretability Beyond Attention Visualization others **Towards interpreting visual information processing in vision-language models demo (dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability & Open Problems Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space Skip-It? Theoretical Conditions for Layer Skipping in Vision-Language Models tools VLM-Lens information flow **Cross-modal Information Flow in Multimodal Large Language Models *From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks *Whatâ€™s in the Image? A Deep-Dive into the Vision of Vision Language Models The Narrow Gate: Localized Image-Text Communication in Vision-Language Models Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference analyses on MLLMs Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training Lost in Embeddings: Information Loss in Visionâ€“Language Models Words or Vision: Do Vision-Language Models Have Blind Faith in Text? Forgotten Polygons: Multimodal Large Language Models are Shape-Blind Vision Transformers Need Registers On the rankability of visual embeddings Other fields of MLLMs visual pretraining
...</p></div><footer class=entry-footer><span title='2025-02-25 15:08:53 +0800 CST'>February 25, 2025</span>&nbsp;Â·&nbsp;8 min&nbsp;Â·&nbsp;3547 words&nbsp;Â·&nbsp;Sirius</footer><a class=entry-link aria-label="post link to Interpretability (& other areas) for Multimodal Models" href=https://Siriuslala.github.io/posts/mm_interp/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Interview Experience</h2></header><div class=entry-content><p>å›½å†… æ™ºè°±AI (AIé™¢)ï¼ˆæ—¥å¸¸å®ä¹ ï¼‰ æ²¡æœ‰ä»£è¡¨æ€§ã€‚
2023å¹´10æœˆä¸­æ—¬å‚åŠ çš„é¢è¯•ã€‚å½“æ—¶ GPT-4 åˆšåˆšå‘å¸ƒåŠå¹´ï¼Œæ™ºè°±çš„ ChatGLM åˆšåˆšç»“æŸ 3.0 ç‰ˆæœ¬çš„è®­ç»ƒã€‚å½“æ—¶å…¬å¸ç®—æ³•å›¢é˜Ÿè¿˜åœ¨èµ›å°”å¤§å¦çš„è€å·¥åŒºé‡Œã€‚
...</p></div><footer class=entry-footer><span title='2024-10-13 12:06:08 +0800 CST'>October 13, 2024</span>&nbsp;Â·&nbsp;6 min&nbsp;Â·&nbsp;2658 words&nbsp;Â·&nbsp;Me</footer><a class=entry-link aria-label="post link to Interview Experience" href=https://Siriuslala.github.io/posts/%E9%9D%A2%E7%BB%8F/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>ä¸€äº›è¯­è¨€å­¦çš„æ¢—å’Œæœ‰æ„æ€çš„çŸ¥è¯†</h2></header><div class=entry-content><p>This post is written in Chinese. If you donâ€™t know Chinese, you can learn it lol. (Sorry for this because simply translating the post into English may not be enough for you to understand).
è¯­è¨€å­¦ä¹å­ çš®é’¦è¯­ (pidgin) å¤§å®¶å¯¹é‚£äº› 1.è¨€è¯­ä¸­ä¸æ—¶å¤¹æ‚ç€è‹±æ–‡å•è¯ 2.è£…/å‡¡å°”èµ› çš„äººè¡¨ç°å‡ºä¸€ç§åŒæ¶ã€‚ä¾‹å¦‚ï¼Œä¸‹é¢æ˜¯æŸæ‹ç»¼é‡Œçš„ä¸€æ®µç•™å­å¯¹è¯çš„ååœºé¢ï¼š
...</p></div><footer class=entry-footer><span title='2024-09-27 17:15:10 +0800 CST'>September 27, 2024</span>&nbsp;Â·&nbsp;3 min&nbsp;Â·&nbsp;1381 words&nbsp;Â·&nbsp;Sirius</footer><a class=entry-link aria-label="post link to ä¸€äº›è¯­è¨€å­¦çš„æ¢—å’Œæœ‰æ„æ€çš„çŸ¥è¯†" href=https://Siriuslala.github.io/posts/%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E5%AD%A6%E7%9A%84%E6%A2%97%E5%92%8C%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%9F%A5%E8%AF%86/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Possible Research Areas in Mechanistic Interpretability</h2></header><div class=entry-content><p>ğŸ’¡ This post is mainly focused on text models. For multi-modal models, please refer to this post.
The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.
Circuit Discovery Methods basic activation patching (causal mediation/interchange interventionsâ€¦) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? ğŸ”­ resources inspirition Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned what is circuit discovery? Towards Best Practices of Activation Patching in Language Models: Metrics and Methods How to use and interpret activation patching representative work activation patching Investigating gender bias in language models using causal mediation analysis (ROME) Locating and Editing Factual Associations in GPT Causal Scrubbing: a method for rigorously testing interpretability hypotheses (AtP) Attribution patching: Activation patching at industrial scale AtP*: An efficient and scalable method for localizing llm behaviour to components path patching (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability (EAP) Attribution Patching Outperforms Automated Circuit Discovery (EAP-IG) Have faith in faithfulness: Going beyond circuit overlap when finding model mechanisms Localizing Model Behavior with Path Patching distributed alignment search (DAS) Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations Interpretability at Scale: Identifying Causal Mechanisms in Alpaca new Using SAE Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models Automatically Identifying Local and Global Circuits with Linear Computation Graphs Contextual Decomposition Mechanistic Interpretation through Contextual Decomposition in Transformers Edge Pruning ? Finding Transformer Circuits with Edge Pruning Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning attribution graph see Applications in the Dictionary Learning section Evaluation lack of ground truth
...</p></div><footer class=entry-footer><span title='2024-09-06 22:52:16 +0800 CST'>September 6, 2024</span>&nbsp;Â·&nbsp;7 min&nbsp;Â·&nbsp;3281 words&nbsp;Â·&nbsp;Sirius</footer><a class=entry-link aria-label="post link to Possible Research Areas in Mechanistic Interpretability" href=https://Siriuslala.github.io/posts/mech_interp_research/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>Exploring Emotional Features in GPT2-Small</h2></header><div class=entry-content><p>ğŸ¶Code in this post can be found at the jupyter notebook in my â€œsaeExplorationâ€ repo.
Find features that reflect positive emotions To find the features related to a specific emotion, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:
1 2 3 4 5 prompt_happy = ["I'll be on a vacation tomorrow and I'm so happy.", "My mombrings home a new puppy and I'm so happy.", "I'm so glad I got the job I wanted.", "I feel so happy when I'm with my friends.", "I'm so happy I got the promotion I wanted.",] I choose to look for features that reflect happiness and sadness. Apart from that, I also wonder if the feature that reflects excitedness has something to do with the one that reflects happiness (they are alike from the semantic level at least.)
...</p></div><footer class=entry-footer><span title='2024-08-29 15:51:59 +0800 CST'>August 29, 2024</span>&nbsp;Â·&nbsp;6 min&nbsp;Â·&nbsp;1114 words&nbsp;Â·&nbsp;Sirius</footer><a class=entry-link aria-label="post link to Exploring Emotional Features in GPT2-Small" href=https://Siriuslala.github.io/posts/happy_feats/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>A Brief Introduction to Mechanistic Interpretability Research</h2></header><div class=entry-content><p>âš ï¸ Warnings
This post was written when I first delved into this area, and it hasnâ€™t been updated for a long time. Thus there might be a lot of errors. Iâ€™m still interested in interpretability and its applications. Iâ€™ll write something new and interesting later ~ ğŸ’¡ This post is accompanied with another post, which contains specific content in this area.
...</p></div><footer class=entry-footer><span title='2024-08-28 13:12:25 +0800 CST'>August 28, 2024</span>&nbsp;Â·&nbsp;16 min&nbsp;Â·&nbsp;3208 words&nbsp;Â·&nbsp;Sirius</footer><a class=entry-link aria-label="post link to A Brief Introduction to Mechanistic Interpretability Research" href=https://Siriuslala.github.io/posts/mech_interp_resource/></a></article></main><footer class=footer><span>&copy; 2026 <a href=https://Siriuslala.github.io/>Siriuslala's Blog!</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>