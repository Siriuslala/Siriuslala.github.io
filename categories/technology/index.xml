<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Technology on Siriuslala&#39;s Blog!</title>
    <link>https://Siriuslala.github.io/categories/technology/</link>
    <description>Recent content in Technology on Siriuslala&#39;s Blog!</description>
    <image>
      <title>Siriuslala&#39;s Blog!</title>
      <url>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.147.6</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Jun 2025 10:50:00 +0800</lastBuildDate>
    <atom:link href="https://Siriuslala.github.io/categories/technology/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Thinking and Reasoning</title>
      <link>https://Siriuslala.github.io/posts/thinking_and_reasoning/</link>
      <pubDate>Mon, 30 Jun 2025 10:50:00 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/thinking_and_reasoning/</guid>
      <description>&lt;h2 id=&#34;the-purpose-i-write-this-blog&#34;&gt;The Purpose I Write This Blog&lt;/h2&gt;
&lt;p&gt;   Thinking models are crazily popualr nowadays. The first time I delved in this area was in September, 2023. Later I gradually forgetted this area, until Deepseek came to life. I want to keep to collect information about LLM reasoning and share my thoughts here.&lt;/p&gt;
&lt;h3 id=&#34;thinking-models&#34;&gt;Thinking Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;text-based&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;explicit reasoning
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.12948?&#34;&gt;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2504.13914&#34;&gt;Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.12599&#34;&gt;Kimi k1.5: Scaling Reinforcement Learning with LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.06471&#34;&gt;GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.22312&#34;&gt;Skywork Open Reasoner 1 Technical Report&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;implicit reasoning
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.06769&#34;&gt;(Coconut) Training Large Language Models to Reason in a Continuous Latent Space&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;others
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2404.02893&#34;&gt;ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;blogs
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://yuanchaofa.com/post/deepseek-r1-paper-reading-notes.html&#34;&gt;自顶向下方式深度解读 DeepSeek-R1，内含大量细节&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yuanchaofa.com/post/hands-on-deepseek-mla.html&#34;&gt;MLA(1)：从代码角度学习和彻底理解 DeepSeek MLA 算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/20161412399&#34;&gt;从头理解思考模型（LLM based Reasoning Model），O1，DeepSeek R1，Kimi K1.5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;overthinking&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>LLM Agents</title>
      <link>https://Siriuslala.github.io/posts/llm-agent/</link>
      <pubDate>Sat, 28 Jun 2025 13:01:09 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/llm-agent/</guid>
      <description>&lt;h2 id=&#34;the-purpose-i-write-this-blog&#34;&gt;The Purpose I Write This Blog&lt;/h2&gt;
&lt;p&gt;   LLM-based agent is gonna change the world. Amazing agent systems have been created to change our life. Since I was once in a team that aimed to build advanced agents for the control of digital devices and for which I was impressed, I want to keep to collect information about LLM agents and share my thoughts here.&lt;/p&gt;
&lt;h2 id=&#34;resource&#34;&gt;Resource&lt;/h2&gt;
&lt;h3 id=&#34;gui-agents&#34;&gt;GUI Agents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;survey&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.18279&#34;&gt;Large Language Model-Brained GUI Agents: A Survey&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzkwNjE2ODMxNQ==&amp;amp;mid=2247488335&amp;amp;idx=1&amp;amp;sn=506d56c87a179b3af119d4f70dd549f5&amp;amp;scene=21&amp;amp;poc_token=HIF1X2ijWKfo2MykofnxM8oq-4mrLjJkhL8TdWx4&#34;&gt;GUI Agent综述 : 揭秘GUI智能体的前世今生-1 : 总览篇-启程&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;models&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;autoglm
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.14040&#34;&gt;ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Xiao9905/AutoGLM/blob/main/static/papers/mobilerl_0820.pdf&#34;&gt;MobileRL: Advancing Mobile Use Agents With Adaptive Online Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2504.19298&#34;&gt;ANDROIDGEN: Building an Android Language Agent under Data Scarcity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.00820&#34;&gt;Autoglm: Autonomous foundation agents for guis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.02337&#34;&gt;WebRL:Training llm web agents via self-evolving online curriculum reinforcement learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2410.24024&#34;&gt;AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;others
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2024/file/1704ddd0bb89f159dfe609b32c889995-Paper-Conference.pdf&#34;&gt;DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.10935&#34;&gt;SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3706598.3713600&#34;&gt;Appagent: Multimodal agents as smartphone users&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.01614&#34;&gt;(SeeAct) GPT-4V(ision) is a Generalist Web Agent, if Grounded&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.11441&#34;&gt;Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;benchmarks&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;web
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2307.13854&#34;&gt;WebArena: A Realistic Web Environment for Building Autonomous Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/5950bf290a1570ea401bf98882128160-Paper-Datasets_and_Benchmarks.pdf&#34;&gt;Mind2web: Towards a generalist agent for the web&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2024/file/5d413e48f84dc61244b6be550f1cd8f5-Paper-Datasets_and_Benchmarks_Track.pdf&#34;&gt;Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.mlr.press/v70/shi17a/shi17a.pdf&#34;&gt;(MiniWob) World of Bits: An Open-Domain Platform for Web-Based Agents&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;android
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/bbbb6308b402fe909c39dd29950c32e0-Paper-Datasets_and_Benchmarks.pdf&#34;&gt;Android in the Wild: A Large-Scale Dataset for Android Device Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.06596&#34;&gt;(AndroidArena) Understanding the weakness of large language model agents within a complex android environment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deepresearch&#34;&gt;DeepResearch&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;survey&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.18096&#34;&gt;Deep Research Agents: A Systematic Examination And Roadmap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.17188v1&#34;&gt;Towards AI Search Paradigm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;models&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.05366&#34;&gt;Search-o1: Agentic search-enhanced large reasoning models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2503.09516&#34;&gt;Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.05592&#34;&gt;R1-searcher: Incentivizing the search capability in llms via reinforcement learning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/PeterGriffinJin/Search-R1&#34;&gt;repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jina-ai/node-DeepResearch&#34;&gt;(Jina) node-DeepResearch Public&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://moonshotai.github.io/Kimi-Researcher/&#34;&gt;Kimi-Researcher: End-to-End RL Training for Emerging Agentic Capabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.20249&#34;&gt;Language Modeling by Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;agentic-rl&#34;&gt;Agentic RL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;papers&lt;/p&gt;</description>
    </item>
    <item>
      <title>Interpretability (&amp; other areas) for Multimodal Models</title>
      <link>https://Siriuslala.github.io/posts/mm_interp/</link>
      <pubDate>Tue, 25 Feb 2025 15:08:53 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/mm_interp/</guid>
      <description>&lt;p&gt;&amp;#x1f4a1;
This post is initially focused on interpretability for multimodal models, while later a lot of papers in other fields are included, just for convenience.&lt;/p&gt;
&lt;h2 id=&#34;resource&#34;&gt;Resource&lt;/h2&gt;
&lt;h3 id=&#34;interpretability-for-mllms&#34;&gt;Interpretability for MLLMs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;survey&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.17516&#34;&gt;A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.01048v1&#34;&gt;Sparks of Explainability Recent Advancements in Explaining Large Vision Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/itsqyh/Awesome-LMMs-Mechanistic-Interpretability?tab=readme-ov-file#-blog&#34;&gt;Awesome LMMs Mechanistic Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;probing&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.17304&#34;&gt;Probing Multimodal Large Language Models for Global and Local Semantic Representations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;representation&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://distill.pub/2020/circuits/zoom-in/?ref=cold-takes&#34;&gt;Zoom in: An introduction to circuits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://distill.pub/2021/multimodal-neurons/&#34;&gt;Multimodal Neurons in Artificial Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.05916&#34;&gt;Interpreting CLIP&amp;rsquo;s Image Representation via Text-Based Decomposition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.04341&#34;&gt;Interpreting the Second-Order Effects of Neurons in CLIP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cnblogs.com/LittleHenry/p/18688886&#34;&gt;CLIP不同层&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023W/CLVL/papers/Schwettmann_Multimodal_Neurons_in_Pretrained_Text-Only_Transformers_ICCVW_2023_paper.pdf&#34;&gt;Multimodal Neurons in Pretrained Text-Only Transformers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;circuit&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.04236&#34;&gt;**(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2404.14349&#34;&gt;Automatic Discovery of Visual Circuits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023W/CLVL/papers/Palit_Towards_Vision-Language_Mechanistic_Interpretability_A_Causal_Tracing_Tool_for_BLIP_ICCVW_2023_paper.pdf&#34;&gt;Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SAE&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/iYFuZo9BMvr6GgMs5/case-study-interpreting-manipulating-and-controlling-clip&#34;&gt;Case study: Interpreting, manipulating, and controlling clip with sparse autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/bCtbuWraqYTDtuARg/towards-multimodal-interpretability-learning-sparse-2&#34;&gt;Towards multimodal interpretability: Learning sparse interpretable features in vision transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2407.14499&#34;&gt;Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;visualization&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/398408338&#34;&gt;Visualizer！简化你的Vision Transformer可视化！&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.02957&#34;&gt;(DVT) Denoising Vision Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.23270&#34;&gt;Token Activation Map to Visually Explain Multimodal LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2024W/XAI4CV/papers/Stan_LVLM-Intrepret_An_Interpretability_Tool_for_Large_Vision-Language_Models_CVPRW_2024_paper.pdf&#34;&gt;LVLM-Intrepret: An Interpretability Tool for Large Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2012.09838&#34;&gt;Transformer Interpretability Beyond Attention Visualization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;others&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2410.07149&#34;&gt;**Towards interpreting visual information processing in vision-language models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://clementneo.com/llava_logit_lens/&#34;&gt;demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.alignmentforum.org/posts/kobJymvvcvhbjWFKe/laying-the-foundations-for-vision-and-multimodal-mechanistic&#34;&gt;(dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability &amp;amp; Open Problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2203.14680&#34;&gt;Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;information flow&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.18620&#34;&gt;**Cross-modal Information Flow in Multimodal Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.06579&#34;&gt;*From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.17491&#34;&gt;*What&amp;rsquo;s in the Image? A Deep-Dive into the Vision of Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.06646&#34;&gt;The Narrow Gate: Localized Image-Text Communication in Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;&#34;&gt;Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.13108&#34;&gt;Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;analyses on MLLMs&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.02199&#34;&gt;Words or Vision: Do Vision-Language Models Have Blind Faith in Text?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.15969&#34;&gt;Forgotten Polygons: Multimodal Large Language Models are Shape-Blind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2309.16588&#34;&gt;Vision Transformers Need Registers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2507.03683&#34;&gt;On the rankability of visual embeddings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;other-fields-of-mllms&#34;&gt;Other fields of MLLMs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;visual pretraining&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks</title>
      <link>https://Siriuslala.github.io/posts/circuit_tuning/</link>
      <pubDate>Wed, 05 Feb 2025 16:22:33 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/circuit_tuning/</guid>
      <description>&lt;!-- The [paper](/pdfs/circuit_tuning) is here. --&gt;
&lt;p&gt;ArXiv(old version): &lt;a href=&#34;https://arxiv.org/pdf/2502.06106&#34;&gt;https://arxiv.org/pdf/2502.06106&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Possible Research Areas in Mechanistic Interpretability</title>
      <link>https://Siriuslala.github.io/posts/mech_interp_research/</link>
      <pubDate>Fri, 06 Sep 2024 22:52:16 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/mech_interp_research/</guid>
      <description>&lt;p&gt;&amp;#x1f4a1;
This post is mainly focused on text models. For multi-modal models, please refer to &lt;a href=&#34;https://Siriuslala.github.io/posts/mm_interp/&#34;&gt;this post&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-purpose-i-write-this-blog&#34;&gt;The Purpose I Write This Blog&lt;/h2&gt;
&lt;p&gt;   To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.&lt;/p&gt;
&lt;h2 id=&#34;circuit-discovery&#34;&gt;Circuit Discovery&lt;/h2&gt;
&lt;h3 id=&#34;methods&#34;&gt;Methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;basic&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;activation patching (causal mediation/interchange interventions&amp;hellip;)&lt;/li&gt;
&lt;li&gt;path patching&lt;/li&gt;
&lt;li&gt;scaling techinques: attribution patching&lt;/li&gt;
&lt;li&gt;DAS (distributed alignment search)   directional activation patching?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;telescope-resources&#34;&gt;&amp;#x1f52d; resources&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;inspirition
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1905.09418&#34;&gt;Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.05262&#34;&gt;(ROME) Locating and Editing Factual Associations in GPT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.neelnanda.io/mechanistic-interpretability/attribution-patching&#34;&gt;Attribution patching: Activation patching at industrial scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.14997&#34;&gt;(ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.10348&#34;&gt;Attribution Patching Outperforms Automated Circuit Discovery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2403.00745&#34;&gt;AtP*: An efficient and scalable method for localizing llm behaviour to components&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing&#34;&gt;Causal Scrubbing: a method for rigorously testing interpretability hypotheses&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;new&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Using SAE
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.19647&#34;&gt;Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2405.13868&#34;&gt;Automatically Identifying Local and Global Circuits with Linear Computation Graphs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Contextual Decomposition
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.00886&#34;&gt;Mechanistic Interpretation through Contextual Decomposition in Transformers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Edge Pruning ?
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.16778&#34;&gt;Finding Transformer Circuits with Edge Pruning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.03779&#34;&gt;Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;lack of ground truth&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring Emotional Features in GPT2-Small</title>
      <link>https://Siriuslala.github.io/posts/happy_feats/</link>
      <pubDate>Thu, 29 Aug 2024 15:51:59 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/happy_feats/</guid>
      <description>&lt;p&gt;&amp;#x1f3b6;Code in this post can be found at &lt;a href=&#34;https://github.com/Siriuslala/saeExploration/blob/main/multilingual_study.ipynb&#34;&gt;the jupyter notebook in my &amp;ldquo;saeExploration&amp;rdquo; repo&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;find-features-that-reflect-positive-emotions&#34;&gt;Find features that reflect positive emotions&lt;/h2&gt;
&lt;p&gt;To find the features related to a specific emotion, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-1&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-1&#34;&gt;1&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-2&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-2&#34;&gt;2&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-3&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-3&#34;&gt;3&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-4&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-4&#34;&gt;4&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-5&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-5&#34;&gt;5&lt;/a&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;prompt_happy&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;ll be on a vacation tomorrow and I&amp;#39;m so happy.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;My mombrings home a new puppy and I&amp;#39;m so happy.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;m so glad I got the job I wanted.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I feel so happy when I&amp;#39;m with my friends.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;m so happy I got the promotion I wanted.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;I choose to look for features that reflect happiness and sadness. Apart from that, I also wonder if the feature that reflects excitedness has something to do with the one that reflects happiness (they are alike from the semantic level at least.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Brief Introduction to Mechanistic Interpretability Research</title>
      <link>https://Siriuslala.github.io/posts/mech_interp_resource/</link>
      <pubDate>Wed, 28 Aug 2024 13:12:25 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/mech_interp_resource/</guid>
      <description>&lt;p&gt;&amp;#x26a0;&amp;#xfe0f; &lt;font color=&#34;red&#34;&gt;&lt;em&gt;&lt;strong&gt;Warnings&lt;/strong&gt;&lt;/em&gt;&lt;/font&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;This post was written when I first delved into this area, and it hasn&amp;rsquo;t been updated for a long time. Thus there might be a lot of errors.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Now I&amp;rsquo;ve changed my attitude to this area. The area is not well-defined, and most of the research in this area is of low quality and is not appealing to me. Besides, I think the study of interpretability should be applied to pratical use, though we can also study it for fun.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;I&amp;rsquo;m still interested in interpretability and its applications. I&amp;rsquo;ll write something new and interesting later ~&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;#x1f4a1;
This post is accompanied with &lt;a href=&#34;https://Siriuslala.github.io/posts/mech_interp_research/&#34;&gt;another post&lt;/a&gt;, which contains specific content in this area.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
