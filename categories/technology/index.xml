<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Technology on Siriuslala&#39;s Blog!</title>
    <link>https://Siriuslala.github.io/categories/technology/</link>
    <description>Recent content in Technology on Siriuslala&#39;s Blog!</description>
    <image>
      <title>Siriuslala&#39;s Blog!</title>
      <url>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.147.6</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Jun 2025 10:50:00 +0800</lastBuildDate>
    <atom:link href="https://Siriuslala.github.io/categories/technology/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Thinking and Reasoning</title>
      <link>https://Siriuslala.github.io/posts/thinking_and_reasoning/</link>
      <pubDate>Mon, 30 Jun 2025 10:50:00 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/thinking_and_reasoning/</guid>
      <description>&lt;h2 id=&#34;the-purpose-i-write-this-blog&#34;&gt;The Purpose I Write This Blog&lt;/h2&gt;
&lt;p&gt;   Thinking models are crazily popualr nowadays. The first time I delved in this area was in September, 2023. Later I gradually forgetted this area, until Deepseek came to life. I want to keep to collect information about LLM reasoning and share my thoughts here.&lt;/p&gt;
&lt;h2 id=&#34;thinking-models&#34;&gt;Thinking Models&lt;/h2&gt;
&lt;h3 id=&#34;text-based&#34;&gt;text-based&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;explicit reasoning
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.12948?&#34;&gt;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2504.13914&#34;&gt;Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.12599&#34;&gt;Kimi k1.5: Scaling Reinforcement Learning with LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.06471&#34;&gt;GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.22312&#34;&gt;Skywork Open Reasoner 1 Technical Report&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;implicit reasoning
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.06769&#34;&gt;(Coconut) Training Large Language Models to Reason in a Continuous Latent Space&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;others
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2404.02893&#34;&gt;ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;blogs
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://yuanchaofa.com/post/deepseek-r1-paper-reading-notes.html&#34;&gt;自顶向下方式深度解读 DeepSeek-R1，内含大量细节&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yuanchaofa.com/post/hands-on-deepseek-mla.html&#34;&gt;MLA(1)：从代码角度学习和彻底理解 DeepSeek MLA 算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/20161412399&#34;&gt;从头理解思考模型（LLM based Reasoning Model），O1，DeepSeek R1，Kimi K1.5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;overthinking&#34;&gt;overthinking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;survey
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2503.16419&#34;&gt;Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs?&#34;&gt;(repo) Awesome-Efficient-Reasoning-LLMs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;papers
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.09388&#34;&gt;Qwen3 Technical Report&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.11896&#34;&gt;AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.13417&#34;&gt;AdaptThink: Reasoning Models Can Learn When to Think&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;blogs
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://yuanchaofa.com/post/slow-fast-thinking-from-qwen3-thinking-mixed-to-adacot-to-adathinking.html&#34;&gt;自适应快慢思考推理模型（Adaptive Reasoning Model）：Qwen3混合思考-&amp;gt;字节AdaCoT-&amp;gt;清华AdaptThinking&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;parallel-thinking&#34;&gt;parallel thinking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.15260&#34;&gt;Deep Think with Confidence&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;visual-reasoning&#34;&gt;visual reasoning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;survey
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.23918&#34;&gt;Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;papers
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2312.14135&#34;&gt;$V^{*}$: Guided Visual Search as a Core Mechanism in Multimodal LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;active perception
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.14362&#34;&gt;DeepEyes: Incentivizing “Thinking with Images” via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.15436&#34;&gt;Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.15879&#34;&gt;GRIT: Teaching MLLMs to Think with Images&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;tool use
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.20289&#34;&gt;VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.19255&#34;&gt;VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.20256&#34;&gt;Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.09403&#34;&gt;Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;imagination
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.22525v1&#34;&gt;Thinking with Generated Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.11409&#34;&gt;Visual Planning: Let&amp;rsquo;s Think Only with Images&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;blogs
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.xiaohongshu.com/explore/683ccdab000000000303c818?app_platform=android&amp;amp;ignoreEngage=true&amp;amp;app_version=8.91.0&amp;amp;share_from_user_hidden=true&amp;amp;xsec_source=app_share&amp;amp;type=normal&amp;amp;xsec_token=CBrIk8OuUeFJeSobm2LWUAhX-LDwR5xnSiLv1nbJnaGwA=&amp;amp;author_share=1&amp;amp;xhsshare=WeixinSession&amp;amp;shareRedId=N0wyNUk4SUA2NzUyOTgwNjY0OTc3SDhP&amp;amp;apptime=1752769027&amp;amp;share_id=1b8ccda2261146b3bef99ba488c1530d&amp;amp;share_channel=wechat&#34;&gt;Thinking with Images 小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;others&#34;&gt;others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/6hFxdtenAsJ0qU1wEP_XOA&#34;&gt;[蒙特卡洛搜索树] MCT Self-Refine (MCTSr)的算法（包含代码理解）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/27278317894&#34;&gt;聊聊推理模型中的PRMs与MCTS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;h3 id=&#34;dataset&#34;&gt;dataset&lt;/h3&gt;
&lt;h2 id=&#34;analyses&#34;&gt;Analyses&lt;/h2&gt;
&lt;h3 id=&#34;analyses-1&#34;&gt;analyses&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.08221&#34;&gt;Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;interpretability&#34;&gt;interpretability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.05744&#34;&gt;Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2507.00432&#34;&gt;Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.02867&#34;&gt;Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.19143&#34;&gt;Thought Anchors: Which LLM Reasoning Steps Matter?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.18167&#34;&gt;Understanding Reasoning in Thinking Language Models via Steering Vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.alphaxiv.org/abs/2025.02&#34;&gt;Chain-of-Thought Is Not Explainability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.04667&#34;&gt;Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.13913&#34;&gt;How Do LLMs Perform Two-Hop Reasoning in Context?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;theories&#34;&gt;theories&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.12514&#34;&gt;Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reinforcement-learning&#34;&gt;Reinforcement Learning&lt;/h2&gt;
&lt;h3 id=&#34;rl-algorithms&#34;&gt;RL algorithms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1506.02438&#34;&gt;(GAE) High-Dimensional Continuous Control Using Generalized Advantage Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf&#34;&gt;(DPO) Direct preference optimization: Your language model is secretly a reward model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2404.12358&#34;&gt;From r to q∗: Your language model is secretly a q-function&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/693746297&#34;&gt;DPO新作Your Language Model is Secretly a Q-Function解读，与OPENAI Q* 的联系？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1707.06347&#34;&gt;(PPO) Proximal Policy Optimization Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.03262&#34;&gt;(REINFORCE++) REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt and Reward Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.12948?&#34;&gt;(GRPO) DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.14476&#34;&gt;(DAPO) DAPO: An Open-Source LLM Reinforcement Learning System at Scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2507.18071&#34;&gt;(GSPO) Group Sequence Policy Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.13585&#34;&gt;(Cispo) MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;blogs&#34;&gt;Blogs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;algorithms
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/7461863937&#34;&gt;人人都能看懂的RL-PPO理论知识&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/25410252053?share_code=2UJFMKKeyurA&amp;amp;utm_psn=1939508205295239578&#34;&gt;Reasoning LLM（三）：LLM+RL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/e6mfNLYwLQV-UKuVo6U6zQ&#34;&gt;RLHF 常见的思维误区&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reward-modeling&#34;&gt;reward modeling&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;text
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.20050&#34;&gt;(PRM) Let&amp;rsquo;s verify step by step&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2507.05197&#34;&gt;(POLAR) Pre-Trained Policy Discriminators are General Reward Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;reward model for generative models
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.13918&#34;&gt;Improving Video Generation with Human Feedback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.21059v1&#34;&gt;VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2311.04155&#34;&gt;Black-Box Prompt Optimization: Aligning Large Language Models without Model Training&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;analyses-2&#34;&gt;analyses&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RL training&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.08221&#34;&gt;Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;entropy&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1917257135730964355&#34;&gt;Reasoning LLM（五）：熵缩过程与能力边界&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1924309705548867391&#34;&gt;LLMxRL】熵坍缩与缓解策略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.22617&#34;&gt;(clip/kl-cov) The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.01939&#34;&gt;(forking tokens) Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RL v.s. SFT&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/25410252053&#34;&gt;3.2 统一视角理解从 SFT 到 RL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.01067&#34;&gt;All Roads Lead to Likelihood: The Value of Reinforcement Learning in Fine-Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.23235&#34;&gt;Generalist Reward Models: Found Inside Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.05629&#34;&gt;(DFT) On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1941156341583418911&#34;&gt;从 SFT 到 RL：一步步看清它们的联系&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.18116&#34;&gt;(NFT) Bridging Supervised Learning and Reinforcement Learning in Math Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;resource&#34;&gt;Resource&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;RL infra
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2409.19256v2&#34;&gt;(verl) HybridFlow: A Flexible and Efficient RLHF Framework&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/index.html&#34;&gt;doc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/volcengine/verl&#34;&gt;repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/THUDM/slime&#34;&gt;slime&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/UbNhNkBed9QO1VrwyR-Faw&#34;&gt;RL Scaling 时代，我们需要什么样的 RL 框架呢？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;blogs&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;training
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1936165441090328553&#34;&gt;浅聊RL框架的勃勃生机、万物竞发&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.anthropic.com/engineering/multi-agent-research-system&#34;&gt;How we built our multi-agent research system&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;verl
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/27676081245&#34;&gt;[AI Infra] VeRL 框架入门&amp;amp;代码带读&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/30876678559&#34;&gt;从零开始的verl框架解析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1907077645071524975&#34;&gt;verl RL支持训练deepseek-v3 671B实习复盘(个人版)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/29149216967&#34;&gt;OpenRLHF&amp;amp;Verl参数转换指南&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/25556718002&#34;&gt;verl小白解读&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1937826285264011929&#34;&gt;一文深度全面解析大模型分布式并行策略：DP/TP/PP/CP/EP/SP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/650383289&#34;&gt;深入理解 Megatron-LM（2）原理介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/m0_60388871/article/details/148958601&#34;&gt;DeepSpeed zero1，zero2，zero3和FSDP区别详解&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;inference
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/711378550&#34;&gt;SGLang：LLM推理引擎发展新方向&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/669926191&#34;&gt;图解大模型计算加速系列：FlashAttention V1，从硬件到计算逻辑&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>LLM Agents</title>
      <link>https://Siriuslala.github.io/posts/llm-agent/</link>
      <pubDate>Sat, 28 Jun 2025 13:01:09 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/llm-agent/</guid>
      <description>&lt;h2 id=&#34;the-purpose-i-write-this-blog&#34;&gt;The Purpose I Write This Blog&lt;/h2&gt;
&lt;p&gt;   LLM-based agent is gonna change the world. Amazing agent systems have been created to change our life. Since I was once in a team that aimed to build advanced agents for the control of digital devices and for which I was impressed, I want to keep to collect information about LLM agents and share my thoughts here.&lt;/p&gt;
&lt;h2 id=&#34;resource&#34;&gt;Resource&lt;/h2&gt;
&lt;h3 id=&#34;gui-agents&#34;&gt;GUI Agents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;survey&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.18279&#34;&gt;Large Language Model-Brained GUI Agents: A Survey&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzkwNjE2ODMxNQ==&amp;amp;mid=2247488335&amp;amp;idx=1&amp;amp;sn=506d56c87a179b3af119d4f70dd549f5&amp;amp;scene=21&amp;amp;poc_token=HIF1X2ijWKfo2MykofnxM8oq-4mrLjJkhL8TdWx4&#34;&gt;GUI Agent综述 : 揭秘GUI智能体的前世今生-1 : 总览篇-启程&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;models&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;autoglm
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.14040&#34;&gt;ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Xiao9905/AutoGLM/blob/main/static/papers/mobilerl_0820.pdf&#34;&gt;MobileRL: Advancing Mobile Use Agents With Adaptive Online Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2504.19298&#34;&gt;ANDROIDGEN: Building an Android Language Agent under Data Scarcity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.00820&#34;&gt;Autoglm: Autonomous foundation agents for guis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.02337&#34;&gt;WebRL:Training llm web agents via self-evolving online curriculum reinforcement learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2410.24024&#34;&gt;AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;others
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2024/file/1704ddd0bb89f159dfe609b32c889995-Paper-Conference.pdf&#34;&gt;DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.10935&#34;&gt;SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3706598.3713600&#34;&gt;Appagent: Multimodal agents as smartphone users&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.01614&#34;&gt;(SeeAct) GPT-4V(ision) is a Generalist Web Agent, if Grounded&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.11441&#34;&gt;Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;benchmarks&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;web
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2307.13854&#34;&gt;WebArena: A Realistic Web Environment for Building Autonomous Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/5950bf290a1570ea401bf98882128160-Paper-Datasets_and_Benchmarks.pdf&#34;&gt;Mind2web: Towards a generalist agent for the web&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2024/file/5d413e48f84dc61244b6be550f1cd8f5-Paper-Datasets_and_Benchmarks_Track.pdf&#34;&gt;Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.mlr.press/v70/shi17a/shi17a.pdf&#34;&gt;(MiniWob) World of Bits: An Open-Domain Platform for Web-Based Agents&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;android
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/bbbb6308b402fe909c39dd29950c32e0-Paper-Datasets_and_Benchmarks.pdf&#34;&gt;Android in the Wild: A Large-Scale Dataset for Android Device Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.06596&#34;&gt;(AndroidArena) Understanding the weakness of large language model agents within a complex android environment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deepresearch&#34;&gt;DeepResearch&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;survey&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.18096&#34;&gt;Deep Research Agents: A Systematic Examination And Roadmap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.17188v1&#34;&gt;Towards AI Search Paradigm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;models&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.05366&#34;&gt;Search-o1: Agentic search-enhanced large reasoning models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2503.09516&#34;&gt;Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.05592&#34;&gt;R1-searcher: Incentivizing the search capability in llms via reinforcement learning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/PeterGriffinJin/Search-R1&#34;&gt;repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jina-ai/node-DeepResearch&#34;&gt;(Jina) node-DeepResearch Public&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://moonshotai.github.io/Kimi-Researcher/&#34;&gt;Kimi-Researcher: End-to-End RL Training for Emerging Agentic Capabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.20249&#34;&gt;Language Modeling by Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;agentic-rl&#34;&gt;Agentic RL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;papers&lt;/p&gt;</description>
    </item>
    <item>
      <title>Interpretability (&amp; other areas) for Multimodal Models</title>
      <link>https://Siriuslala.github.io/posts/mm_interp/</link>
      <pubDate>Tue, 25 Feb 2025 15:08:53 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/mm_interp/</guid>
      <description>&lt;p&gt;&amp;#x1f4a1;
This post is initially focused on interpretability for multimodal models, while later a lot of papers in other fields are included, just for convenience.&lt;/p&gt;
&lt;h2 id=&#34;resource&#34;&gt;Resource&lt;/h2&gt;
&lt;h3 id=&#34;interpretability-for-mllms&#34;&gt;Interpretability for MLLMs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;survey&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.17516&#34;&gt;A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.01048v1&#34;&gt;Sparks of Explainability Recent Advancements in Explaining Large Vision Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/itsqyh/Awesome-LMMs-Mechanistic-Interpretability?tab=readme-ov-file#-blog&#34;&gt;Awesome LMMs Mechanistic Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;probing&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.17304&#34;&gt;Probing Multimodal Large Language Models for Global and Local Semantic Representations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;representation&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://distill.pub/2020/circuits/zoom-in/?ref=cold-takes&#34;&gt;Zoom in: An introduction to circuits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://distill.pub/2021/multimodal-neurons/&#34;&gt;Multimodal Neurons in Artificial Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.05916&#34;&gt;Interpreting CLIP&amp;rsquo;s Image Representation via Text-Based Decomposition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.04341&#34;&gt;Interpreting the Second-Order Effects of Neurons in CLIP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cnblogs.com/LittleHenry/p/18688886&#34;&gt;CLIP不同层&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023W/CLVL/papers/Schwettmann_Multimodal_Neurons_in_Pretrained_Text-Only_Transformers_ICCVW_2023_paper.pdf&#34;&gt;Multimodal Neurons in Pretrained Text-Only Transformers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;circuit&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.04236&#34;&gt;**(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2404.14349&#34;&gt;Automatic Discovery of Visual Circuits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023W/CLVL/papers/Palit_Towards_Vision-Language_Mechanistic_Interpretability_A_Causal_Tracing_Tool_for_BLIP_ICCVW_2023_paper.pdf&#34;&gt;Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SAE&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/iYFuZo9BMvr6GgMs5/case-study-interpreting-manipulating-and-controlling-clip&#34;&gt;Case study: Interpreting, manipulating, and controlling clip with sparse autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/bCtbuWraqYTDtuARg/towards-multimodal-interpretability-learning-sparse-2&#34;&gt;Towards multimodal interpretability: Learning sparse interpretable features in vision transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2407.14499&#34;&gt;Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;visualization&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/398408338&#34;&gt;Visualizer！简化你的Vision Transformer可视化！&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.02957&#34;&gt;(DVT) Denoising Vision Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.23270&#34;&gt;Token Activation Map to Visually Explain Multimodal LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2024W/XAI4CV/papers/Stan_LVLM-Intrepret_An_Interpretability_Tool_for_Large_Vision-Language_Models_CVPRW_2024_paper.pdf&#34;&gt;LVLM-Intrepret: An Interpretability Tool for Large Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2012.09838&#34;&gt;Transformer Interpretability Beyond Attention Visualization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;others&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2410.07149&#34;&gt;**Towards interpreting visual information processing in vision-language models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://clementneo.com/llava_logit_lens/&#34;&gt;demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.alignmentforum.org/posts/kobJymvvcvhbjWFKe/laying-the-foundations-for-vision-and-multimodal-mechanistic&#34;&gt;(dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability &amp;amp; Open Problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2203.14680&#34;&gt;Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;information flow&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.18620&#34;&gt;**Cross-modal Information Flow in Multimodal Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.06579&#34;&gt;*From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.17491&#34;&gt;*What&amp;rsquo;s in the Image? A Deep-Dive into the Vision of Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.06646&#34;&gt;The Narrow Gate: Localized Image-Text Communication in Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;&#34;&gt;Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.13108&#34;&gt;Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;analyses on MLLMs&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.02199&#34;&gt;Words or Vision: Do Vision-Language Models Have Blind Faith in Text?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.15969&#34;&gt;Forgotten Polygons: Multimodal Large Language Models are Shape-Blind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2309.16588&#34;&gt;Vision Transformers Need Registers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2507.03683&#34;&gt;On the rankability of visual embeddings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;other-fields-of-mllms&#34;&gt;Other fields of MLLMs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;visual pretraining&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks</title>
      <link>https://Siriuslala.github.io/posts/circuit_tuning/</link>
      <pubDate>Wed, 05 Feb 2025 16:22:33 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/circuit_tuning/</guid>
      <description>&lt;!-- The [paper](/pdfs/circuit_tuning) is here. --&gt;
&lt;p&gt;ArXiv(old version): &lt;a href=&#34;https://arxiv.org/pdf/2502.06106&#34;&gt;https://arxiv.org/pdf/2502.06106&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Possible Research Areas in Mechanistic Interpretability</title>
      <link>https://Siriuslala.github.io/posts/mech_interp_research/</link>
      <pubDate>Fri, 06 Sep 2024 22:52:16 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/mech_interp_research/</guid>
      <description>&lt;p&gt;&amp;#x1f4a1;
This post is mainly focused on text models. For multi-modal models, please refer to &lt;a href=&#34;https://Siriuslala.github.io/posts/mm_interp/&#34;&gt;this post&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-purpose-i-write-this-blog&#34;&gt;The Purpose I Write This Blog&lt;/h2&gt;
&lt;p&gt;   To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.&lt;/p&gt;
&lt;h2 id=&#34;circuit-discovery&#34;&gt;Circuit Discovery&lt;/h2&gt;
&lt;h3 id=&#34;methods&#34;&gt;Methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;basic&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;activation patching (causal mediation/interchange interventions&amp;hellip;)&lt;/li&gt;
&lt;li&gt;path patching&lt;/li&gt;
&lt;li&gt;scaling techinques: attribution patching&lt;/li&gt;
&lt;li&gt;DAS (distributed alignment search)   directional activation patching?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;telescope-resources&#34;&gt;&amp;#x1f52d; resources&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;inspirition
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1905.09418&#34;&gt;Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.05262&#34;&gt;(ROME) Locating and Editing Factual Associations in GPT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.neelnanda.io/mechanistic-interpretability/attribution-patching&#34;&gt;Attribution patching: Activation patching at industrial scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.14997&#34;&gt;(ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.10348&#34;&gt;Attribution Patching Outperforms Automated Circuit Discovery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2403.00745&#34;&gt;AtP*: An efficient and scalable method for localizing llm behaviour to components&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing&#34;&gt;Causal Scrubbing: a method for rigorously testing interpretability hypotheses&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;new&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Using SAE
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.19647&#34;&gt;Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2405.13868&#34;&gt;Automatically Identifying Local and Global Circuits with Linear Computation Graphs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Contextual Decomposition
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.00886&#34;&gt;Mechanistic Interpretation through Contextual Decomposition in Transformers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Edge Pruning ?
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.16778&#34;&gt;Finding Transformer Circuits with Edge Pruning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.03779&#34;&gt;Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;lack of ground truth&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring Emotional Features in GPT2-Small</title>
      <link>https://Siriuslala.github.io/posts/happy_feats/</link>
      <pubDate>Thu, 29 Aug 2024 15:51:59 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/happy_feats/</guid>
      <description>&lt;p&gt;&amp;#x1f3b6;Code in this post can be found at &lt;a href=&#34;https://github.com/Siriuslala/saeExploration/blob/main/multilingual_study.ipynb&#34;&gt;the jupyter notebook in my &amp;ldquo;saeExploration&amp;rdquo; repo&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;find-features-that-reflect-positive-emotions&#34;&gt;Find features that reflect positive emotions&lt;/h2&gt;
&lt;p&gt;To find the features related to a specific emotion, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-1&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-1&#34;&gt;1&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-2&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-2&#34;&gt;2&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-3&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-3&#34;&gt;3&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-4&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-4&#34;&gt;4&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-5&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-5&#34;&gt;5&lt;/a&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;prompt_happy&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;ll be on a vacation tomorrow and I&amp;#39;m so happy.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;My mombrings home a new puppy and I&amp;#39;m so happy.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;m so glad I got the job I wanted.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I feel so happy when I&amp;#39;m with my friends.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;m so happy I got the promotion I wanted.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;I choose to look for features that reflect happiness and sadness. Apart from that, I also wonder if the feature that reflects excitedness has something to do with the one that reflects happiness (they are alike from the semantic level at least.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Brief Introduction to Mechanistic Interpretability Research</title>
      <link>https://Siriuslala.github.io/posts/mech_interp_resource/</link>
      <pubDate>Wed, 28 Aug 2024 13:12:25 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/mech_interp_resource/</guid>
      <description>&lt;p&gt;&amp;#x26a0;&amp;#xfe0f; &lt;font color=&#34;red&#34;&gt;&lt;em&gt;&lt;strong&gt;Warnings&lt;/strong&gt;&lt;/em&gt;&lt;/font&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;This post was written when I first delved into this area, and it hasn&amp;rsquo;t been updated for a long time. Thus there might be a lot of errors.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Now I&amp;rsquo;ve changed my attitude to this area. The area is not well-defined, and most of the research in this area is of low quality and is not appealing to me. Besides, I think the study of interpretability should be applied to pratical use, though we can also study it for fun.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;I&amp;rsquo;m still interested in interpretability and its applications. I&amp;rsquo;ll write something new and interesting later ~&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;#x1f4a1;
This post is accompanied with &lt;a href=&#34;https://Siriuslala.github.io/posts/mech_interp_research/&#34;&gt;another post&lt;/a&gt;, which contains specific content in this area.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
