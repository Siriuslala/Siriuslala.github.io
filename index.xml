<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Siriuslala&#39;s Blog!</title>
    <link>https://Siriuslala.github.io/</link>
    <description>Recent content on Siriuslala&#39;s Blog!</description>
    <image>
      <title>Siriuslala&#39;s Blog!</title>
      <url>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.133.1</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Aug 2024 15:51:59 +0800</lastBuildDate>
    <atom:link href="https://Siriuslala.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Exploring Emotional Features in GPT2-Small</title>
      <link>https://Siriuslala.github.io/posts/happy_feats/</link>
      <pubDate>Thu, 29 Aug 2024 15:51:59 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/happy_feats/</guid>
      <description>&amp;#x1f3b6;Code in this post can be found at the jupyter notebook in my &amp;ldquo;saeExploration&amp;rdquo; repo.
Find features that reflect positive emotions To find the features related to a specific emotion and reduce data deviation, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:
1 2 3 4 5 prompt_happy = [&amp;#34;I&amp;#39;ll be on a vacation tomorrow and I&amp;#39;m so happy.&amp;#34;, &amp;#34;My mombrings home a new puppy and I&amp;#39;m so happy.</description>
    </item>
    <item>
      <title>A Brief Introduction to Mechanistic Interpretability Research</title>
      <link>https://Siriuslala.github.io/posts/mech_interp_resource/</link>
      <pubDate>Wed, 28 Aug 2024 13:12:25 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/mech_interp_resource/</guid>
      <description>The purpose I write this blog Mechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challanges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc).</description>
    </item>
    <item>
      <title>About Myself</title>
      <link>https://Siriuslala.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://Siriuslala.github.io/about/</guid>
      <description>About myself</description>
    </item>
    <item>
      <title>faq</title>
      <link>https://Siriuslala.github.io/faq/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://Siriuslala.github.io/faq/</guid>
      <description>faq</description>
    </item>
    <item>
      <title>Yueyan Li</title>
      <link>https://Siriuslala.github.io/helper/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://Siriuslala.github.io/helper/</guid>
      <description>PreviousNext &amp;nbsp; &amp;nbsp;/ [pdf]View the PDF file here.</description>
    </item>
  </channel>
</rss>
