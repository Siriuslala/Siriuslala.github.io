[{"content":"This post is written in Chinese. If you don\u0026rsquo;t know Chinese, you can learn it lol. (Sorry for this because simply translating the post into English may not be enough for you to understand).\n纯玩梗 语言现象背后蕴含的知识 皮钦语 (pidgin) 大家对那些 1.言语中不时夹杂着英文单词 2.装/凡尔赛 的人表现出一种厌恶。例如，下面是某恋综里的一段名场面：\n————————————————————————————————\n\u0026hellip;\n男A：\u0026ldquo;可能因为我学校在伦敦，所以\u0026hellip;\u0026rdquo; 女A：\u0026ldquo;哦我也是。我是高中在 York (Yorkshire) 附近，然后本科研究生在伦敦\u0026rdquo;\n男B：\u0026ldquo;我喜欢 nə —— uhh —— Southampton\u0026rdquo;\n女A：\u0026ldquo;在海边！\u0026rdquo;\n男B：\u0026ldquo;对超美！\u0026rdquo;\n男A：\u0026ldquo;然后，我也挺喜欢曼 —— emmm —— Manchester 的\u0026rdquo;\n\u0026hellip;\n————————————————————————————————\n好的嘲讽完毕，点到为止。\n实际上语言学里的语言演化领域有一个名词叫皮钦语（pidgin），也被称作混杂语，是语言发展的一个阶段，指在没有共同语言而又急于进行交流的人群中间产生的一种混合语言，属于不同语言人群的联系语言。感兴趣详见：皮克特语到四阶皮钦语——苏格兰语言迭代史。\n笔者印象里比较深的，一个是洋泾浜语，即流行在20世纪初的上海滩的一种英汉混杂语；另一个是协和语，指流行于伪满洲国的汉语、日语的混合语言。两者都是殖民地和半殖民地文化的产物。以下是一些例子：\n洋泾浜语：\u0026ldquo;来是\u0026rsquo;康姆\u0026rsquo;（come），去是\u0026rsquo;狗\u0026rsquo;（go）\u0026quot;（详见汪仲贤所著《上海俗话图说》；dbq耳边想起赵丽蓉老师的小品） 协和语：\u0026ldquo;你的帮我，我的钱的大大的给。\u0026rdquo; 此外，刘慈欣《三体》中也提到了由汉语和英语结合而成的新语言（舰队混合语）。\n其实，正常的、真诚的表达往往不会惹人厌烦，例如：\n\u0026ldquo;我希望能够做点兼职，至少能够 cover 掉我一部分的学费\u0026rdquo; \u0026ldquo;这个 checkpoint 我测了一下，跟 baseline 相比有很大的优势，但是跟 SOTA 比还是有很大的差距\u0026rdquo; \u0026ldquo;哈喽哈喽，你好呀~\u0026rdquo; 语言磨蚀 ","permalink":"https://Siriuslala.github.io/posts/%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E5%AD%A6%E7%9A%84%E6%A2%97%E5%92%8C%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%9F%A5%E8%AF%86/","summary":"This post is written in Chinese. If you don\u0026rsquo;t know Chinese, you can learn it lol. (Sorry for this because simply translating the post into English may not be enough for you to understand). 纯玩梗 语言现象背后蕴含的知识 皮钦语 (pidgin) 大家对那些 1.言语中不时夹杂着英文单","title":"一些语言学的梗和有意思的知识"},{"content":"The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.\nCircuits Discovery Methods basic activation patching (causal mediation/interchange interventions\u0026hellip;) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? \u0026#x1f52d; resources (ROME) Locating and Editing Factual Associations in GPT Attribution patching: Activation patching at industrial scale (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability Attribution Patching Outperforms Automated Circuit Discovery AtP*: An efficient and scalable method for localizing llm behaviour to components Causal Scrubbing: a method for rigorously testing interpretability hypotheses new Using SAE Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models Automatically Identifying Local and Global Circuits with Linear Computation Graphs Contextual Decomposition Mechanistic Interpretation through Contextual Decomposition in Transformers Edge Pruning ? Finding Transformer Circuits with Edge Pruning Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning Evaluation lack of ground truth\nhuman interpretability automatic faithfulness (see this)\nhow much of the full model’s performance can a circuit account for. completeness computational efficiency Issues ablation methods: dropout out is also an ablation, so does zero ablation work? superposition, need the help of SAE? Dictionary Learning SAE\nTraining and optimization proper SAE width dead neurons \u0026#x1f52d; resources Improving Dictionary Learning with Gated Sparse Autoencoders Scaling and evaluating sparse autoencoders Evaluation human auto \u0026#x1f52d; resources (Anthropic, 2024-8, contrastive eval \u0026amp; sort eval) Interpretability Evals for Dictionary Learning (RAVEL) RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations Language models can explain neurons in language models Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control Analysis feature splitting Applications SAE + feature discovery \u0026#x1f52d; resources Sparse Autoencoders Find Highly Interpretable Features in Language Models Anthropic research Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet SAE + circuit discovery \u0026#x1f52d; resources Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models SAE + explain model components Sparse Autoencoders Work on Attention Layer Outputs Interpreting Attention Layer Outputs with Sparse Autoencoders Transcoders Find Interpretable LLM Feature Circuits SAE + explain model behaviors The Geometry of Concepts: Sparse Autoencoder Feature Structure SAE + model steering Steering vectors activation steering Steering Language Models With Activation Engineering (lesswrong) Steering GPT-2-XL by adding an activation vector Steering Llama 2 via Contrastive Activation Addition Inference-Time Intervention: Eliciting Truthful Answers from a Language Model feature steering Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Evaluating feature steering: A case study in mitigating social biases representation engineering Representation Engineering: A Top-Down Approach to AI Transparency Others Mechanistically Eliciting Latent Behaviors in Language Models Explain Model Components explain neurons, attention heads and circuits\nExplain neurons \u0026#x1f52d; resources Finding Neurons In A Haystack LatentQA: Teaching LLMs to Decode Activations Into Natural Language Language models can explain neurons in language models Multimodal Neurons in Artificial Neural Networks Finding Safety Neurons in Large Language Models Explain attention heads different heads in one layer/heads in different layer -\u0026gt; grammer/semantic feats\n\u0026#x1f52d; resources Copy Suppression: Comprehensively Understanding An Attention Head Explain circuits understand specific circuits on the subspace level\n(IOI) Interpretability in The Wild: A Circuit For Indirect Object Identification in GPT-2 Small What Do the Circuits Mean? A Knowledge Edit View (DAS) Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations LLM Circuit Analyses Are Consistent Across Training and Scale Explain layernorm On the Nonlinearity of Layer Normalization Others The Quantization Model of Neural Scaling Explain Model Behaviors Feature representations linear representations theory Linear Explanations for Individual Neurons multilingual representations Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs Emerging Cross-lingual Structure in Pretrained Language Models Probing the Emergence of Cross-lingual Alignment during LLM Training Exploring Alignment in Shared Cross-lingual Spaces mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models? Probing LLMs for Joint Encoding of Linguistic Categories Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure multimodal representations Interpreting CLIP\u0026rsquo;s Image Representation via Text-Based Decomposition safety reprs \u0026#x1f52d; resources Linear Representations of sentiment in large language models nonlinear representations Not All Language Model Features Are Linear Model capabilities training dynamics\nhow post-training affect model representations and mechanisms\nfine-tuning\nMechanistically analyzing the effects of fine-tuning on procedurally defined tasks Understanding Catastrophic Forgetting in Language Models via Implicit Inference Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking in-context learning\nbasic In-context Learning and Induction Heads What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation bad in-context learning (learn wrong things) Overthinking The Truth: Understanding How language Models Process False Demonstrations chain of thought (COT)\nhow and why step by step?\nzero-shot COT ???\nanalyses Iteration Head: A Mechanistic Study of Chain-of-Thought How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation Do Large Language Models Latently Perform Multi-Hop Reasoning? unfaithful COT Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting Does the model already know the answer while reasoning, or the model really has a goal? instruction following\nhow does reinforcement learning change the inside of a model? understand RL at mechanistic level high efficient RLxF memorization \u0026amp; generalization\nExtractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts planning\nEvaluating Cognitive Maps and Planning in Large Language Models with CogEval reasoning\nWhat Do Learning Dynamics Reveal About Generalization in LLM Reasoning? duplication\nself-repair\nThe Hydra Effect: Emergent Self-repair in Language Model Computations Explorations of Self-Repair in Language Models grokking\nProgress Measures For Grokking Via Mechanistic Interpretability Towards Understanding Grokking: An Effective Theory of Representation Learning phase transition\nIn-context Learning and Induction Heads double descent\nDeep double descent Narrow tasks counting greater-than How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model Indirect Object Indentification (IOI) Interpretability in The Wild: A Circuit For Indirect Object Identification in GPT-2 Small Interpretable model structure also called intrinsic interpretability\n(SoLU) Softmax Linear Units Application AI alignment 3H (helpful, honest, harmless) Avoid bias and harmful behaviors\nconcept-based interpretability representation-based interpretability red-teaming perturbations\nbackdoor detection, red-teaming, capability discovery\nanomaly detection\nbackdoor detection\nSleeper Agents: Training Deceptive LLMs that Persist Through Safety Training SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks Mechanistic anomaly detection and ELK A gentle introduction to mechanistic anomaly detection Concrete empirical research projects in mechanistic anomaly detection refuse to request \u0026amp; jailbreak circuit; SAE; steering vector (anti-refusal)\nmeasures (repr engineering) Improving Alignment and Robustness with Circuit Breakers (steering) Refusal in Language Models Is Mediated by a Single Direction (steering) Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models (training) Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications analyses Many-shot jailbreaking Jailbroken: How Does LLM Safety Training Fail? Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs (vlm) Visual Adversarial Examples Jailbreak Aligned Large Language Models (vlm) Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models (vlm) Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything evaluation \u0026amp; benchmark Jailbreak prompts finding on Twitter JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models power-seeking\nParametrically Retargetable Decision-Makers Tend To Seek Power social injustice prejudice, gender bias: doctor \u0026amp; nurse, discrimination\ntraining dynamics; dataset; gradient descent; SAE circuits\nEvaluating feature steering: A case study in mitigating social biases Prejudice and Volatility: A Statistical Framework for Measuring Social Discrimination in Large Language Models Unveiling Gender Bias in Large Language Models: Using Teacher’s Evaluation in Higher Education As an Example deception\ndishonesty\nAlignment faking in large language models Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training Language Models Learn To Mislead Humans Via RLHF How do Large Language Models Navigate Conflicts between Honesty and Helpfulness? fine-tuning\nFine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To! reward hacking\nReward Hacking in Reinforcement Learning measurement tampering\nThe AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome.\nBenchmarks for Detecting Measurement Tampering persona drift\nMeasuring and Controlling Persona Drift in Language Model Dialogs other human values\nSycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models Reducing sycophancy and improving honesty via activation steering Modulating sycophancy in an RLHF model via activation steering Agency\nUnderstanding and Controlling a Maze-Solving Policy Network Parametrically Retargetable Decision-Makers Tend To Seek Power Avoiding Side Effects in Complex Environments \u0026#x1f52d; resources AI Alignment: A Comprehensive Survey Representation Engineering: A Top-Down Approach to AI Transparency Mechanistic Interpretability for AI Safety A Review Alignmnet theory Both alignment and interpretability are related to AI safety, so the mech interp tools are widely used in alignment research. I\u0026rsquo;ll put some good resources of alignment work here.\nqualitative work (findings, analyses, concepts, \u0026hellip;) *\nalignment representation\ninstrumental convergence\nshard theory\nProposed by Alex Turner (TurnTrout) and Quintin Pope\nThe Shard Theory of Human Values (lesswrong) The Shard Theory of Human Values Research Limitations Current work mainly focuses on Transformer-based models. Is transformer a inevitable model structure for generative language models? How can we use post-hoc methods as a guide for training a more interpretable and controllable model? Other Interpretability Fields Neural network interpretability Not necessarily a transformer-based model, maybe an lstm or simply a toy model\nTheories for DL foocker/deeplearningtheory Feature Learning Huang wei\u0026rsquo;s repo game (chess) Evidence of Learned Look-Ahead in a Chess-Playing Neural Network (Sokoban, planning) Planning behavior in a recurrent neural network that plays Sokoban geometry Reasoning in Large Language Models: A Geometric Perspective Other Surveys A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models Towards Uncovering How Large Language Model Works: An Explainability Perspective ","permalink":"https://Siriuslala.github.io/posts/mech_interp_research/","summary":"The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.\nCircuits Discovery Methods basic activation patching (causal mediation/interchange interventions\u0026hellip;) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? \u0026#x1f52d; resources (ROME) Locating and Editing Factual Associations in GPT Attribution patching: Activation patching at industrial scale (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability Attribution Patching Outperforms Automated Circuit Discovery AtP*: An efficient and scalable method for localizing llm behaviour to components Causal Scrubbing: a method for rigorously testing interpretability hypotheses new Using SAE Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models Automatically Identifying Local and Global Circuits with Linear Computation Graphs Contextual Decomposition Mechanistic Interpretation through Contextual Decomposition in Transformers Edge Pruning ?","title":"Possible Research Areas in Mechanistic Interpretability"},{"content":"\u0026#x1f3b6;Code in this post can be found at the jupyter notebook in my \u0026ldquo;saeExploration\u0026rdquo; repo.\nFind features that reflect positive emotions To find the features related to a specific emotion, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:\n1 2 3 4 5 prompt_happy = [\u0026#34;I\u0026#39;ll be on a vacation tomorrow and I\u0026#39;m so happy.\u0026#34;, \u0026#34;My mombrings home a new puppy and I\u0026#39;m so happy.\u0026#34;, \u0026#34;I\u0026#39;m so glad I got the job I wanted.\u0026#34;, \u0026#34;I feel so happy when I\u0026#39;m with my friends.\u0026#34;, \u0026#34;I\u0026#39;m so happy I got the promotion I wanted.\u0026#34;,] I choose to look for features that reflect happiness and sadness. Apart from that, I also wonder if the feature that reflects excitedness has something to do with the one that reflects happiness (they are alike from the semantic level at least.)\nFor a start, I inspected the residual stream in layer_7. The SAE we choose is gpt2-small-res-jb which hooks at the residual stream at the entrance of a layer. The prompts were fed into the model and the outputs were SAE activations. I checked the activations at the word “happy” for all the prompts and calculated the mean value of them. I visualized them as below:\nFigure 1: Feature activations at the keywords for happy emotion Obviously there are three SAE features that activate most actively on the happy emotion, and their feature ids are 2392, 9840 and 21753. I checked the feature 2392 in Neuronpedia and got its feature dashboard:\nFigure 2: SAE feature 2393 on the dashboard (https://neuronpedia.org/gpt2-small/7-res-jb/2392?embed=true\u0026embedexplanation=true\u0026embedplots=true\u0026embedtest=true\u0026height=300). Later I found features for sadness and excitedness in the same way. The top-3 features activating for “sad” and “excited” are [2045, 23774, 10866] and [8935, 9840, 3247] respectively.\nCompare the features related to happiness and excitedness I want to see the difference between \u0026ldquo;happy features\u0026rdquo; and \u0026ldquo;excited features\u0026rdquo; since they are both positive emotions. So I compared their top-3 features and only kept the features that activate on both “happy” and “excited”. They are visualized as below:\nFigure 3: Feature activations on both happy and excited emotions at the residual pre stream in layer_7. From the figure above, we can easily find out the two features shared by happiness and excitedness. It seems that these two features contain positive emotion concepts, which means they are close to each other on the semantic level.\nA deeper investigation into the features related to happiness From the previous result, we can see that there are 3 features that fire quite actively on happiness. Since they share similar semantic meanings, I guess that the representations of features activating on the same emotion (i.e. 2392, 9840 and 21753) have high similarities. To prove this, I try to inspect the representation of one prompt which expresses happiness and calculate the cosine similarities among representations of different features. Here the representations of different layers are calculated as below: $$ feat \\_ repr = W_{dec} * SAE \\_ activations $$ Note that the * operation is a dot product. The W_dec in the formula above is the decoder matrix of SAE with a shape of (d_sae, d_model). We know that according to the definition of dictionary learning, each vector in the dimension of d_sae corresponds to a base vector for an SAE feature. Each element in the SAE_activations can be seen as the intensity of the feature at that position. So by multiplying W_dec and SAE_activations we can get a representation of shape $ (d \\_ model,) $ which expresses the features in the vector space of SAE that is sparser and more interpretable than that of the original model.\nTo visualize the similarities among features, I choose to use the heatmap. Note that in order to make it clear, I mainly focus on features 2392, 9840 and 21753 and using negative sampling to get some other features for comparison. I randomly pick 6 features except for the three features mentioned above. The similarities are calculated and shown as below:\nFigure 4: Feature similarities on the word “happy” at the residual pre stream in layer_7. Obviously we can find that the three features that fire most actively on “happy” are more similar to each other (the top left 3*3 square), thus my hypothesis is proved intuitively.\nFeatures in different layers Figure 5: Feature activations on the word “happy” at the residual pre stream in 12 layers. \u0026ensp;\u0026ensp;Previously I inspected in the 8th layer of gpt2-small and found some emotional features. Now I want to know if similar features exist in other layers, and how they are related to each other. I inspect the autoencoder features in each layer and observe their activations on the word “happy”. The result is shown in Fig 5. We can find that: * Eachlayer has more than 3 features that activate on the happy emotion. * Thepositions of activating features are different from those in other layers. Though the positions of activating features are different, I guess it\u0026rsquo;s just the problem of feature orders. For example, the feature 7683 in layer_1 may be the same kind or even exactly the same feature as feature 13928 in layer_3, though the positions are different. In order to prove this, I choose to visualize the feature representations of SAE outputs. The result is shown below:\nFigure 6: Feature similarities on the word “happy” at the residual pre stream in 12 layers. From the figure above, I got some interesting points about the feature distribution:\nThe features in the first layer have lower similarities with those in later layers. I guess this is because the first layer gets limited information from previous layers, so it cannot express relatively complicated concepts like emotions. I think maybe the first layer contains some low level concepts that are not shown in this figure, which I will explore in the future. Afeature in a layer often corresponds to a feature in another layer. For example, feature 7683 in layer_1 corresponds to feature 13928 in layer_2, which has a similarity of 0.939. This means they are related to each other across different layers, sharing similar semantic meanings. Afeature in a layer tends to be more alike to features in nearby layers. For example, feature 7683 in layer_1 is more similar to feature 13928 in layer_2 than feature 2392 in layer_7, with a similarity of 0.939 to 0.825. I think it\u0026rsquo;s because the vector spaces of nearby layers are relatively close to each other. When two layers are far from each other, the difference between their vector spaces is significant due to a lot of linear and nonlinear manipulations between layers. Thus the features would share low similarities regardless of similar semantic meanings. ","permalink":"https://Siriuslala.github.io/posts/happy_feats/","summary":"\u0026#x1f3b6;Code in this post can be found at the jupyter notebook in my \u0026ldquo;saeExploration\u0026rdquo; repo.\nFind features that reflect positive emotions To find the features related to a specific emotion, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:\n1 2 3 4 5 prompt_happy = [\u0026#34;I\u0026#39;ll be on a vacation tomorrow and I\u0026#39;m so happy.\u0026#34;, \u0026#34;My mombrings home a new puppy and I\u0026#39;m so happy.","title":"Exploring Emotional Features in GPT2-Small"},{"content":"The purpose I write this blog Mechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challenges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc). Thus it\u0026rsquo;s a little bit difficult for people new to this area to figure out what researchers are really doing.\nTherefore I write this blog to give a brief introduction to mechanistic interpretability without so much of horrible concepts. The blog aims to help you understand the basic ideas, main directions and latest achievements of this field, providing a list of resources to help you get started at the same time!\nIf you really want to do some cool research as a beginner, I highly recommend the guide by Neel Nanda.\nWhat is Mechanistic interpretability? Speaking of AI research, neural network is the tool that is used most widely nowadays for its excellent representation and generalization ability. What does a neural network do? It receives an input and gives an output after some calculations. Specifically speaking, it usually gets the representations of an input and maps it to an expected output under a predefined computation graph. From my perspective, neural networks mainly care about two things: create representations to extract features and establish the relationship between the representations and the output.\nWhy is neural network so popular? An important reason is that the neural network can save a lot of time for researchers to manually design features. For example, for natural language processing (NLP) people often designed features like \u0026ldquo;the frequency of a word that appears\u0026rdquo; or \u0026ldquo;the co-occurrence probabilities\u0026rdquo; in the past. Manually designing features caused too much labor, so people choose to use neural networks to find features automatically. As for optimizing, they set a goal of minimizing the loss function and using backward propagation (BP) to update the parameters of the model. Thus neural networks free our hands and improve performance at the same time.\nAll is well, so why do we concern about interpretability? Though neural networks can extract a lot of features with a high efficiency, we cannot have a clear understanding of what the features really are. For example, we know that a filter with Laplacian operator can extract the edge of an image, but we don\u0026rsquo;t know what the features extracted by a convolution layer mean because the parameters of the filters inside are often randomly initialized and optimized using BP algorithm. As a result, features in neural networks are often ambiguous.\nWhy is interpretability important? Actually this statement is controversial because some people say interpretability is bullshit\u0026#x1f4a9;. I\u0026rsquo;m not angry about this. Anyway, people\u0026rsquo;s taste varies, just like many people enjoy Picasso\u0026rsquo;s abstract paintings while I don\u0026rsquo;t. Interpretability still lacks exploring so it\u0026rsquo;s now far from application, and that\u0026rsquo;s why some people look down on it. While it is this lack of exploration that excites me most because there are a lot of unknown things waiting for me to discover! Actually, Interpretability is a key component in the AI alignment cycle (see Figure 0). The goal of alignment is to \u0026ldquo;make AI systems behave in line with human intentions and values\u0026rdquo; and interpretability plays an important role in ensuring AI safety. For example, unwanted things like malicious text generated by a language model may be avoided using model steering (a trick played on the activations during the forward propagation). Besides, having a clear understanding of neural networks enables us to focus on the relevant part of a model to a specific task and perform fine-tuning in a more precise way (haha here is an ad for my project: circuit-tuning).\nFigure 0: The position of interpretability in the AI alignment cycle (from this survey)\rLast question: what is mechanistic interpretability? Let\u0026rsquo;s call it mech interp first because I\u0026rsquo;m really tired of typing the full name\u0026#x1f4a6;. There seems not to be a rigorous definition, but I here I want to quote the explanation by Chris Olah:\nMechanistic interpretability seeks to reverse engineer neural networks, similar to how one might reverse engineer a compiled binary computer program.\nAnother thing: there are various of categories of interpretability, such as studies from the geometry perspective or from the game theory and symbol system perspective, which can be found at ICML, ICLR, NeurlPS, etc. When we say mech interp, we often refer to the studies on Transformer-based generative language models now (though the research started before 2017) which will be introduced briefly in the next section. So before we start, let\u0026rsquo;s briefly go over the structure of Transformer first!\nFigure 1: The structure of Transformer (from Arena)\rFigure 2: The structure of the self-attention block in a Transformer block (from Arena)\rFigure 3: The structure of the MLP layer in a Transformer block (from Arena)\rFigure 4: The structure of the layer normalization in a Transformer block (from Arena)\rBasic ideas and research topics In this section, I\u0026rsquo;m gonna explain some terms for mech interp and help you understand the basic ideas of doing mech interp research. I\u0026rsquo;ll try to make it easy!\nNote that I\u0026rsquo;ll only introduce something that I think is important. If you wanna learn more about the concepts in mech interp, please refer to: A Comprehensive Mechanistic Interpretability Explainer \u0026amp; Glossary which is a very comprehensive guide for beginners that I strongly recommend!\nImportant concepts Features\nThere are a lot of definitions for features. Unfortunately none of the definitions above can be widely recognized, so it\u0026rsquo;s open for anyone who wants to seek for the essence of the features. Generally speaking, a feature is a property of an input which is interpretable or cannot be understand by humans. Practically speaking, a feature could be an activation value of a hidden state in a model (at least lots of work is focusing on this).\nHow to find a feature? Or how to know that the thing you find is likely to be a feature? Here I want to quote the concept of \u0026ldquo;the signal of structure\u0026rdquo; proposed by Chris Olah in the post of his thoughts on qualitative research:\nThe signal of structure is any structure in one\u0026rsquo;s qualitative observations which cannot be an artifact of measurement or have come from another source, but instead must reflect some kind of structure in the object of inquiry, even if we don\u0026rsquo;t understand it.\nJust like the discovery of cells under a microscope. The shape of the cells cannot be random noise but strong evidence for the structure of them.\nCircuits\nIf we view a language model as a directed acyclic graph (DAG) $M$ where nodes are terms in its forward pass (neurons, attention heads, embeddings, etc.) and edges are the interactions between those terms (residual connections, attention, projections, etc.), a circuit $C$ is a subgraph of $M$ responsible for some behavior. That means the components inside the circuit have a big influence on the output of the task, while the components outside the subgraph have almost no influence.\nFrom my perspective, a circuit is a path from an input to an output, just like the way between two hosts in the routing networks.\nFigure 5: The computational graph of a model (from Arena)\rSuperposition\nSuperposition is a hypothesis that models can represent more features than the dimensions they have.\nIdeally we expect that each neuron only corresponds to one feature, so we can investigate or even control the feature using the neuron reserved for it. But in practice we find that a neuron fires for more than one features, which is called the phenomenon of polysemanticity in neurons. We believe that we have more features than model dimensions, so we can also say that more than one neurons fire when a feature appears. That is to say, there is not a one-to-one correspondence between neurons and features.\nPrivileged basis\nIt\u0026rsquo;s a weird idea that I have some doubt on it (maybe I haven\u0026rsquo;t grasp the core idea of it\u0026hellip;).\nMy understanding: There are many vector spaces in a model, for example, the residual stream in a layer, the output of the ReLU in a MLP layer, etc. Each vector space can be seen as a representation. Given an input, we can get the hidden states in different vector spaces during the forward propagation of the model. If we could view neurons as directions which may correspond to features in a vector space, then we say there is a privileged basis in this vector space. That is to say, each value at a specific dimension is aligned with a neuron, and that value may be a interpretable feature (maybe not).\nNot all vector spaces in a model have privileged basis. The most accepted view is that privileged bases exist in attention patterns and MLP activations, but not in residual streams. A general law is that a privilege basis often appears with a elementwise nonlinear operation, for instance, ReLu, Softmax, etc. If the operations around a representation are all linear, then we say the basis in the representation is non-privileged. For example, the operations around a residual stream are often non-linear (e.g. $W_{in}$ and $W_{out}$ of a MLP layer which correspond to the \u0026ldquo;read\u0026rdquo; and \u0026ldquo;write\u0026rdquo; operation on the residual stream). If we apply a rotation matrix to the original operations to change the basis, then the result will be unchanged because In other words, something is a privileged basis if it is not rotation-independent, i.e. the nature of computation done on it means that the basis directions have some special significance. A privileged basis is a meaningful basis for a vector space. That is, the coordinates in that basis have some meaning, that coordinates in an arbitrary basis do not have. It does not, necessarily, mean that this is an interpretable basis.\na space can have an interpretable basis without having a privileged basis. In order to be privileged, a basis needs to be interpretable a priori - i.e. we can predict it solely from the structure of the network architecture.\nResearch techniques Circuits Discovery\nFinding the circuit for a specific task attracts the attention of lots of researchers. The thing we wanna do is to get the relevant components for a specific task. A naive idea is to test the components one by one using causal intervention: change the value of one component while keeping others unchanged, and check if it influences the output. This technique is also called ablation or knockout.\nTo achieve this, we have two possible ways: denoising (find useful components) and noising (delete unuseful components). We usually prepare a clean prompt which is relevant to the task (results in a correct answer) and a corrupted prompt which has nothing to do with the task. Before finding circuits, the two prompts are fed into the model separately to get a clean run and a corrupted run.\nIf we use denoising, at each step we replace (patch) the value of a component in the corrupted run with that in the clean run. If the output is closer to the correct answer under a specific metric (e.g. KL divergence or logit difference), then we add the component into the circuit. If we use noising, then we should replace a component in the clean run with that in the corrupted run. If the output is almost unchanged under a threshold, then we regard the component as useless and delete it. Generally speaking, denoising is better than noising. To understand this, I want to quote a line in Arena: noising tells you what is necessary, denoising tells you what is sufficient.\nSeveral techniques in this area:\nactivation patching (aka causal mediation/interchange interventions\u0026hellip;) A method for circuits discovery that take nodes into consideration. path patching A variant of activation patching that also take edges into consideration to study which connections between components matter. For a pair of components A and B, we patch in the clean output of A, but only along paths that affect the input of component B. While in activation patching, all the subsequent components after A are affected. attribution patching An approximation of activation patching using a first-order Taylor expansion on the metric. This method is used to speed up circuits finding. Figure 6: Comparison of activation patching and path patching (from Arena)\rThe difference between activation patching and path patching are shown in Figure 6. In activation patching, we simply patch the node $D$ with $D\u0026rsquo;$, so the nodes after $D$ ($H, G$ and $F$) are affected. While in path patching, we patch edges rather than nodes. For example, we only want to patch the edge $D \\to G$, which means the only change is the information from node $D$ to node $G$. As a result, only $G$ and $F$ are affected while $H$ isn\u0026rsquo;t.\n\u0026#x1f52d; Recommended papers: (ROME) Locating and Editing Factual Associations in GPT (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability (attribution patching) Attribution patching: Activation patching at industrial scale (IOI) INTERPRETABILITY IN THE WILD: A CIRCUIT FOR INDIRECT OBJECT IDENTIFICATION IN GPT-2 SMALL Dictionary Learning\nDictionary Learning aims to deal with the problem of superposition. The idea is like compression sensing in the field of signal processing and is discussed in this article. The implementation of dictionary learning is to train a sparse autoencoder (SAE).\nAn autoencoder consists of an encoder and a decoder. The encoder receives an input and compresses it to a lower dimension, and the decoder maps the hidden representation to the original input. The goal of the autoencoder is to get the representation of the input while compressing it. The autoencoder is optimized using the reconstruction loss.\nCompared with the autoencoedr, the dimension of the hidden representation in SAE is always higher than that of the input, which means the SAE does something completely opposite to the autoencoder. The idea behind is that the model dimension is smaller than the number of features to represent. The model may use superposition to make full use of limited neurons to represent more features. To get one-to-one correspondence between neurons and features, we map the representation to a higher dimensional vector space with SAE encoder. Once we get the representation in SAE (let\u0026rsquo;s call it sparse features), we maps it back to the original input with SAE decoder.\nIn practice, any hidden state in a model can be studied using SAE. For example, when we want to get the sparse features of the activations $h$ in a MLP layer. We can do as follows:\n$$ z = ReLU(W_{enc}h + b_{enc}) $$ $$ h^{\\prime} = W_{dec}z + b_{dec} $$\n$$ loss = \\mathbb{E}_{h}\\left[||h-h^{\\prime}||_{2}^{2} + \\lambda||z||\\right] $$\nNote that $ h = [h_{1}, h_{2},\u0026hellip;,h_{n}]^{T} \\in \\mathbb{R}^{n\\times1} $ is a hidden state with $n$ dimensions, and each $h_{i} \\in H$ is the value of a specific dimension $i$. $W_{enc} \\in \\mathbb{R}^{m\\times n}$ maps the hidden state to a new vector space with dimension $m\u0026gt;n$, $W_{dec} \\in \\mathbb{R}^{n\\times m}$ maps the sparse features back to the original shape, $ b_{enc} \\in \\mathbb{R}^{n} $ and $ b_{dec} \\in \\mathbb{R}^{n} $ are learned bias. The loss function consists of two parts: the MSE loss as the reconstruction loss and L1 norm with a coefficient $\\lambda$ to encourage the sparsity of feature activations. It is the regularization term that separates SAE from ordinary autoencoders, so as to discourage superposition and encourage monosemanticity.\nTo better understanding the encoder and decoder in SAE, we can write a sparse feature $ f_{i} $ as an element of $ z $ :\n$$ f_{i}(h) = z_{i} = ReLU(W^{enc}_{i,.}\\cdot h + b^{enc}_{i}) $$\nEach sparse feature $ f_{i} $ is calculated using row $i$ of the encoder weight matrix. As for decoder, we can write $ h^{\\prime} $ as:\n$$ h^{\\prime} = \\sum_{i=1}^{m}f_{i}(h) \\cdot W^{dec}_{.,i} + b_{dec} $$\n;The reconstructed activation $h^\\prime$ can been seen as a linear addition of all the features. Each column of the decoder matrix corresponds to a feature, so we call it a \u0026ldquo;feature direction\u0026rdquo;. Note that sometimes the L1 norm term in the loss function can be replaced by $ \\lambda\\sum_{i=1}^{m}f_{i}(h)||W^{dec}_{.,i}||_{2} $ which places a constraint to the decoder weights to reduce ambiguity in the addition operation (we want only one or a few features to be large).\nFor simplicity, The details of the model structure, training method and evaluation will not be shown here.\n\u0026#x1f52d; Recommended papers: Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Model Steering\nA useful technique for eliciting certain model behaviors in a mechanistic way.\n\u0026#x1f52d; Recommended papers: Activation Addition: Steering Language Models Without Optimization (lesswrong) Steering GPT-2-XL by adding an activation vector Steering Llama 2 via Contrastive Activation Addition Mechanistically Eliciting Latent Behaviors in Language Models Research Areas Figure 7: The route of mech interp (from transformer-circuits.pub/2024/july-update/)\rTheory\nUnderstand model components Understand model behaviors Application\ninterpretable model structure AI alignment\nAvoid bias and harmful behaviors Some Useful Resources Here I list some resources that would be helpful for you to get started quickly in the field.\nTutorials Arena A tutorial created and maintained by Callum McDougall et al, providing a guided path for anyone who finds themselves overwhelmed by the amount of technical AI safety content out there. Neel Nanda\u0026rsquo;s Tutorial Neel\u0026rsquo;s tutorial for mech interp. Neel Nanda\u0026rsquo;s Quickstart Guide A quick start for mech interp. Neel Nanda\u0026rsquo;s remommended papers Some classic and important papers for mech interp. Neel Nanda\u0026rsquo;s problems v1 Neel\u0026rsquo;s old questions for mech interp. Neel Nanda\u0026rsquo;s problems v2 Neel\u0026rsquo;s 200 new questions for mech interp. Alignment Research Field Guide (by the MIRI team) Frameworks and Libraries TransformerLens A library maintained by Bryce Meyer and created by Neel Nanda. SAELens Originates from TransformerLens, and is separated from it because of the popularity and importance of SAE. CircuitsVis A good tool for visualizing LLMs. Plotly A good tool for plotting. Forums and Communities Transformer Circuits Thread The research posts of Anthropic alignment group. Lesswrong AI Alignment Forum Companies, Institutes, Labs and Programs Anthropic DeepMind FAR Apollo RedWood CHAI (UC Berkeley) MIRI (UC Berkeley) Alignment Research Center (ARC) MATS The ML Alignment \u0026amp; Theory Scholars, an independent research and educational seminar program that connects talented scholars with top mentors in the fields of AI alignment, interpretability, and governance. SPAR Supervised Program for Alignment Research Blogs Chris Olah Neel Nanda Arthur Conmy Andy Zou Jacob Steinhardt Trenton Bricken Callum Mcdougall Alex Turner(TurnTrout) \u0026hellip;\n","permalink":"https://Siriuslala.github.io/posts/mech_interp_resource/","summary":"The purpose I write this blog Mechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challenges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc).","title":"A Brief Introduction to Mechanistic Interpretability Research"},{"content":"Who am I? Hi~ I\u0026rsquo;m Yueyan Li, a researcher(still a student now) in China. I\u0026rsquo;m now at the Center of Intelligence Science and Technology, BUPT. My research focuses on machine learning, deep learning and natural language processing, mainly interpretability for neural networks and cognitive science now! Here is my CV.\nExcept from my research area, I\u0026rsquo;m also interested in communication engineering\u0026#x1f4fb; which was my major when I was an undergraduate. If you like that, feel free to share something interesting together~\nApart from technologies, I\u0026rsquo;m a lover for nature. I like the mountains, the rivers, the forests\u0026hellip;if you like hiking outdoors, don\u0026rsquo;t forget me! Also, I\u0026rsquo;m a Bboy\u0026#x270c;\u0026#xfe0f;\u0026#x1f918;. If you like breaking or any kind of street dance, just call me\u0026#x1f44b;!\nSometimes I paint as a waste of time, though I\u0026rsquo;m not professional.\nBelow are some of my interests. If you are interested in some of them, please reach me at any time~\nmachine learning, deep learning communication engineering street dance music (Buyi Mao, Eason, Huazhou, Shen Zhou / Avicii, Coldplay, / \u0026hellip;) Linguistics and Languages hiking, mountain climbing\u0026hellip; Street fitness (push-ups, muscle-ups\u0026hellip;) \u0026hellip; About my nickname I use the name Sirius/Sirius Jr./siriuslala\u0026hellip; everywhere on the Internet. This originates from Sirius Black - my favourite character in the Harry Potter series.\nHe is not only an extraodinary wizard with wild and intractable appearence but also the godfather of Harry Potter - the only family alive for Harry who had brought him warmth that is hard to replace. That\u0026rsquo;s why I admire him.\n","permalink":"https://Siriuslala.github.io/about/","summary":"About myself","title":"About Myself"},{"content":"","permalink":"https://Siriuslala.github.io/faq/","summary":"faq","title":"faq"},{"content":"\rPrevious\rNext \u0026nbsp; \u0026nbsp;\r/ [pdf]\rView the PDF file here.\r","permalink":"https://Siriuslala.github.io/helper/","summary":"\rPrevious\rNext \u0026nbsp; \u0026nbsp;\r/ [pdf]\rView the PDF file here.\r","title":"Yueyan Li"}]