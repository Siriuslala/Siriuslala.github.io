[{"content":"Resource Interpretability for MLLMs survey A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models Sparks of Explainability Recent Advancements in Explaining Large Vision Models Awesome LMMs Mechanistic Interpretability probing Probing Multimodal Large Language Models for Global and Local Semantic Representations CLIP Interpreting CLIP\u0026rsquo;s Image Representation via Text-Based Decomposition Interpreting the Second-Order Effects of Neurons in CLIP CLIP不同层 information flow *What\u0026rsquo;s in the Image? A Deep-Dive into the Vision of Vision Language Models The Narrow Gate: Localized Image-Text Communication in Vision-Language Models Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference **Cross-modal Information Flow in Multimodal Large Language Models *From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks others **Towards interpreting visual information processing in vision-language models demo **(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models (dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability \u0026amp; Open Problems Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space visualization Transformer Interpretability Beyond Attention Visualization Analyses on MLLMs dataset\n(GQA) GQA:ANewDataset for Real-World Visual Reasoning and Compositional Question Answering https://cs.stanford.edu/people/dorarad/gqa/index.html image token compression\n(multimodal image token compression)\n*AdaFV: Rethinking of Visual-Language alignment for VLM acceleration (FasterVLM) [CLS] Attention is All You Need for Training-FreeVisual Token Pruning: Make VLM Inference Faster Sparsevlm: Visual token sparsification for efficient vision-languag (FastV) An image is worth 1/2 tokens after layer 2: Plug-and-PLay Acceleration for VLLM Inference LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token *Inference Optimal VLMs Need Only One Visual Token but Larger Models TokenPacker: Efficient Visual Projector for Multimodal LLM Matryoshka Multimodal Models Matryoshka Query Transformer for Large Vision-Language Models FlashSloth: Lightning Multimodal Large Language Models via Embedded Visual Compression FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding spatial\ngood Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas (ViT+LLM \u0026gt; ViT) Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision Encoder ! Learning Visual Composition through Improved Semantic Guidance (prompt-based) Things not Written in Text: Exploring Spatial Commonsense from Visual Signals (prompt-based) Does CLIP Bind Concepts? Probing Compositionality in Large Image Models Probing the Role of Positional Information in Vision-Language Models dataset (ARO) When and why vision-language models behave like bags-of-words, and what to do about it? (Whatsup) What’s “up” with vision-language models? Investigating their struggle with spatial reasoning https://github.com/amitakamath/whatsup_vlms (VSR) Visual Spatial Reasoning https://github.com/cambridgeltl/visual-spatial-reasoning (GQA) GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering https://cs.stanford.edu/people/dorarad/gqa/index.html evaluation Can Multimodal Large Language Models Understand Spatial Relations? SpaRE:Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data SPHERE: Unveiling Spatial Blind Spots in Vision-Language Models Through Hierarchical Evaluation REC (看 related work) Exploring Spatial Language Grounding Through Referring Expressions (文本引导) ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension attention\nLVLM-Intrepret: An Interpretability Tool for Large Vision Language Models MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs Lost in the middle: How language models use long contexts Efficient streaming language models with attention sinks (delimiters) Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and Joint Low-Rank Projection MagicPIG: LSH Sampling for Efficient LLM Generation Label words are anchors: An information flow perspective for understanding in-context learning POS “闭门造车”之多模态思路浅谈（三）：位置编码 Transformer Language Models without Positional Encodings Still Learn Positional Information Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings Length Generalization of Causal Transformers without Position Encoding Sensitivity Meets Sparsity: The Impact of Extremely Sparse Parameter Patterns on Theory-of-Mind of Large Language Models Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding hallucination\nsurvey Hallucination of Multimodal Large Language Models: A Survey *Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs Interpreting and editing vision-language representations to mitigate hallucinations Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference Debiasing Multimodal Large Language Models image tokens\nVision Transformers Need Registers Words or Vision: Do Vision-Language Models Have Blind Faith in Text? Papers vlm\nbasic (Transformer) Attention is all you need (VIT) An Image is Worth 16x16 Words: Transformers for Image Recognition fat Scale (CLIP) Learning Transferable Visual Models From Natural Language Supervision (PACL) Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning (DINO) Emerging Properties in Self-Supervised Vision Transformers DINOv2: Learning Robust Visual Features without Supervision (BLIP-2) BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond Qwen2-VL: Enhancing Vision-Language Model\u0026rsquo;s Perception of the World at Any Resolution Qwen2.5-VL Technical Report (DFN) Data Filtering Networks LLaVA系列 (LLaVA) Visual Instruction Tuning (LLaVA-1.5) Improved Baselines with Visual Instruction Tuning InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks (InternVL 1.5) How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites (InternVL 2.5) Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling resolution Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution Swin Transformer: Hierarchical Vision Transformer using Shifted Windows DualFocus: A Unified Framework for Integrating Positive and Negative Descriptors in Text-based Person Retrieval generative models\nimage\nbasic (T2I)\n(DDPM) Denoising Diffusion Probabilistic Models (DDIM) Denoising Diffusion Implicit Models (Classifier-guided) Diffusion Models Beat GANs on Image Synthesis (Classifier-free) Classifier-free diffusion guidance (VQVAE) Taming transformers for high-resolution image synthesis (DIT) Scalable Diffusion Models with Transformers (LDM) High-resolution image synthesis with latent diffusion models (GLIDE) GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models (Imagen) (DALL-E) Zero-Shot Text-to-Image Generation (DALL-E-2) Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E-3) Improving Image Generation with Better Captions (ControlNet) Adding Conditional Control to Text-to-Image Diffusion Models Consistency Models (LCM) Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference (SDXL Turbo) Adversarial Diffusion Distillation (EDM) Elucidating the Design Space of Diffusion-Based Generative Models Flow Matching for Generative Modeling (Stable Diffusion 3) Scaling Rectified Flow Transformers for High-Resolution Image Synthesis image editing\nInstructPix2Pix: Learning to Follow Image Editing Instructions Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry Diffusion Model-Based Image Editing: A Survey video\nsurvey\nThe Dawn of Video Generation: Preliminary Explorations with SORA-like Models generation\nVideo Diffusion Models Latte: Latent Diffusion Transformer for Video Generation Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning I2V\nvideo editing\nAnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks TokenFlow: Consistent Diffusion Features for Consistent Video Editing STABLEV2V: Stablizing Shape Consistency in Video-to-Video Editing VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing AGI\nworld models navigation\nNavigation World Models spatial reasoning\nphysical reasoning\nDenoising Hamiltonian Network for Physical Reasoning Analysis\nConceptAttention: Diffusion Transformers Learn Highly Interpretable Features Model capabilities\n","permalink":"https://Siriuslala.github.io/posts/mm_interp/","summary":"\u003ch2 id=\"resource\"\u003eResource\u003c/h2\u003e\n\u003ch3 id=\"interpretability-for-mllms\"\u003eInterpretability for MLLMs\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003esurvey\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2502.17516\"\u003eA Survey on Mechanistic Interpretability for Multi-Modal Foundation Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2502.01048v1\"\u003eSparks of Explainability Recent Advancements in Explaining Large Vision Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/itsqyh/Awesome-LMMs-Mechanistic-Interpretability?tab=readme-ov-file#-blog\"\u003eAwesome LMMs Mechanistic Interpretability\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eprobing\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2402.17304\"\u003eProbing Multimodal Large Language Models for Global and Local Semantic Representations\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCLIP\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2310.05916\"\u003eInterpreting CLIP\u0026rsquo;s Image Representation via Text-Based Decomposition\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2406.04341\"\u003eInterpreting the Second-Order Effects of Neurons in CLIP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.cnblogs.com/LittleHenry/p/18688886\"\u003eCLIP不同层\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003einformation flow\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2411.17491\"\u003e*What\u0026rsquo;s in the Image? A Deep-Dive into the Vision of Vision Language Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2412.06646\"\u003eThe Narrow Gate: Localized Image-Text Communication in Vision-Language Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"\"\u003ePerformance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2503.13108\"\u003eLifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2411.18620\"\u003e**Cross-modal Information Flow in Multimodal Large Language Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2406.06579\"\u003e*From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eothers\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2410.07149\"\u003e**Towards interpreting visual information processing in vision-language models\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://clementneo.com/llava_logit_lens/\"\u003edemo\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2406.04236\"\u003e**(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.alignmentforum.org/posts/kobJymvvcvhbjWFKe/laying-the-foundations-for-vision-and-multimodal-mechanistic\"\u003e(dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability \u0026amp; Open Problems\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2203.14680\"\u003eTransformer feed-forward layers build predictions by promoting concepts in the vocabulary space\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003evisualization\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2012.09838\"\u003eTransformer Interpretability Beyond Attention Visualization\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"analyses-on-mllms\"\u003eAnalyses on MLLMs\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edataset\u003c/strong\u003e\u003c/p\u003e","title":"Interpretability for Multimodal Models"},{"content":" ArXiv(old version): https://arxiv.org/pdf/2502.06106\n","permalink":"https://Siriuslala.github.io/posts/circuit_tuning/","summary":"\u003c!-- The [paper](/pdfs/circuit_tuning) is here. --\u003e\n\u003cp\u003eArXiv(old version): \u003ca href=\"https://arxiv.org/pdf/2502.06106\"\u003ehttps://arxiv.org/pdf/2502.06106\u003c/a\u003e\u003c/p\u003e","title":"Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks"},{"content":"This post is written in Chinese. If you don\u0026rsquo;t know Chinese, you can learn it lol. (Sorry for this because simply translating the post into English may not be enough for you to understand).\n纯玩梗 语言现象背后蕴含的知识 皮钦语 (pidgin) 大家对那些 1.言语中不时夹杂着英文单词 2.装/凡尔赛 的人表现出一种厌恶。例如，下面是某恋综里的一段名场面：\n————————————————————————————————\n\u0026hellip;\n男A：\u0026ldquo;可能因为我学校在伦敦，所以\u0026hellip;\u0026rdquo; 女A：\u0026ldquo;哦我也是。我是高中在 York (Yorkshire) 附近，然后本科研究生在伦敦\u0026rdquo;\n男B：\u0026ldquo;我喜欢 nə —— uhh —— Southampton\u0026rdquo;\n女A：\u0026ldquo;在海边！\u0026rdquo;\n男B：\u0026ldquo;对超美！\u0026rdquo;\n男A：\u0026ldquo;然后，我也挺喜欢曼 —— emmm —— Manchester 的\u0026rdquo;\n\u0026hellip;\n————————————————————————————————\n好的嘲讽完毕，点到为止。\n实际上语言学里的语言演化领域有一个名词叫皮钦语（pidgin），也被称作混杂语，是语言发展的一个阶段，指在没有共同语言而又急于进行交流的人群中间产生的一种混合语言，属于不同语言人群的联系语言。感兴趣详见：皮克特语到四阶皮钦语——苏格兰语言迭代史。\n笔者印象里比较深的，一个是洋泾浜语，即流行在20世纪初的上海滩的一种英汉混杂语；另一个是协和语，指流行于伪满洲国的汉语、日语的混合语言。两者都是殖民地和半殖民地文化的产物。以下是一些例子：\n洋泾浜语：\u0026ldquo;来是\u0026rsquo;康姆\u0026rsquo;（come），去是\u0026rsquo;狗\u0026rsquo;（go）\u0026quot;（详见汪仲贤所著《上海俗话图说》；dbq耳边想起赵丽蓉老师的小品） 协和语：\u0026ldquo;你的帮我，我的钱的大大的给。\u0026rdquo; 此外，刘慈欣《三体》中也提到了由汉语和英语结合而成的新语言（舰队混合语）。\n其实，正常的、真诚的表达往往不会惹人厌烦，例如：\n\u0026ldquo;我希望能够做点兼职，至少能够 cover 掉我一部分的学费\u0026rdquo; \u0026ldquo;这个 checkpoint 我测了一下，跟 baseline 相比有很大的优势，但是跟 SOTA 比还是有很大的差距\u0026rdquo; \u0026ldquo;哈喽哈喽，你好呀~\u0026rdquo; 语言磨蚀 ","permalink":"https://Siriuslala.github.io/posts/%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E5%AD%A6%E7%9A%84%E6%A2%97%E5%92%8C%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%9F%A5%E8%AF%86/","summary":"\u003cp\u003eThis post is written in Chinese. If you don\u0026rsquo;t know Chinese, you can learn it lol. (Sorry for this because simply translating the post into English may not be enough for you to understand).\u003c/p\u003e\n\u003ch2 id=\"纯玩梗\"\u003e纯玩梗\u003c/h2\u003e\n\u003ch2 id=\"语言现象背后蕴含的知识\"\u003e语言现象背后蕴含的知识\u003c/h2\u003e\n\u003ch3 id=\"皮钦语-pidgin\"\u003e皮钦语 (pidgin)\u003c/h3\u003e\n\u003cp\u003e大家对那些 1.言语中不时夹杂着英文单词 2.装/凡尔赛 的人表现出一种厌恶。例如，下面是某恋综里的一段名场面：\u003c/p\u003e","title":"一些语言学的梗和有意思的知识"},{"content":"\u0026#x1f4a1; This post is mainly focused on text models. For multi-modal models, please refer to this post.\nThe Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.\nCircuit Discovery Methods basic activation patching (causal mediation/interchange interventions\u0026hellip;) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? \u0026#x1f52d; resources inspirition Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned (ROME) Locating and Editing Factual Associations in GPT Attribution patching: Activation patching at industrial scale (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability Attribution Patching Outperforms Automated Circuit Discovery AtP*: An efficient and scalable method for localizing llm behaviour to components Causal Scrubbing: a method for rigorously testing interpretability hypotheses new Using SAE Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models Automatically Identifying Local and Global Circuits with Linear Computation Graphs Contextual Decomposition Mechanistic Interpretation through Contextual Decomposition in Transformers Edge Pruning ? Finding Transformer Circuits with Edge Pruning Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning Evaluation lack of ground truth\nhuman interpretability automatic faithfulness (see this)\nhow much of the full model’s performance can a circuit account for.\ncompleteness\ncomputational efficiency Hypothesis Testing the Circuit Hypothesis in LLMs Issues ablation methods: dropout out is also an ablation, so does zero ablation work? superposition, need the help of SAE? Dictionary Learning SAE\nTraining and optimization proper SAE width dead neurons \u0026#x1f52d; resources A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models Circuit Tracing: Revealing Computational Graphs in Language Models Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders Sparse Crosscoders for Cross-Layer Features and Model Diffing Open Source Replication of Anthropic’s Crosscoder paper for model-diffing Transcoders Find Interpretable LLM Feature Circuits Scaling and evaluating sparse autoencoders Improving Dictionary Learning with Gated Sparse Autoencoders Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Evaluation human auto \u0026#x1f52d; resources SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability website (Anthropic, 2024-8, contrastive eval \u0026amp; sort eval) Interpretability Evals for Dictionary Learning (RAVEL) RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations Language models can explain neurons in language models Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control Analysis A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders Are Sparse Autoencoders Useful? A Case Study in Sparse Probing Applications SAE + feature discovery \u0026#x1f52d; resources Sparse Autoencoders Find Highly Interpretable Features in Language Models Anthropic research Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet SAE + circuit discovery \u0026#x1f52d; resources Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models SAE + explain model components Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors Sparse Autoencoders Work on Attention Layer Outputs Interpreting Attention Layer Outputs with Sparse Autoencoders SAE + explain model behaviors The Geometry of Concepts: Sparse Autoencoder Feature Structure SAE + model steering SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders Steering vectors activation steering Steering Language Models With Activation Engineering (lesswrong) Steering GPT-2-XL by adding an activation vector Steering Llama 2 via Contrastive Activation Addition Inference-Time Intervention: Eliciting Truthful Answers from a Language Model Function vectors in large language models feature steering Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Evaluating feature steering: A case study in mitigating social biases representation engineering Representation Engineering: A Top-Down Approach to AI Transparency Others LUNAR: LLM Unlearning via Neural Activation Redirection Mechanistically Eliciting Latent Behaviors in Language Models Model Diffing Stage-wise fine-tuning fine-tuning interp fine-tuning\nscaling Stage-Wise Model Diffing Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks Understanding Catastrophic Forgetting in Language Models via Implicit Inference Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To! intrinsic dimension Measuring the Intrinsic Dimension of Objective Landscapes Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning Dataset-wise Modeldiff: A framework for comparing learning algorithms Algorithm-wise Adversarial robustness as a prior for learned representations Representation equivariance meta-SNE Visualizing Representations: Deep Learning and Human Beings model stitching Understanding image representations by measuring their equivariance and equivalence Revisiting model stitching to compare neural representations SVCCA and similar methods SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability others analyses The Platonic Representation Hypothesis theory When Representations Align: Universality in Representation Learning Dynamics Explain Model Components explain neurons, attention heads and circuits\nExplain neurons \u0026#x1f52d; resources Finding Neurons In A Haystack LatentQA: Teaching LLMs to Decode Activations Into Natural Language Language models can explain neurons in language models Multimodal Neurons in Artificial Neural Networks Finding Safety Neurons in Large Language Models Explain attention heads different heads in one layer/heads in different layer -\u0026gt; grammar/semantic feats\nattention pattern\nAnalyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention 为什么Transformer 需要进行 Multi-head Attention？ special heads\nCopy Suppression: Comprehensively Understanding An Attention Head Explain circuits understand specific circuits on the subspace level\n(IOI) Interpretability in The Wild: A Circuit For Indirect Object Identification in GPT-2 Small What Do the Circuits Mean? A Knowledge Edit View (DAS) Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations LLM Circuit Analyses Are Consistent Across Training and Scale Explain layernorm On the Nonlinearity of Layer Normalization Others The Quantization Model of Neural Scaling Explain Model Behaviors Feature representations linear representations theory Linear Explanations for Individual Neurons multilingual representations Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs Emerging Cross-lingual Structure in Pretrained Language Models Probing the Emergence of Cross-lingual Alignment during LLM Training Exploring Alignment in Shared Cross-lingual Spaces mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models? Probing LLMs for Joint Encoding of Linguistic Categories Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure multimodal representations Interpreting the Second-Order Effects of Neurons in CLIP Interpreting CLIP\u0026rsquo;s Image Representation via Text-Based Decomposition safety reprs \u0026#x1f52d; resources Linear Representations of sentiment in large language models nonlinear representations Not All Language Model Features Are Linear other analyses The Origins of Representation Manifolds in Large Language Models Model capabilities training (learning) dynamics\nin-context learning\nbasic In-context Learning and Induction Heads What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation bad in-context learning (learn wrong things) Overthinking The Truth: Understanding How language Models Process False Demonstrations chain of thought (COT)\nhow and why step by step?\nzero-shot COT ???\nanalyses Iteration Head: A Mechanistic Study of Chain-of-Thought How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation Do Large Language Models Latently Perform Multi-Hop Reasoning? unfaithful COT Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting Does the model already know the answer while reasoning, or the model really has a goal? reasoning\nUnveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization How Do LLMs Perform Two-Hop Reasoning in Context? planning\nEvaluating Cognitive Maps and Planning in Large Language Models with CogEval instruction following\nhow does reinforcement learning change the inside of a model? understand RL at mechanistic level high efficient RLxF SAIF: A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models knowledge\n*Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models *(Entity Recognition and Hallucinations) On the Biology of a Large Language Model *Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level (Post 1) Dissecting Recall of Factual Associations in Auto-Regressive Language Models Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts Locating and Editing Factual Associations in GPT website Characterizing Mechanisms for Factual Recall in Language Models Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans The Geometry of Concepts: Sparse Autoencoder Feature Structure memorization \u0026amp; generalization phase transition\nWhat Do Learning Dynamics Reveal About Generalization in LLM Reasoning? In-context Learning and Induction Heads grokking Progress Measures For Grokking Via Mechanistic Interpretability Towards Understanding Grokking: An Effective Theory of Representation Learning learning dynamics\nLearning Dynamics of LLM Finetuning Loss Landscape Degeneracy Drives Stagewise Development in Transformers Loss landscape geometry reveals stagewise development of transformers Multi-Component Learning and S-Curves Deep double descent duplication\nself-repair\nThe Hydra Effect: Emergent Self-repair in Language Model Computations Explorations of Self-Repair in Language Models massive activations\nMassive Activations in Large Language Models Systematic Outliers in Large Language Models Narrow tasks counting greater-than How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model Indirect Object Indentification (IOI) Interpretability in The Wild: A Circuit For Indirect Object Identification in GPT-2 Small gender Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias Interpretable model structure also called intrinsic interpretability\nModifying Model Components (SoLU) Softmax Linear Units Reengineering Model Architecture (Except interpretable model architectures, I also list some fresh-new architectures.)\nCBM (Concept Bottleneck Models)\nConcept Bottleneck Models Concept Bottleneck Large Language Models Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization Backpack Language Models\nBackpack Language Models others\nSeeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability SEER: Self-Explainability Enhancement of Large Language Models’ Representations Modular Training of Neural Networks aids Interpretability Compositional attention networks for machine reasoning new architetures\nA Path Towards Autonomous Machine Intelligence Large Concept Models: Language Modeling in a Sentence Representation Space Large Language Diffusion Models Fractal Generative Models (VLM single transformer) The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer JEPA Brain Inspired Findings We Can’t Understand AI Using our Existing Vocabulary Creations Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration Other Methods \u0026amp; Analysis decomposing a model Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition representation SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability (CKA) Similarity of Neural Network Representations Revisited 神经网络表征度量（一） We Can’t Understand AI Using our Existing Vocabulary Application AI alignment 3H (helpful, honest, harmless) Avoid bias and harmful behaviors\nconcept-based interpretability representation-based interpretability red-teaming perturbations\nbackdoor detection, red-teaming, capability discovery\nanomaly detection\nbackdoor detection\nSleeper Agents: Training Deceptive LLMs that Persist Through Safety Training SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks Mechanistic anomaly detection and ELK A gentle introduction to mechanistic anomaly detection Concrete empirical research projects in mechanistic anomaly detection refuse to request \u0026amp; jailbreak circuit; SAE; steering vector (anti-refusal)\nmeasures (repr engineering) Improving Alignment and Robustness with Circuit Breakers (steering) Refusal in Language Models Is Mediated by a Single Direction (steering) Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models (training) Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications analyses Universal and Transferable Adversarial Attacks on Aligned Language Models Many-shot jailbreaking Jailbroken: How Does LLM Safety Training Fail? Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs (vlm) Visual Adversarial Examples Jailbreak Aligned Large Language Models (vlm) Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models (vlm) Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything evaluation \u0026amp; benchmark Jailbreak prompts finding on Twitter JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models power-seeking\nParametrically Retargetable Decision-Makers Tend To Seek Power social injustice prejudice, gender bias: doctor \u0026amp; nurse, discrimination\ntraining dynamics; dataset; gradient descent; SAE circuits\nEvaluating feature steering: A case study in mitigating social biases Prejudice and Volatility: A Statistical Framework for Measuring Social Discrimination in Large Language Models Unveiling Gender Bias in Large Language Models: Using Teacher’s Evaluation in Higher Education As an Example deception\ndishonesty\nAlignment faking in large language models Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training Language Models Learn To Mislead Humans Via RLHF How do Large Language Models Navigate Conflicts between Honesty and Helpfulness? reward hacking\nReward Hacking in Reinforcement Learning measurement tampering\nThe AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome.\nBenchmarks for Detecting Measurement Tampering persona drift\nMeasuring and Controlling Persona Drift in Language Model Dialogs other human values\nSycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models Reducing sycophancy and improving honesty via activation steering Modulating sycophancy in an RLHF model via activation steering Agency\nUnderstanding and Controlling a Maze-Solving Policy Network Parametrically Retargetable Decision-Makers Tend To Seek Power Avoiding Side Effects in Complex Environments Alignmnet theory\nBoth alignment and interpretability are related to AI safety, so the mech interp tools are widely used in alignment research. I\u0026rsquo;ll put some good resources of alignment work here.\nqualitative work (findings, analyses, concepts, \u0026hellip;) * alignment representation * instrumental convergence * shard theory\nProposed by Alex Turner (TurnTrout) and Quintin Pope The Shard Theory of Human Values (lesswrong) The Shard Theory of Human Values \u0026#x1f52d; resources AI Alignment: A Comprehensive Survey Representation Engineering: A Top-Down Approach to AI Transparency Mechanistic Interpretability for AI Safety A Review Improved Algorithms ReFT: Representation Finetuning for Language Models Harmonic Loss Trains Interpretable AI Models Research Limitations Current work mainly focuses on Transformer-based models. Is transformer a inevitable model structure for generative language models? How can we use post-hoc methods as a guide for training a more interpretable and controllable model? Other Interpretability Fields Neural network interpretability Not necessarily a transformer-based model, maybe an lstm or simply a toy model\nTheories for DL foocker/deeplearningtheory Feature Learning Huang wei\u0026rsquo;s repo game (chess) Evidence of Learned Look-Ahead in a Chess-Playing Neural Network (Sokoban, planning) Planning behavior in a recurrent neural network that plays Sokoban geometry Reasoning in Large Language Models: A Geometric Perspective Bertology A Primer in BERTology: What we know about how BERT works What do you mean, BERT? Assessing BERT as a Distributional Semantics Model Other Surveys Open Problems in Mechanistic Interpretability A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models Mechanistic Interpretability for AI Safety: A Review A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models Towards Uncovering How Large Language Model Works: An Explainability Perspective ","permalink":"https://Siriuslala.github.io/posts/mech_interp_research/","summary":"\u003cp\u003e\u0026#x1f4a1;\nThis post is mainly focused on text models. For multi-modal models, please refer to \u003ca href=\"https://Siriuslala.github.io/posts/mm_interp/\"\u003ethis post\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"the-purpose-i-write-this-blog\"\u003eThe Purpose I Write This Blog\u003c/h2\u003e\n\u003cp\u003e   To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.\u003c/p\u003e\n\u003ch2 id=\"circuit-discovery\"\u003eCircuit Discovery\u003c/h2\u003e\n\u003ch3 id=\"methods\"\u003eMethods\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ebasic\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eactivation patching (causal mediation/interchange interventions\u0026hellip;)\u003c/li\u003e\n\u003cli\u003epath patching\u003c/li\u003e\n\u003cli\u003escaling techinques: attribution patching\u003c/li\u003e\n\u003cli\u003eDAS (distributed alignment search)   directional activation patching?\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"telescope-resources\"\u003e\u0026#x1f52d; resources\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003einspirition\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/1905.09418\"\u003eAnalyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2202.05262\"\u003e(ROME) Locating and Editing Factual Associations in GPT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.neelnanda.io/mechanistic-interpretability/attribution-patching\"\u003eAttribution patching: Activation patching at industrial scale\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2304.14997\"\u003e(ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2310.10348\"\u003eAttribution Patching Outperforms Automated Circuit Discovery\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2403.00745\"\u003eAtP*: An efficient and scalable method for localizing llm behaviour to components\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing\"\u003eCausal Scrubbing: a method for rigorously testing interpretability hypotheses\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003enew\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUsing SAE\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2403.19647\"\u003eSparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2405.13868\"\u003eAutomatically Identifying Local and Global Circuits with Linear Computation Graphs\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eContextual Decomposition\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2407.00886\"\u003eMechanistic Interpretation through Contextual Decomposition in Transformers\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eEdge Pruning ?\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2406.16778\"\u003eFinding Transformer Circuits with Edge Pruning\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2407.03779\"\u003eFunctional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"\"\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"evaluation\"\u003eEvaluation\u003c/h3\u003e\n\u003cp\u003elack of ground truth\u003c/p\u003e","title":"Possible Research Areas in Mechanistic Interpretability"},{"content":"\u0026#x1f3b6;Code in this post can be found at the jupyter notebook in my \u0026ldquo;saeExploration\u0026rdquo; repo.\nFind features that reflect positive emotions To find the features related to a specific emotion, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:\n1 2 3 4 5 prompt_happy = [\u0026#34;I\u0026#39;ll be on a vacation tomorrow and I\u0026#39;m so happy.\u0026#34;, \u0026#34;My mombrings home a new puppy and I\u0026#39;m so happy.\u0026#34;, \u0026#34;I\u0026#39;m so glad I got the job I wanted.\u0026#34;, \u0026#34;I feel so happy when I\u0026#39;m with my friends.\u0026#34;, \u0026#34;I\u0026#39;m so happy I got the promotion I wanted.\u0026#34;,] I choose to look for features that reflect happiness and sadness. Apart from that, I also wonder if the feature that reflects excitedness has something to do with the one that reflects happiness (they are alike from the semantic level at least.)\nFor a start, I inspected the residual stream in layer_7. The SAE we choose is gpt2-small-res-jb which hooks at the residual stream at the entrance of a layer. The prompts were fed into the model and the outputs were SAE activations. I checked the activations at the word “happy” for all the prompts and calculated the mean value of them. I visualized them as below:\nFigure 1: Feature activations at the keywords for happy emotion Obviously there are three SAE features that activate most actively on the happy emotion, and their feature ids are 2392, 9840 and 21753. I checked the feature 2392 in Neuronpedia and got its feature dashboard:\nFigure 2: SAE feature 2393 on the dashboard (https://neuronpedia.org/gpt2-small/7-res-jb/2392?embed=true\u0026embedexplanation=true\u0026embedplots=true\u0026embedtest=true\u0026height=300). Later I found features for sadness and excitedness in the same way. The top-3 features activating for “sad” and “excited” are [2045, 23774, 10866] and [8935, 9840, 3247] respectively.\nCompare the features related to happiness and excitedness I want to see the difference between \u0026ldquo;happy features\u0026rdquo; and \u0026ldquo;excited features\u0026rdquo; since they are both positive emotions. So I compared their top-3 features and only kept the features that activate on both “happy” and “excited”. They are visualized as below:\nFigure 3: Feature activations on both happy and excited emotions at the residual pre stream in layer_7. From the figure above, we can easily find out the two features shared by happiness and excitedness. It seems that these two features contain positive emotion concepts, which means they are close to each other on the semantic level.\nA deeper investigation into the features related to happiness From the previous result, we can see that there are 3 features that fire quite actively on happiness. Since they share similar semantic meanings, I guess that the representations of features activating on the same emotion (i.e. 2392, 9840 and 21753) have high similarities. To prove this, I try to inspect the representation of one prompt which expresses happiness and calculate the cosine similarities among representations of different features. Here the representations of different layers are calculated as below: $$ feat \\_ repr = W_{dec} * SAE \\_ activations $$ Note that the * operation is a dot product. The W_dec in the formula above is the decoder matrix of SAE with a shape of (d_sae, d_model). We know that according to the definition of dictionary learning, each vector in the dimension of d_sae corresponds to a base vector for an SAE feature. Each element in the SAE_activations can be seen as the intensity of the feature at that position. So by multiplying W_dec and SAE_activations we can get a representation of shape $ (d \\_ model,) $ which expresses the features in the vector space of SAE that is sparser and more interpretable than that of the original model.\nTo visualize the similarities among features, I choose to use the heatmap. Note that in order to make it clear, I mainly focus on features 2392, 9840 and 21753 and using negative sampling to get some other features for comparison. I randomly pick 6 features except for the three features mentioned above. The similarities are calculated and shown as below:\nFigure 4: Feature similarities on the word “happy” at the residual pre stream in layer_7. Obviously we can find that the three features that fire most actively on “happy” are more similar to each other (the top left 3*3 square), thus my hypothesis is proved intuitively.\nFeatures in different layers Figure 5: Feature activations on the word “happy” at the residual pre stream in 12 layers. \u0026ensp;\u0026ensp;Previously I inspected in the 8th layer of gpt2-small and found some emotional features. Now I want to know if similar features exist in other layers, and how they are related to each other. I inspect the autoencoder features in each layer and observe their activations on the word “happy”. The result is shown in Fig 5. We can find that: * Eachlayer has more than 3 features that activate on the happy emotion. * Thepositions of activating features are different from those in other layers. Though the positions of activating features are different, I guess it\u0026rsquo;s just the problem of feature orders. For example, the feature 7683 in layer_1 may be the same kind or even exactly the same feature as feature 13928 in layer_3, though the positions are different. In order to prove this, I choose to visualize the feature representations of SAE outputs. The result is shown below:\nFigure 6: Feature similarities on the word “happy” at the residual pre stream in 12 layers. From the figure above, I got some interesting points about the feature distribution:\nThe features in the first layer have lower similarities with those in later layers. I guess this is because the first layer gets limited information from previous layers, so it cannot express relatively complicated concepts like emotions. I think maybe the first layer contains some low level concepts that are not shown in this figure, which I will explore in the future. Afeature in a layer often corresponds to a feature in another layer. For example, feature 7683 in layer_1 corresponds to feature 13928 in layer_2, which has a similarity of 0.939. This means they are related to each other across different layers, sharing similar semantic meanings. Afeature in a layer tends to be more alike to features in nearby layers. For example, feature 7683 in layer_1 is more similar to feature 13928 in layer_2 than feature 2392 in layer_7, with a similarity of 0.939 to 0.825. I think it\u0026rsquo;s because the vector spaces of nearby layers are relatively close to each other. When two layers are far from each other, the difference between their vector spaces is significant due to a lot of linear and nonlinear manipulations between layers. Thus the features would share low similarities regardless of similar semantic meanings. ","permalink":"https://Siriuslala.github.io/posts/happy_feats/","summary":"\u003cp\u003e\u0026#x1f3b6;Code in this post can be found at \u003ca href=\"https://github.com/Siriuslala/saeExploration/blob/main/multilingual_study.ipynb\"\u003ethe jupyter notebook in my \u0026ldquo;saeExploration\u0026rdquo; repo\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"find-features-that-reflect-positive-emotions\"\u003eFind features that reflect positive emotions\u003c/h2\u003e\n\u003cp\u003eTo find the features related to a specific emotion, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\" id=\"hl-0-1\"\u003e\u003ca class=\"lnlinks\" href=\"#hl-0-1\"\u003e1\u003c/a\u003e\n\u003c/span\u003e\u003cspan class=\"lnt\" id=\"hl-0-2\"\u003e\u003ca class=\"lnlinks\" href=\"#hl-0-2\"\u003e2\u003c/a\u003e\n\u003c/span\u003e\u003cspan class=\"lnt\" id=\"hl-0-3\"\u003e\u003ca class=\"lnlinks\" href=\"#hl-0-3\"\u003e3\u003c/a\u003e\n\u003c/span\u003e\u003cspan class=\"lnt\" id=\"hl-0-4\"\u003e\u003ca class=\"lnlinks\" href=\"#hl-0-4\"\u003e4\u003c/a\u003e\n\u003c/span\u003e\u003cspan class=\"lnt\" id=\"hl-0-5\"\u003e\u003ca class=\"lnlinks\" href=\"#hl-0-5\"\u003e5\u003c/a\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eprompt_happy\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;I\u0026#39;ll be on a vacation tomorrow and I\u0026#39;m so happy.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\u0026#34;My mombrings home a new puppy and I\u0026#39;m so happy.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\u0026#34;I\u0026#39;m so glad I got the job I wanted.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\u0026#34;I feel so happy when I\u0026#39;m with my friends.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\u0026#34;I\u0026#39;m so happy I got the promotion I wanted.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eI choose to look for features that reflect happiness and sadness. Apart from that, I also wonder if the feature that reflects excitedness has something to do with the one that reflects happiness (they are alike from the semantic level at least.)\u003c/p\u003e","title":"Exploring Emotional Features in GPT2-Small"},{"content":"\u0026#x26a0;\u0026#xfe0f; Warnings\nThis post was written when I first delved into this area, and it hasn\u0026rsquo;t been updated for a long time. Thus there might be a lot of errors. Now I\u0026rsquo;ve changed my attitude to this area. The area is not well-defined, and most of the research in this area is of low quality and is not appealing to me. Besides, I think the study of interpretability should be applied to pratical use, though we can also study it for fun. I\u0026rsquo;m still interested in interpretability and its applications. I\u0026rsquo;ll write something new and interesting later ~ \u0026#x1f4a1; This post is accompanied with another post, which contains specific content in this area.\nThe purpose I write this blog Mechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challenges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc). Thus it\u0026rsquo;s a little bit difficult for people new to this area to figure out what researchers are really doing.\nTherefore I write this blog to give a brief introduction to mechanistic interpretability without so much of horrible concepts. The blog aims to help you understand the basic ideas, main directions and latest achievements of this field, providing a list of resources to help you get started at the same time!\nIf you really want to do some cool research as a beginner, I highly recommend the guide by Neel Nanda.\nWhat is Mechanistic interpretability? Speaking of AI research, neural network is the tool that is used most widely nowadays for its excellent representation and generalization ability. What does a neural network do? It receives an input and gives an output after some calculations. Specifically speaking, it usually gets the representations of an input and maps it to an expected output under a predefined computation graph. From my perspective, neural networks mainly care about two things: create representations to extract features and establish the relationship between the representations and the output.\nWhy is neural network so popular? An important reason is that the neural network can save a lot of time for researchers to manually design features. For example, for natural language processing (NLP) people often designed features like \u0026ldquo;the frequency of a word that appears\u0026rdquo; or \u0026ldquo;the co-occurrence probabilities\u0026rdquo; in the past. Manually designing features caused too much labor, so people choose to use neural networks to find features automatically. As for optimizing, they set a goal of minimizing the loss function and using backward propagation (BP) to update the parameters of the model. Thus neural networks free our hands and improve performance at the same time.\nAll is well, so why do we concern about interpretability? Though neural networks can extract a lot of features with a high efficiency, we cannot have a clear understanding of what the features really are. For example, we know that a filter with Laplacian operator can extract the edge of an image, but we don\u0026rsquo;t know what the features extracted by a convolution layer mean because the parameters of the filters inside are often randomly initialized and optimized using BP algorithm. As a result, features in neural networks are often ambiguous.\nWhy is interpretability important? Actually this statement is controversial because some people say interpretability is bullshit\u0026#x1f4a9;. I\u0026rsquo;m not angry about this. Anyway, people\u0026rsquo;s taste varies, just like many people enjoy Picasso\u0026rsquo;s abstract paintings while I don\u0026rsquo;t. Interpretability still lacks exploring so it\u0026rsquo;s now far from application, and that\u0026rsquo;s why some people look down on it. While it is this lack of exploration that excites me most because there are a lot of unknown things waiting for me to discover! Actually, Interpretability is a key component in the AI alignment cycle (see Figure 0). The goal of alignment is to \u0026ldquo;make AI systems behave in line with human intentions and values\u0026rdquo; and interpretability plays an important role in ensuring AI safety. For example, unwanted things like malicious text generated by a language model may be avoided using model steering (a trick played on the activations during the forward propagation). Besides, having a clear understanding of neural networks enables us to focus on the relevant part of a model to a specific task and perform fine-tuning in a more precise way (haha here is an ad for my project: circuit-tuning).\nFigure 0: The position of interpretability in the AI alignment cycle (from this survey)\rLast question: what is mechanistic interpretability? Let\u0026rsquo;s call it mech interp first because I\u0026rsquo;m really tired of typing the full name\u0026#x1f4a6;. There seems not to be a rigorous definition, but I here I want to quote the explanation by Chris Olah:\nMechanistic interpretability seeks to reverse engineer neural networks, similar to how one might reverse engineer a compiled binary computer program.\nAnother thing: there are various of categories of interpretability, such as studies from the geometry perspective or from the game theory and symbol system perspective, which can be found at ICML, ICLR, NeurlPS, etc. When we say mech interp, we often refer to the studies on Transformer-based generative language models now (though the research started before 2017) which will be introduced briefly in the next section. So before we start, let\u0026rsquo;s briefly go over the structure of Transformer first!\nFigure 1: The structure of Transformer (from Arena)\rFigure 2: The structure of the self-attention block in a Transformer block (from Arena)\rFigure 3: The structure of the MLP layer in a Transformer block (from Arena)\rFigure 4: The structure of the layer normalization in a Transformer block (from Arena)\rBasic ideas and research topics In this section, I\u0026rsquo;m gonna explain some terms for mech interp and help you understand the basic ideas of doing mech interp research. I\u0026rsquo;ll try to make it easy!\nNote that I\u0026rsquo;ll only introduce something that I think is important. If you wanna learn more about the concepts in mech interp, please refer to: A Comprehensive Mechanistic Interpretability Explainer \u0026amp; Glossary which is a very comprehensive guide for beginners that I strongly recommend!\nImportant concepts Features\nThere are a lot of definitions for features. Unfortunately none of the definitions above can be widely recognized, so it\u0026rsquo;s open for anyone who wants to seek for the essence of the features. Generally speaking, a feature is a property of an input which is interpretable or cannot be understand by humans. Practically speaking, a feature could be an activation value of a hidden state in a model (at least lots of work is focusing on this).\nHow to find a feature? Or how to know that the thing you find is likely to be a feature? Here I want to quote the concept of \u0026ldquo;the signal of structure\u0026rdquo; proposed by Chris Olah in the post of his thoughts on qualitative research:\nThe signal of structure is any structure in one\u0026rsquo;s qualitative observations which cannot be an artifact of measurement or have come from another source, but instead must reflect some kind of structure in the object of inquiry, even if we don\u0026rsquo;t understand it.\nJust like the discovery of cells under a microscope. The shape of the cells cannot be random noise but strong evidence for the structure of them.\nCircuits\nIf we view a language model as a directed acyclic graph (DAG) $M$ where nodes are terms in its forward pass (neurons, attention heads, embeddings, etc.) and edges are the interactions between those terms (residual connections, attention, projections, etc.), a circuit $C$ is a subgraph of $M$ responsible for some behavior. That means the components inside the circuit have a big influence on the output of the task, while the components outside the subgraph have almost no influence.\nFrom my perspective, a circuit is a path from an input to an output, just like the way between two hosts in the routing networks.\nFigure 5: The computational graph of a model (from Arena)\rSuperposition\nSuperposition is a hypothesis that models can represent more features than the dimensions they have.\nIdeally we expect that each neuron only corresponds to one feature, so we can investigate or even control the feature using the neuron reserved for it. But in practice we find that a neuron fires for more than one features, which is called the phenomenon of polysemanticity in neurons. We believe that we have more features than model dimensions, so we can also say that more than one neurons fire when a feature appears. That is to say, there is not a one-to-one correspondence between neurons and features.\nPrivileged basis\nIt\u0026rsquo;s a weird idea that I have some doubt on it (maybe I haven\u0026rsquo;t grasp the core idea of it\u0026hellip;).\nMy understanding: There are many vector spaces in a model, for example, the residual stream in a layer, the output of the ReLU in a MLP layer, etc. Each vector space can be seen as a representation. Given an input, we can get the hidden states in different vector spaces during the forward propagation of the model. If we could view neurons as directions which may correspond to features in a vector space, then we say there is a privileged basis in this vector space. That is to say, each value at a specific dimension is aligned with a neuron, and that value may be a interpretable feature (maybe not).\nNot all vector spaces in a model have privileged basis. The most accepted view is that privileged bases exist in attention patterns and MLP activations, but not in residual streams. A general law is that a privilege basis often appears with a elementwise nonlinear operation, for instance, ReLu, Softmax, etc. If the operations around a representation are all linear, then we say the basis in the representation is non-privileged. For example, the operations around a residual stream are often non-linear (e.g. $W_{in}$ and $W_{out}$ of a MLP layer which correspond to the \u0026ldquo;read\u0026rdquo; and \u0026ldquo;write\u0026rdquo; operation on the residual stream). If we apply a rotation matrix to the original operations to change the basis, then the result will be unchanged because In other words, something is a privileged basis if it is not rotation-independent, i.e. the nature of computation done on it means that the basis directions have some special significance. A privileged basis is a meaningful basis for a vector space. That is, the coordinates in that basis have some meaning, that coordinates in an arbitrary basis do not have. It does not, necessarily, mean that this is an interpretable basis.\na space can have an interpretable basis without having a privileged basis. In order to be privileged, a basis needs to be interpretable a priori - i.e. we can predict it solely from the structure of the network architecture.\nResearch techniques Circuits Discovery\nFinding the circuit for a specific task attracts the attention of lots of researchers. The thing we wanna do is to get the relevant components for a specific task. A naive idea is to test the components one by one using causal intervention: change the value of one component while keeping others unchanged, and check if it influences the output. This technique is also called ablation or knockout.\nTo achieve this, we have two possible ways: denoising (find useful components) and noising (delete unuseful components). We usually prepare a clean prompt which is relevant to the task (results in a correct answer) and a corrupted prompt which has nothing to do with the task. Before finding circuits, the two prompts are fed into the model separately to get a clean run and a corrupted run.\nIf we use denoising, at each step we replace (patch) the value of a component in the corrupted run with that in the clean run. If the output is closer to the correct answer under a specific metric (e.g. KL divergence or logit difference), then we add the component into the circuit. If we use noising, then we should replace a component in the clean run with that in the corrupted run. If the output is almost unchanged under a threshold, then we regard the component as useless and delete it. Generally speaking, denoising is better than noising. To understand this, I want to quote a line in Arena: noising tells you what is necessary, denoising tells you what is sufficient.\nSeveral techniques in this area:\nactivation patching (aka causal mediation/interchange interventions\u0026hellip;) A method for circuits discovery that take nodes into consideration. path patching A variant of activation patching that also take edges into consideration to study which connections between components matter. For a pair of components A and B, we patch in the clean output of A, but only along paths that affect the input of component B. While in activation patching, all the subsequent components after A are affected. attribution patching An approximation of activation patching using a first-order Taylor expansion on the metric. This method is used to speed up circuits finding. Figure 6: Comparison of activation patching and path patching (from Arena)\rThe difference between activation patching and path patching are shown in Figure 6. In activation patching, we simply patch the node $D$ with $D\u0026rsquo;$, so the nodes after $D$ ($H, G$ and $F$) are affected. While in path patching, we patch edges rather than nodes. For example, we only want to patch the edge $D \\to G$, which means the only change is the information from node $D$ to node $G$. As a result, only $G$ and $F$ are affected while $H$ isn\u0026rsquo;t.\n\u0026#x1f52d; Recommended papers: (ROME) Locating and Editing Factual Associations in GPT (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability (attribution patching) Attribution patching: Activation patching at industrial scale (IOI) INTERPRETABILITY IN THE WILD: A CIRCUIT FOR INDIRECT OBJECT IDENTIFICATION IN GPT-2 SMALL Dictionary Learning\nDictionary Learning aims to deal with the problem of superposition. The idea is like compression sensing in the field of signal processing and is discussed in this article. The implementation of dictionary learning is to train a sparse autoencoder (SAE).\nAn autoencoder consists of an encoder and a decoder. The encoder receives an input and compresses it to a lower dimension, and the decoder maps the hidden representation to the original input. The goal of the autoencoder is to get the representation of the input while compressing it. The autoencoder is optimized using the reconstruction loss.\nCompared with the autoencoedr, the dimension of the hidden representation in SAE is always higher than that of the input, which means the SAE does something completely opposite to the autoencoder. The idea behind is that the model dimension is smaller than the number of features to represent. The model may use superposition to make full use of limited neurons to represent more features. To get one-to-one correspondence between neurons and features, we map the representation to a higher dimensional vector space with SAE encoder. Once we get the representation in SAE (let\u0026rsquo;s call it sparse features), we maps it back to the original input with SAE decoder.\nIn practice, any hidden state in a model can be studied using SAE. For example, when we want to get the sparse features of the activations $h$ in a MLP layer. We can do as follows:\n$$ z = ReLU(W_{enc}h + b_{enc}) $$ $$ h^{\\prime} = W_{dec}z + b_{dec} $$\n$$ loss = \\mathbb{E}_{h}\\left[||h-h^{\\prime}||_{2}^{2} + \\lambda||z||\\right] $$\nNote that $ h = [h_{1}, h_{2},\u0026hellip;,h_{n}]^{T} \\in \\mathbb{R}^{n\\times1} $ is a hidden state with $n$ dimensions, and each $h_{i} \\in H$ is the value of a specific dimension $i$. $W_{enc} \\in \\mathbb{R}^{m\\times n}$ maps the hidden state to a new vector space with dimension $m\u0026gt;n$, $W_{dec} \\in \\mathbb{R}^{n\\times m}$ maps the sparse features back to the original shape, $ b_{enc} \\in \\mathbb{R}^{n} $ and $ b_{dec} \\in \\mathbb{R}^{n} $ are learned bias. The loss function consists of two parts: the MSE loss as the reconstruction loss and L1 norm with a coefficient $\\lambda$ to encourage the sparsity of feature activations. It is the regularization term that separates SAE from ordinary autoencoders, so as to discourage superposition and encourage monosemanticity.\nTo better understanding the encoder and decoder in SAE, we can write a sparse feature $ f_{i} $ as an element of $ z $ :\n$$ f_{i}(h) = z_{i} = ReLU(W^{enc}_{i,.}\\cdot h + b^{enc}_{i}) $$\nEach sparse feature $ f_{i} $ is calculated using row $i$ of the encoder weight matrix. As for decoder, we can write $ h^{\\prime} $ as:\n$$ h^{\\prime} = \\sum_{i=1}^{m}f_{i}(h) \\cdot W^{dec}_{.,i} + b_{dec} $$\n;The reconstructed activation $h^\\prime$ can been seen as a linear addition of all the features. Each column of the decoder matrix corresponds to a feature, so we call it a \u0026ldquo;feature direction\u0026rdquo;. Note that sometimes the L1 norm term in the loss function can be replaced by $ \\lambda\\sum_{i=1}^{m}f_{i}(h)||W^{dec}_{.,i}||_{2} $ which places a constraint to the decoder weights to reduce ambiguity in the addition operation (we want only one or a few features to be large).\nFor simplicity, The details of the model structure, training method and evaluation will not be shown here.\n\u0026#x1f52d; Recommended papers: Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Model Steering\nA useful technique for eliciting certain model behaviors in a mechanistic way.\n\u0026#x1f52d; Recommended papers: Activation Addition: Steering Language Models Without Optimization (lesswrong) Steering GPT-2-XL by adding an activation vector Steering Llama 2 via Contrastive Activation Addition Mechanistically Eliciting Latent Behaviors in Language Models Research Areas Figure 7: The route of mech interp (from transformer-circuits.pub/2024/july-update/)\rTheory\nUnderstand model components Understand model behaviors Application\ninterpretable model structure AI alignment\nAvoid bias and harmful behaviors Some Useful Resources Here I list some resources that would be helpful for you to get started quickly in the field.\nTutorials Arena A tutorial created and maintained by Callum McDougall et al, providing a guided path for anyone who finds themselves overwhelmed by the amount of technical AI safety content out there. Neel Nanda\u0026rsquo;s Tutorial Neel\u0026rsquo;s tutorial for mech interp. Neel Nanda\u0026rsquo;s Quickstart Guide A quick start for mech interp. Neel Nanda\u0026rsquo;s remommended papers Some classic and important papers for mech interp. Neel Nanda\u0026rsquo;s problems v1 Neel\u0026rsquo;s old questions for mech interp. Neel Nanda\u0026rsquo;s problems v2 Neel\u0026rsquo;s 200 new questions for mech interp. Alignment Research Field Guide (by the MIRI team) Frameworks and Libraries TransformerLens A library maintained by Bryce Meyer and created by Neel Nanda. SAELens Originates from TransformerLens, and is separated from it because of the popularity and importance of SAE. CircuitsVis A good tool for visualizing LLMs. Plotly A good tool for plotting. Forums and Communities Transformer Circuits Thread The research posts of Anthropic alignment group. Lesswrong AI Alignment Forum Companies, Institutes, Labs and Programs Anthropic DeepMind FAR Apollo RedWood CHAI (UC Berkeley) MIRI (UC Berkeley) Alignment Research Center (ARC) MATS The ML Alignment \u0026amp; Theory Scholars, an independent research and educational seminar program that connects talented scholars with top mentors in the fields of AI alignment, interpretability, and governance. SPAR Supervised Program for Alignment Research Blogs Chris Olah Neel Nanda * Neel Nanda at the Alignment Forum\nArthur Conmy Andy Zou Jacob Steinhardt David Bau Max Tegmark Trenton Bricken Callum Mcdougall Alex Turner(TurnTrout) \u0026hellip;\n","permalink":"https://Siriuslala.github.io/posts/mech_interp_resource/","summary":"\u003cp\u003e\u0026#x26a0;\u0026#xfe0f; \u003cfont color=\"red\"\u003e\u003cem\u003e\u003cstrong\u003eWarnings\u003c/strong\u003e\u003c/em\u003e\u003c/font\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eThis post was written when I first delved into this area, and it hasn\u0026rsquo;t been updated for a long time. Thus there might be a lot of errors.\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eNow I\u0026rsquo;ve changed my attitude to this area. The area is not well-defined, and most of the research in this area is of low quality and is not appealing to me. Besides, I think the study of interpretability should be applied to pratical use, though we can also study it for fun.\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eI\u0026rsquo;m still interested in interpretability and its applications. I\u0026rsquo;ll write something new and interesting later ~\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u0026#x1f4a1;\nThis post is accompanied with \u003ca href=\"https://Siriuslala.github.io/posts/mech_interp_research/\"\u003eanother post\u003c/a\u003e, which contains specific content in this area.\u003c/p\u003e","title":"A Brief Introduction to Mechanistic Interpretability Research"},{"content":"Who am I? Hi~ I\u0026rsquo;m Yueyan Li, a researcher(still a student now) in China. I\u0026rsquo;m now at the Center of Intelligence Science and Technology, BUPT. My research focuses on machine learning, deep learning and natural language processing, mainly interpretability for neural networks and cognitive science now! Here is my CV.\nExcept from my research area, I\u0026rsquo;m also interested in communication engineering\u0026#x1f4fb; which was my major when I was an undergraduate. If you like that, feel free to share something interesting together~\nApart from technologies, I\u0026rsquo;m a lover for nature. I like the mountains, the rivers, the forests\u0026hellip;if you like hiking outdoors, don\u0026rsquo;t forget me! Also, I\u0026rsquo;m a Bboy\u0026#x270c;\u0026#xfe0f;\u0026#x1f918;. If you like breaking or any kind of street dance, just call me\u0026#x1f44b;!\nSometimes I paint as a waste of time, though I\u0026rsquo;m not professional.\nBelow are some of my interests. If you are interested in some of them, please reach me at any time~\nmachine learning, deep learning communication engineering street dance music (Buyi Mao, Eason, Huazhou, Shen Zhou / Avicii, Coldplay, / \u0026hellip;) Linguistics and Languages hiking, mountain climbing\u0026hellip; Street fitness (push-ups, muscle-ups\u0026hellip;) \u0026hellip; About my nickname I use the name Sirius/Sirius Jr./siriuslala\u0026hellip; everywhere on the Internet. This originates from Sirius Black - my favourite character in the Harry Potter series.\nHe is not only an extraodinary wizard with wild and intractable appearence but also the godfather of Harry Potter - the only family alive for Harry who had brought him warmth that is hard to replace. That\u0026rsquo;s why I admire him.\n","permalink":"https://Siriuslala.github.io/about/","summary":"About myself","title":"About Myself"},{"content":"\rPrevious\rNext \u0026nbsp; \u0026nbsp;\r/ [pdf]\rView the PDF file here.\r","permalink":"https://Siriuslala.github.io/pdfs/circuit_tuning/","summary":"\u003cscript crossorigin=\"anonymous\" src=\"/pdf-js/build/pdf.js\"\u003e\u003c/script\u003e\r\n\r\n\r\n\u003cstyle\u003e\r\n  #embed-pdf-container {\r\n    position: relative;\r\n    width: 100%;\r\n    height: auto;\r\n    min-height: 20vh;\r\n     \r\n  }\r\n  \r\n  .pdf-canvas {\r\n    border: 1px solid black;\r\n    direction: ltr;\r\n    width: 100%;\r\n    height: auto;\r\n    display: none;\r\n  }\r\n  \r\n  #the-canvas {\r\n    border: 1px solid black;\r\n    direction: ltr;\r\n    width: 100%;\r\n    height: auto;\r\n    display: none;\r\n  }\r\n  \r\n  \r\n  .pdf-loadingWrapper {\r\n    display: none;\r\n    justify-content: center;\r\n    align-items: center;\r\n    width: 100%;\r\n    height: 350px;\r\n  }\r\n  \r\n  .pdf-loading {\r\n    display: inline-block;\r\n    width: 50px;\r\n    height: 50px;\r\n    border: 3px solid #d2d0d0;;\r\n    border-radius: 50%;\r\n    border-top-color: #383838;\r\n    animation: spin 1s ease-in-out infinite;\r\n    -webkit-animation: spin 1s ease-in-out infinite;\r\n  }\r\n  \r\n  \r\n  \r\n  \r\n  \r\n  #overlayText {\r\n    word-wrap: break-word;\r\n    display: grid;\r\n    justify-content: end;\r\n  }\r\n  \r\n  #overlayText a {\r\n    position: relative;\r\n    top: 10px;\r\n    right: 4px;\r\n    color: #000;\r\n    margin: auto;\r\n    background-color: #eeeeee;\r\n    padding: 0.3em 1em;\r\n    border: solid 2px;\r\n    border-radius: 12px;\r\n    border-color: #00000030;\r\n    text-decoration: none;\r\n  }\r\n  \r\n  #overlayText svg {\r\n    height: clamp(1em, 2vw, 1.4em);\r\n    width:  clamp(1em, 2vw, 1.4em);\r\n  }\r\n  \r\n  \r\n  \r\n  @keyframes spin {\r\n    to { -webkit-transform: rotate(360deg); }\r\n  }\r\n  @-webkit-keyframes spin {\r\n    to { -webkit-transform: rotate(360deg); }\r\n  }\r\n  \u003c/style\u003e\u003cdiv class=\"embed-pdf-container\" id=\"embed-pdf-container-fe6807af\"\u003e\r\n    \u003cdiv class=\"pdf-loadingWrapper\" id=\"pdf-loadingWrapper-fe6807af\"\u003e\r\n        \u003cdiv class=\"pdf-loading\" id=\"pdf-loading-fe6807af\"\u003e\u003c/div\u003e\r\n    \u003c/div\u003e\r\n    \u003cdiv id=\"overlayText\"\u003e\r\n      \u003ca href=\"./Circuit_tuning.pdf\" aria-label=\"Download\" download\u003e\r\n        \u003csvg aria-hidden=\"true\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 18 18\"\u003e\r\n            \u003cpath d=\"M9 13c.3 0 .5-.1.7-.3L15.4 7 14 5.6l-4 4V1H8v8.6l-4-4L2.6 7l5.7 5.7c.2.2.4.3.7.3zm-7 2h14v2H2z\" /\u003e\r\n        \u003c/svg\u003e\r\n      \u003c/a\u003e\r\n    \u003c/div\u003e\r\n    \u003ccanvas class=\"pdf-canvas\" id=\"pdf-canvas-fe6807af\"\u003e\u003c/canvas\u003e\r\n\u003c/div\u003e\r\n\r\n\u003cdiv class=\"pdf-paginator\" id=\"pdf-paginator-fe6807af\"\u003e\r\n    \u003cbutton id=\"pdf-prev-fe6807af\"\u003ePrevious\u003c/button\u003e\r\n    \u003cbutton id=\"pdf-next-fe6807af\"\u003eNext\u003c/button\u003e \u0026nbsp; \u0026nbsp;\r\n    \u003cspan\u003e\r\n      \u003cspan class=\"pdf-pagenum\" id=\"pdf-pagenum-fe6807af\"\u003e\u003c/span\u003e / \u003cspan class=\"pdf-pagecount\" id=\"pdf-pagecount-fe6807af\"\u003e\u003c/span\u003e\r\n    \u003c/span\u003e\r\n    \u003ca class=\"pdf-source\" id=\"pdf-source-fe6807af\" href=\"./Circuit_tuning.pdf\"\u003e[pdf]\u003c/a\u003e\r\n\u003c/div\u003e\r\n\r\n\u003cnoscript\u003e\r\nView the PDF file \u003ca class=\"pdf-source\" id=\"pdf-source-noscript-fe6807af\" href=\"./Circuit_tuning.pdf\"\u003ehere\u003c/a\u003e.\r\n\u003c/noscript\u003e\r\n\r\n\u003cscript type=\"text/javascript\"\u003e\r\n    (function(){\r\n    var url = '.\\/Circuit_tuning.pdf';\r\n\r\n    var hidePaginator = \"\" === \"true\";\r\n    var hideLoader = \"\" === \"true\";\r\n    var selectedPageNum = parseInt(\"\") || 1;\r\n\r\n    \r\n    var pdfjsLib = window['pdfjs-dist/build/pdf'];\r\n\r\n    \r\n    if (pdfjsLib.GlobalWorkerOptions.workerSrc == '')\r\n      \r\n      pdfjsLib.GlobalWorkerOptions.workerSrc = \"\\/pdf-js\\/build\\/pdf.worker.js\";\r\n\r\n\r\n\r\n    \r\n    var pdfDoc = null,\r\n        pageNum = selectedPageNum,\r\n        pageRendering = false,\r\n        pageNumPending = null,\r\n        scale = 3,\r\n        canvas = document.getElementById('pdf-canvas-fe6807af'),\r\n        ctx = canvas.getContext('2d'),\r\n        paginator = document.getElementById(\"pdf-paginator-fe6807af\"),\r\n        loadingWrapper = document.getElementById('pdf-loadingWrapper-fe6807af');\r\n\r\n\r\n    \r\n    showPaginator();\r\n    showLoader();\r\n\r\n    \n\r\n    function renderPage(num) {\r\n      pageRendering = true;\r\n      \r\n      pdfDoc.getPage(num).then(function(page) {\r\n        var viewport = page.getViewport({scale: scale});\r\n        canvas.height = viewport.height;\r\n        canvas.width = viewport.width;\r\n\r\n        \r\n        var renderContext = {\r\n          canvasContext: ctx,\r\n          viewport: viewport\r\n        };\r\n        var renderTask = page.render(renderContext);\r\n\r\n        \r\n        renderTask.promise.then(function() {\r\n          pageRendering = false;\r\n          showContent();\r\n\r\n          if (pageNumPending !== null) {\r\n            \r\n            renderPage(pageNumPending);\r\n            pageNumPending = null;\r\n          }\r\n        });\r\n      });\r\n\r\n      \r\n      document.getElementById('pdf-pagenum-fe6807af').textContent = num;\r\n    }\r\n\r\n    \n\r\n    function showContent() {\r\n      loadingWrapper.style.display = 'none';\r\n      canvas.style.display = 'block';\r\n    }\r\n\r\n    \n\r\n    function showLoader() {\r\n      if(hideLoader) return\r\n      loadingWrapper.style.display = 'flex';\r\n      canvas.style.display = 'none';\r\n    }\r\n\r\n    \n\r\n    function showPaginator() {\r\n      if(hidePaginator) return\r\n      paginator.style.display = 'block';\r\n    }\r\n\r\n    \n\r\n    function queueRenderPage(num) {\r\n      if (pageRendering) {\r\n        pageNumPending = num;\r\n      } else {\r\n        renderPage(num);\r\n      }\r\n    }\r\n\r\n    \n\r\n    function onPrevPage() {\r\n      if (pageNum \u003c= 1) {\r\n        return;\r\n      }\r\n      pageNum--;\r\n      queueRenderPage(pageNum);\r\n    }\r\n    document.getElementById('pdf-prev-fe6807af').addEventListener('click', onPrevPage);\r\n\r\n    \n\r\n    function onNextPage() {\r\n      if (pageNum \u003e= pdfDoc.numPages) {\r\n        return;\r\n      }\r\n      pageNum++;\r\n      queueRenderPage(pageNum);\r\n    }\r\n    document.getElementById('pdf-next-fe6807af').addEventListener('click', onNextPage);\r\n\r\n    \n\r\n    pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) {\r\n      pdfDoc = pdfDoc_;\r\n      var numPages = pdfDoc.numPages;\r\n      document.getElementById('pdf-pagecount-fe6807af').textContent = numPages;\r\n\r\n      \r\n      if(pageNum \u003e numPages) {\r\n        pageNum = numPages\r\n      }\r\n\r\n      \r\n      renderPage(pageNum);\r\n    });\r\n    })();\r\n\u003c/script\u003e","title":"Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks"},{"content":"","permalink":"https://Siriuslala.github.io/faq/","summary":"faq","title":"faq"},{"content":"\rPrevious\rNext \u0026nbsp; \u0026nbsp;\r/ [pdf]\rView the PDF file here.\r","permalink":"https://Siriuslala.github.io/pdfs/helper/","summary":"\u003cscript crossorigin=\"anonymous\" src=\"/pdf-js/build/pdf.js\"\u003e\u003c/script\u003e\r\n\r\n\r\n\u003cstyle\u003e\r\n  #embed-pdf-container {\r\n    position: relative;\r\n    width: 100%;\r\n    height: auto;\r\n    min-height: 20vh;\r\n     \r\n  }\r\n  \r\n  .pdf-canvas {\r\n    border: 1px solid black;\r\n    direction: ltr;\r\n    width: 100%;\r\n    height: auto;\r\n    display: none;\r\n  }\r\n  \r\n  #the-canvas {\r\n    border: 1px solid black;\r\n    direction: ltr;\r\n    width: 100%;\r\n    height: auto;\r\n    display: none;\r\n  }\r\n  \r\n  \r\n  .pdf-loadingWrapper {\r\n    display: none;\r\n    justify-content: center;\r\n    align-items: center;\r\n    width: 100%;\r\n    height: 350px;\r\n  }\r\n  \r\n  .pdf-loading {\r\n    display: inline-block;\r\n    width: 50px;\r\n    height: 50px;\r\n    border: 3px solid #d2d0d0;;\r\n    border-radius: 50%;\r\n    border-top-color: #383838;\r\n    animation: spin 1s ease-in-out infinite;\r\n    -webkit-animation: spin 1s ease-in-out infinite;\r\n  }\r\n  \r\n  \r\n  \r\n  \r\n  \r\n  #overlayText {\r\n    word-wrap: break-word;\r\n    display: grid;\r\n    justify-content: end;\r\n  }\r\n  \r\n  #overlayText a {\r\n    position: relative;\r\n    top: 10px;\r\n    right: 4px;\r\n    color: #000;\r\n    margin: auto;\r\n    background-color: #eeeeee;\r\n    padding: 0.3em 1em;\r\n    border: solid 2px;\r\n    border-radius: 12px;\r\n    border-color: #00000030;\r\n    text-decoration: none;\r\n  }\r\n  \r\n  #overlayText svg {\r\n    height: clamp(1em, 2vw, 1.4em);\r\n    width:  clamp(1em, 2vw, 1.4em);\r\n  }\r\n  \r\n  \r\n  \r\n  @keyframes spin {\r\n    to { -webkit-transform: rotate(360deg); }\r\n  }\r\n  @-webkit-keyframes spin {\r\n    to { -webkit-transform: rotate(360deg); }\r\n  }\r\n  \u003c/style\u003e\u003cdiv class=\"embed-pdf-container\" id=\"embed-pdf-container-af35b2b1\"\u003e\r\n    \u003cdiv class=\"pdf-loadingWrapper\" id=\"pdf-loadingWrapper-af35b2b1\"\u003e\r\n        \u003cdiv class=\"pdf-loading\" id=\"pdf-loading-af35b2b1\"\u003e\u003c/div\u003e\r\n    \u003c/div\u003e\r\n    \u003cdiv id=\"overlayText\"\u003e\r\n      \u003ca href=\"./cv.pdf\" aria-label=\"Download\" download\u003e\r\n        \u003csvg aria-hidden=\"true\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 18 18\"\u003e\r\n            \u003cpath d=\"M9 13c.3 0 .5-.1.7-.3L15.4 7 14 5.6l-4 4V1H8v8.6l-4-4L2.6 7l5.7 5.7c.2.2.4.3.7.3zm-7 2h14v2H2z\" /\u003e\r\n        \u003c/svg\u003e\r\n      \u003c/a\u003e\r\n    \u003c/div\u003e\r\n    \u003ccanvas class=\"pdf-canvas\" id=\"pdf-canvas-af35b2b1\"\u003e\u003c/canvas\u003e\r\n\u003c/div\u003e\r\n\r\n\u003cdiv class=\"pdf-paginator\" id=\"pdf-paginator-af35b2b1\"\u003e\r\n    \u003cbutton id=\"pdf-prev-af35b2b1\"\u003ePrevious\u003c/button\u003e\r\n    \u003cbutton id=\"pdf-next-af35b2b1\"\u003eNext\u003c/button\u003e \u0026nbsp; \u0026nbsp;\r\n    \u003cspan\u003e\r\n      \u003cspan class=\"pdf-pagenum\" id=\"pdf-pagenum-af35b2b1\"\u003e\u003c/span\u003e / \u003cspan class=\"pdf-pagecount\" id=\"pdf-pagecount-af35b2b1\"\u003e\u003c/span\u003e\r\n    \u003c/span\u003e\r\n    \u003ca class=\"pdf-source\" id=\"pdf-source-af35b2b1\" href=\"./cv.pdf\"\u003e[pdf]\u003c/a\u003e\r\n\u003c/div\u003e\r\n\r\n\u003cnoscript\u003e\r\nView the PDF file \u003ca class=\"pdf-source\" id=\"pdf-source-noscript-af35b2b1\" href=\"./cv.pdf\"\u003ehere\u003c/a\u003e.\r\n\u003c/noscript\u003e\r\n\r\n\u003cscript type=\"text/javascript\"\u003e\r\n    (function(){\r\n    var url = '.\\/cv.pdf';\r\n\r\n    var hidePaginator = \"\" === \"true\";\r\n    var hideLoader = \"\" === \"true\";\r\n    var selectedPageNum = parseInt(\"\") || 1;\r\n\r\n    \r\n    var pdfjsLib = window['pdfjs-dist/build/pdf'];\r\n\r\n    \r\n    if (pdfjsLib.GlobalWorkerOptions.workerSrc == '')\r\n      \r\n      pdfjsLib.GlobalWorkerOptions.workerSrc = \"\\/pdf-js\\/build\\/pdf.worker.js\";\r\n\r\n\r\n\r\n    \r\n    var pdfDoc = null,\r\n        pageNum = selectedPageNum,\r\n        pageRendering = false,\r\n        pageNumPending = null,\r\n        scale = 3,\r\n        canvas = document.getElementById('pdf-canvas-af35b2b1'),\r\n        ctx = canvas.getContext('2d'),\r\n        paginator = document.getElementById(\"pdf-paginator-af35b2b1\"),\r\n        loadingWrapper = document.getElementById('pdf-loadingWrapper-af35b2b1');\r\n\r\n\r\n    \r\n    showPaginator();\r\n    showLoader();\r\n\r\n    \n\r\n    function renderPage(num) {\r\n      pageRendering = true;\r\n      \r\n      pdfDoc.getPage(num).then(function(page) {\r\n        var viewport = page.getViewport({scale: scale});\r\n        canvas.height = viewport.height;\r\n        canvas.width = viewport.width;\r\n\r\n        \r\n        var renderContext = {\r\n          canvasContext: ctx,\r\n          viewport: viewport\r\n        };\r\n        var renderTask = page.render(renderContext);\r\n\r\n        \r\n        renderTask.promise.then(function() {\r\n          pageRendering = false;\r\n          showContent();\r\n\r\n          if (pageNumPending !== null) {\r\n            \r\n            renderPage(pageNumPending);\r\n            pageNumPending = null;\r\n          }\r\n        });\r\n      });\r\n\r\n      \r\n      document.getElementById('pdf-pagenum-af35b2b1').textContent = num;\r\n    }\r\n\r\n    \n\r\n    function showContent() {\r\n      loadingWrapper.style.display = 'none';\r\n      canvas.style.display = 'block';\r\n    }\r\n\r\n    \n\r\n    function showLoader() {\r\n      if(hideLoader) return\r\n      loadingWrapper.style.display = 'flex';\r\n      canvas.style.display = 'none';\r\n    }\r\n\r\n    \n\r\n    function showPaginator() {\r\n      if(hidePaginator) return\r\n      paginator.style.display = 'block';\r\n    }\r\n\r\n    \n\r\n    function queueRenderPage(num) {\r\n      if (pageRendering) {\r\n        pageNumPending = num;\r\n      } else {\r\n        renderPage(num);\r\n      }\r\n    }\r\n\r\n    \n\r\n    function onPrevPage() {\r\n      if (pageNum \u003c= 1) {\r\n        return;\r\n      }\r\n      pageNum--;\r\n      queueRenderPage(pageNum);\r\n    }\r\n    document.getElementById('pdf-prev-af35b2b1').addEventListener('click', onPrevPage);\r\n\r\n    \n\r\n    function onNextPage() {\r\n      if (pageNum \u003e= pdfDoc.numPages) {\r\n        return;\r\n      }\r\n      pageNum++;\r\n      queueRenderPage(pageNum);\r\n    }\r\n    document.getElementById('pdf-next-af35b2b1').addEventListener('click', onNextPage);\r\n\r\n    \n\r\n    pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) {\r\n      pdfDoc = pdfDoc_;\r\n      var numPages = pdfDoc.numPages;\r\n      document.getElementById('pdf-pagecount-af35b2b1').textContent = numPages;\r\n\r\n      \r\n      if(pageNum \u003e numPages) {\r\n        pageNum = numPages\r\n      }\r\n\r\n      \r\n      renderPage(pageNum);\r\n    });\r\n    })();\r\n\u003c/script\u003e","title":"Yueyan Li"}]