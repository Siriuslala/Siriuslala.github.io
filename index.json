[{"content":"\u0026#x1f3b6;Code in this post can be found at the jupyter notebook in my \u0026ldquo;saeExploration\u0026rdquo; repo.\nFind features that reflect positive emotions To find the features related to a specific emotion and reduce data deviation, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:\n1 2 3 4 5 prompt_happy = [\u0026#34;I\u0026#39;ll be on a vacation tomorrow and I\u0026#39;m so happy.\u0026#34;, \u0026#34;My mombrings home a new puppy and I\u0026#39;m so happy.\u0026#34;, \u0026#34;I\u0026#39;m so glad I got the job I wanted.\u0026#34;, \u0026#34;I feel so happy when I\u0026#39;m with my friends.\u0026#34;, \u0026#34;I\u0026#39;m so happy I got the promotion I wanted.\u0026#34;,] I choose to look for features that reflect happiness and sadness. Apart from that, I also wonder if the feature that reflects excitedness has something to do with the one that reflects happiness (they are alike from the semantic level at least.)\nFor a start, I inspected the residual stream in layer_7. The SAE we choose is gpt2-small-res-jb which hooks at the residual stream at the entrance of a layer. The prompts were fed into the model and the outputs were SAE activations. I checked the activations at the word “happy” for all the prompts and calculated the mean value of them. I visualized them as below:\nObviously there are three autoencoder features that activate most actively on the happy emotion, and their feature ids are 2392, 9840 and 21753. I checked the feature 2392 in Neuronpedia and got its feature dashboard:\nLater I found features for sadness and excitedness in the same way. The top-3 features activating for “sad” and “excited” are [2045, 23774, 10866] and [8935, 9840, 3247] respectively.\nCompare the features related to happiness and excitedness ","permalink":"https://Siriuslala.github.io/posts/happy_feats/","summary":"\u0026#x1f3b6;Code in this post can be found at the jupyter notebook in my \u0026ldquo;saeExploration\u0026rdquo; repo.\nFind features that reflect positive emotions To find the features related to a specific emotion and reduce data deviation, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:\n1 2 3 4 5 prompt_happy = [\u0026#34;I\u0026#39;ll be on a vacation tomorrow and I\u0026#39;m so happy.\u0026#34;, \u0026#34;My mombrings home a new puppy and I\u0026#39;m so happy.","title":"Exploring Emotional Features in GPT2-Small"},{"content":"The purpose I write this blog Mechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challanges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc). Thus it\u0026rsquo;s a little bit difficult for people new to this area to figure out what researchers are really doing.\nTherefore I write this blog to give a brief introduction to mechanistic interpretability without so much of horrible concepts. The blog aims to help you understand the basic ideas, main directions and latest achievements of this field, providing a list of resources to help you get started at the same time!\nWhat is Mechanistic interpretability? Speaking of AI research, neural network is the tool that is used most widely nowadays for its excellent representation and generalization ability. What does a neural network do? It receives an input and gives an output after some calculations. Specifically speaking, it usually gets the representations of an input and maps it to an expected output under a predefined computation graph. From my perspective, neural networks mainly care about two things: create representations to extract features and establishe the relationship between the representations and the output. Why is neural network so popular? An important reason is that the neural network can save a lot of time for researchers to manually design features. For example, for natural language processing (NLP) people often designed features like \u0026ldquo;the frequency of a word that appears\u0026rdquo; or \u0026ldquo;the co-occurence probabilities\u0026rdquo; in the past. Manually designing features caused too much labor, so people choose to use neural networks to find features automatically. As for optimizing, they set a goal of minimizing the loss function and using backforward propagation (BP) to update the parameters of the model. Thus neural networks free our hands and improve performance at the same time.\nAll is well, so why do we concern about interpretability? Though neural networks can extract a lot of features with a high efficiency, we cannot have a clear understanding of what the features really are. For example, we know that a filter with Laplacian operator can extract the edge of an image, but we don\u0026rsquo;t know what the features extracted by a convolution layer mean because the parameters of the filters inside are often randomly initialized and optimized using BP algorithm. As a result, features in neural networks are often ambiguous.\nWhy is interpretability important? Actually this statement is controversial because some people say interpretability is bullshit\u0026#x1f4a9;. I\u0026rsquo;m not angry about this. Anyway, people\u0026rsquo;s taste varies, just like many people enjoy Picasso\u0026rsquo;s abstract paintings while I don\u0026rsquo;t. Interpretabilty still lacks exploring so it\u0026rsquo;s now far from application, and that\u0026rsquo;s why some people look down on it. While it is this lack of exploration that excites me most because there are a lot of unknown things waiting for me to discover! Apart from that, interpretabilty is closely related to AI safety because unwanted things like malicious text generated by a language model may be avoided using model steering (a trick using interpretability).\nLast question: what is mechanistic interpretability? Let\u0026rsquo;s call it mech interp first because I\u0026rsquo;m really tired of typing the full name\u0026#x1f4a6;. There seems not to be a rigorous definition, but I here I want to quote the explanation by Chiris Olah:\nMechanistic interpretability seeks to reverse engineer neural networks, similar to how one might reverse engineer a compiled binary computer program.\nAnother thing: there are various of categories of interpretability, such as studies from the geometry perspective or from the game theory and symbol system perspective, which can be found at ICML, ICLR, NeurlPS, etc. When we say mech interp, we often refer to the studies on Transformer-based generative language models now (though the research started before 2017) which will be introduced briefly in the next section.\nBasic ideas and research topics In this section, I\u0026rsquo;m gonna explain some terms for mech interp and help you understand the basic ideas of doing mech interp research. I\u0026rsquo;ll try to make it easy!\nNote that I\u0026rsquo;ll only introduce something that I think is important. If you wanna learn more about the concepts in mech interp, please refer to: A Comprehensive Mechanistic Interpretability Explainer \u0026amp; Glossary\nImportant concepts Features\nThere are a lot of definitions for features. Unfortunately none of the definitions above can be widely recognized, so it\u0026rsquo;s open for anyone who wants to seek for the essence of the features.\nHow to find a feature? Or how to know that the thing you find is likely to be a feature? Here I want to quote the concept of \u0026ldquo;the signal of structure\u0026rdquo; proposed by Chris Olah in the post of his thoughts on qualititive research:\nThe signal of structure is any structure in one\u0026rsquo;s qualitative observations which cannot be an artifact of measurement or have come from another source, but instead must reflect some kind of structure in the object of inquiry, even if we don\u0026rsquo;t understand it.\nJust like the discovery of cells under a microscope. The shape of the cells cannot be random noise but strong evidence for the structure of them.\nCircuits\nIf we view a language model as a directed acyclic graph (DAG) $M$ where nodes are terms in its forward pass (neurons, attention heads, embeddings, etc.) and edges are the interactions between those terms (residual connections, attention, projections, etc.), a circuit $C$ is a subgraph of $M$ responsible for some behavior. That means the components inside the circuit have a big influence on the output of the task, while the components outside the subgraph have almost no influence.\nFrom my perspective, a circuit is a path from an input to an output, just like the way between two hosts in the routing networks.\nSuperposition\nSuperposition is the phenomenon that models can represent more features than the dimensions they have.\nIdeally we expect that each neuron only corresponds to one feature, so we can investigate or even control the feature using the neuron reserved for it. But in practice we find that a neuron fires for more than one features. We believe that we have more features than model dimensions, so we can also say that more than one neurons fire when a feature appears. That is to say, there is not a one-to-one correspondence between neurons and features.\nPrivileged bases\nrefer to this post\nResearch topics Circuits Discovery\nFinding the circuit for a specific task attracts the attention of lots of researchers. The thing we wanna do is to delete the unrelevant nodes and leave only the useful nodes for the task. A naive idea is to test the nodes one by one using causal intervention\nnoising and denoising\nactivation patching: nodes\npath patching: edges\nIOI ACDC attribution\nG H L input corrupted \u0026hellip;\nSparse Autoencoder\nA Sparse autoencoder (SAE) aims to deal with the problem of superposition.\nAn autoencoder consists of an encoder and a decoder. The encoder receives an input and compresses it to a lower dimension, and the decoder maps the hidden representation to the original input. The goal of the autoencoder is to get the representation of the input while compressing it. The autoencoder is optimized using the reconstruction loss.\nCompared with the autoencoedr, the dimension of the hidden representation in SAE is always higher than that of the input, which means the SAE does something completely opposite to the autoencoder. The idea behind is that the model dimension is smaller than the number of features to represent. The model may use superposition to make full use of limited neurons to represent more features. To get one-to-one correspondence between neurons and features, we map the representation to a higher dimensional vector space with SAE encoder. Once we get the representation in SAE (let\u0026rsquo;s call it sparse features), we maps it back to the original input with SAE decoder.\nIn practice, any hidden state in a model can be studied using SAE. For example, when we want to get the sparse features of the activations $H$ in a MLP layer. We can do as follows:\n$$ \\overline{h_{i}} = h_{i} - b_{dec} $$ $$ z_{i} = ReLU(W_{enc}\\overline{h_{i}} + b_{enc}) $$ $$ h_{i}^{\\prime} = W_{dec}z_{i} + b_{dec} $$\n$$ loss = \\frac{1}{m}\\sum_{i=0}^{m-1}||h_{i}-h_{i}^{\\prime}||_{2}^{2} + \\lambda||H|| $$\nNote that $ H = [h_{1}, h_{2},\u0026hellip;,h_{n}] \\in \\mathbb{R}^{n\\times1} $ is a hidden state with $n$ dimensions, and each $h_{i} \\in H$ is the value of a specific dimension $i$. $W_{enc} \\in \\mathbb{R}^{m\\times n}$ maps the hidden state to a new vector space with dimension $m\u0026gt;n$, and $W_{dec} \\in \\mathbb{R}^{n\\times m}$ maps the sparse features back to the original shape. The loss function consists of two parts: the MSE loss as the reconstruction loss and L1 norm with a coefficient $\\lambda$ to encourage the sparsity of feature activations. It is the regularization term that separates SAE from ordinary autoencoders, so as to discourages superposition.\nFor simplicity, The details of the model structure, training method and evaluation will not be shown here.\n\u0026#x1f52d; Recommended papers: Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Some Useful Resources Here I list some resources that would be helpful for you to get started quickly in the field.\nTutorials Arena A tutorial created and maintained by Callum McDougall et al, providing a guided path for anyone who finds themselves overwhelmed by the amount of technical AI safety content out there. Neel Nanda\u0026rsquo;s Tutorial Neel\u0026rsquo;s tutorial for mech interp. Neel Nanda\u0026rsquo;s Quickstart Guide A quick start for mech interp. Neel Nanda\u0026rsquo;s remommended papers Some classic and important papers for mech interp. Neel Nanda\u0026rsquo;s problems v1 Neel\u0026rsquo;s old questions for mech interp. Neel Nanda\u0026rsquo;s problems v2 Neel\u0026rsquo;s 200 new questions for mech interp. Frameworks and Libraries TransformerLens A library maintained by Bryce Meyer and created by Neel Nanda.\nSAELens Originates from TransformerLens, and is separated from it because of the popularity and importance of SAE.\nForums and Communities Transformer Circuits Thread The research posts of Anthropic alignment group. Lesswrong AI Alignment Forum Institutes and Programs MATS An independent research and educational seminar program that connects talented scholars with top mentors in the fields of AI alignment, interpretability, and governance. RedWood Blogs Chris Olah Neel Nanda Arthur Conmy Trenton Bricken Callum Mcdougall \u0026hellip;\n","permalink":"https://Siriuslala.github.io/posts/mech_interp_resource/","summary":"The purpose I write this blog Mechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challanges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc).","title":"A Brief Introduction to Mechanistic Interpretability Research"},{"content":"Who am I? Hi~ I\u0026rsquo;m Yueyan Li, a researcher(still a student now) in China. I\u0026rsquo;m now at the Center of Intelligence Science and Technology, BUPT. My research focuses on machine learning, deep learning and natural language processing, mainly mechanistic interpretability now! Here is my CV.\nExcept from my research area, I\u0026rsquo;m also interested in communication engineering\u0026#x1f4fb; which was my major when I was an undergraduate. If you like that, feel free to share something interesting together~\nApart from technologies, I\u0026rsquo;m a lover for nature. I like the mountains, the rivers, the forests\u0026hellip;if you like hiking outdoors, don\u0026rsquo;t forget me! Also, I\u0026rsquo;m a Bboy\u0026#x270c;\u0026#xfe0f;\u0026#x1f918;. If you like breaking or any kind of street dance, just call me\u0026#x1f44b;!\nSometimes I paint as a waste of time, though I\u0026rsquo;m not professional.\nAbout my nickname I use the name Sirius/Sirius Jr./siriuslala\u0026hellip; everywhere on the Internet. This originates from Sirius Black-my favourite character in the Harry Potter series.\nHe is not only an extraodinary wizard with wild and intractable appearence but also the godfather of Harry Potter-the only family alive for Harry who had brought him warmth that is hard to replace. That\u0026rsquo;s why I admire him.\n","permalink":"https://Siriuslala.github.io/about/","summary":"About myself","title":"About Myself"},{"content":"","permalink":"https://Siriuslala.github.io/faq/","summary":"faq","title":"faq"},{"content":"\rPrevious\rNext \u0026nbsp; \u0026nbsp;\r/ [pdf]\rView the PDF file here.\r","permalink":"https://Siriuslala.github.io/helper/","summary":"\rPrevious\rNext \u0026nbsp; \u0026nbsp;\r/ [pdf]\rView the PDF file here.\r","title":"Yueyan Li"}]