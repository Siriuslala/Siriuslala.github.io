[{"content":"The Purpose I Write This Blog Thinking models are crazily popualr nowadays. The first time I delved in this area was in September, 2023. Later I gradually forgetted this area, until Deepseek came to life. I want to keep to collect information about LLM reasoning and share my thoughts here.\nThinking Models text-based\nexplicit reasoning DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning Kimi k1.5: Scaling Reinforcement Learning with LLMs GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models Skywork Open Reasoner 1 Technical Report implicit reasoning (Coconut) Training Large Language Models to Reason in a Continuous Latent Space others ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline blogs 自顶向下方式深度解读 DeepSeek-R1，内含大量细节 MLA(1)：从代码角度学习和彻底理解 DeepSeek MLA 算法 从头理解思考模型（LLM based Reasoning Model），O1，DeepSeek R1，Kimi K1.5 overthinking\nsurvey Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models (repo) Awesome-Efficient-Reasoning-LLMs papers Qwen3 Technical Report AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning AdaptThink: Reasoning Models Can Learn When to Think blogs 自适应快慢思考推理模型（Adaptive Reasoning Model）：Qwen3混合思考-\u0026gt;字节AdaCoT-\u0026gt;清华AdaptThinking parallel thinking\nDeep Think with Confidence visual reasoning\nsurvey Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers papers $V^{*}$: Guided Visual Search as a Core Mechanism in Multimodal LLMs active perception DeepEyes: Incentivizing “Thinking with Images” via Reinforcement Learning Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL GRIT: Teaching MLLMs to Think with Images tool use VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration imagination Thinking with Generated Images Visual Planning: Let\u0026rsquo;s Think Only with Images blogs Thinking with Images 小结 others\n[蒙特卡洛搜索树] MCT Self-Refine (MCTSr)的算法（包含代码理解）\n聊聊推理模型中的PRMs与MCTS\nEvaluation dataset Analyses analyses Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning interpretability Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning Thought Anchors: Which LLM Reasoning Steps Matter? Understanding Reasoning in Thinking Language Models via Steering Vectors Chain-of-Thought Is Not Explainability Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization How Do LLMs Perform Two-Hop Reasoning in Context? theories Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought Reinforcement Learning RL algorithms\n(GAE) High-Dimensional Continuous Control Using Generalized Advantage Estimation (DPO) Direct preference optimization: Your language model is secretly a reward model From r to q∗: Your language model is secretly a q-function DPO新作Your Language Model is Secretly a Q-Function解读，与OPENAI Q* 的联系？ (PPO) Proximal Policy Optimization Algorithms (REINFORCE++) REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt and Reward Models (GRPO) DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (DAPO) DAPO: An Open-Source LLM Reinforcement Learning System at Scale (GSPO) Group Sequence Policy Optimization (Cispo) MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention Blogs\nalgorithms 人人都能看懂的RL-PPO理论知识 Reasoning LLM（三）：LLM+RL RLHF 常见的思维误区 reward modeling\n(PRM) Let\u0026rsquo;s verify step by step (POLAR) Pre-Trained Policy Discriminators are General Reward Models analyses\nRL training Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning entropy Reasoning LLM（五）：熵缩过程与能力边界 LLMxRL】熵坍缩与缓解策略 (clip/kl-cov) The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models (forking tokens) Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning RL v.s. SFT 3.2 统一视角理解从 SFT 到 RL All Roads Lead to Likelihood: The Value of Reinforcement Learning in Fine-Tuning Generalist Reward Models: Found Inside Large Language Models (DFT) On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification 从 SFT 到 RL：一步步看清它们的联系 (NFT) Bridging Supervised Learning and Reinforcement Learning in Math Reasoning Resource RL infra (verl) HybridFlow: A Flexible and Efficient RLHF Framework doc repo slime RL Scaling 时代，我们需要什么样的 RL 框架呢？ blogs training 浅聊RL框架的勃勃生机、万物竞发 How we built our multi-agent research system verl [AI Infra] VeRL 框架入门\u0026amp;代码带读 从零开始的verl框架解析 verl RL支持训练deepseek-v3 671B实习复盘(个人版) OpenRLHF\u0026amp;Verl参数转换指南 verl小白解读 一文深度全面解析大模型分布式并行策略：DP/TP/PP/CP/EP/SP 深入理解 Megatron-LM（2）原理介绍 DeepSpeed zero1，zero2，zero3和FSDP区别详解 inference SGLang：LLM推理引擎发展新方向 图解大模型计算加速系列：FlashAttention V1，从硬件到计算逻辑 ","permalink":"https://Siriuslala.github.io/posts/thinking_and_reasoning/","summary":"\u003ch2 id=\"the-purpose-i-write-this-blog\"\u003eThe Purpose I Write This Blog\u003c/h2\u003e\n\u003cp\u003e   Thinking models are crazily popualr nowadays. The first time I delved in this area was in September, 2023. Later I gradually forgetted this area, until Deepseek came to life. I want to keep to collect information about LLM reasoning and share my thoughts here.\u003c/p\u003e\n\u003ch3 id=\"thinking-models\"\u003eThinking Models\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003etext-based\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eexplicit reasoning\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2501.12948?\"\u003eDeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2504.13914\"\u003eSeed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2501.12599\"\u003eKimi k1.5: Scaling Reinforcement Learning with LLMs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2508.06471\"\u003eGLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2505.22312\"\u003eSkywork Open Reasoner 1 Technical Report\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eimplicit reasoning\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2412.06769\"\u003e(Coconut) Training Large Language Models to Reason in a Continuous Latent Space\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eothers\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2404.02893\"\u003eChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eblogs\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://yuanchaofa.com/post/deepseek-r1-paper-reading-notes.html\"\u003e自顶向下方式深度解读 DeepSeek-R1，内含大量细节\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://yuanchaofa.com/post/hands-on-deepseek-mla.html\"\u003eMLA(1)：从代码角度学习和彻底理解 DeepSeek MLA 算法\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/20161412399\"\u003e从头理解思考模型（LLM based Reasoning Model），O1，DeepSeek R1，Kimi K1.5\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eoverthinking\u003c/strong\u003e\u003c/p\u003e","title":"Thinking and Reasoning"},{"content":"The Purpose I Write This Blog LLM-based agent is gonna change the world. Amazing agent systems have been created to change our life. Since I was once in a team that aimed to build advanced agents for the control of digital devices and for which I was impressed, I want to keep to collect information about LLM agents and share my thoughts here.\nResource GUI Agents survey Large Language Model-Brained GUI Agents: A Survey GUI Agent综述 : 揭秘GUI智能体的前世今生-1 : 总览篇-启程 models autoglm ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents MobileRL: Advancing Mobile Use Agents With Adaptive Online Reinforcement Learning Autoglm: Autonomous foundation agents for guis WebRL:Training llm web agents via self-evolving online curriculum reinforcement learning AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents others DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents Appagent: Multimodal agents as smartphone users (SeeAct) GPT-4V(ision) is a Generalist Web Agent, if Grounded Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V benchmarks web WebArena: A Realistic Web Environment for Building Autonomous Agents Mind2web: Towards a generalist agent for the web Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments (MiniWob) World of Bits: An Open-Domain Platform for Web-Based Agents android Android in the Wild: A Large-Scale Dataset for Android Device Control (AndroidArena) Understanding the weakness of large language model agents within a complex android environment DeepResearch survey Deep Research Agents: A Systematic Examination And Roadmap Towards AI Search Paradigm models Search-o1: Agentic search-enhanced large reasoning models Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning R1-searcher: Incentivizing the search capability in llms via reinforcement learning repo (Jina) node-DeepResearch Public Kimi-Researcher: End-to-End RL Training for Emerging Agentic Capabilities Language Modeling by Language Models Agentic RL Reasoning LLM（四）：Agentic RL Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning ","permalink":"https://Siriuslala.github.io/posts/llm-agent/","summary":"\u003ch2 id=\"the-purpose-i-write-this-blog\"\u003eThe Purpose I Write This Blog\u003c/h2\u003e\n\u003cp\u003e   LLM-based agent is gonna change the world. Amazing agent systems have been created to change our life. Since I was once in a team that aimed to build advanced agents for the control of digital devices and for which I was impressed, I want to keep to collect information about LLM agents and share my thoughts here.\u003c/p\u003e\n\u003ch2 id=\"resource\"\u003eResource\u003c/h2\u003e\n\u003ch3 id=\"gui-agents\"\u003eGUI Agents\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003esurvey\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2411.18279\"\u003eLarge Language Model-Brained GUI Agents: A Survey\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://mp.weixin.qq.com/s?__biz=MzkwNjE2ODMxNQ==\u0026amp;mid=2247488335\u0026amp;idx=1\u0026amp;sn=506d56c87a179b3af119d4f70dd549f5\u0026amp;scene=21\u0026amp;poc_token=HIF1X2ijWKfo2MykofnxM8oq-4mrLjJkhL8TdWx4\"\u003eGUI Agent综述 : 揭秘GUI智能体的前世今生-1 : 总览篇-启程\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003emodels\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eautoglm\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2508.14040\"\u003eComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Xiao9905/AutoGLM/blob/main/static/papers/mobilerl_0820.pdf\"\u003eMobileRL: Advancing Mobile Use Agents With Adaptive Online Reinforcement Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2411.00820\"\u003eAutoglm: Autonomous foundation agents for guis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2411.02337\"\u003eWebRL:Training llm web agents via self-evolving online curriculum reinforcement learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2410.24024\"\u003eAndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eothers\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2024/file/1704ddd0bb89f159dfe609b32c889995-Paper-Conference.pdf\"\u003eDigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2401.10935\"\u003eSeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://dl.acm.org/doi/pdf/10.1145/3706598.3713600\"\u003eAppagent: Multimodal agents as smartphone users\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2401.01614\"\u003e(SeeAct) GPT-4V(ision) is a Generalist Web Agent, if Grounded\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2310.11441\"\u003eSet-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ebenchmarks\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eweb\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2307.13854\"\u003eWebArena: A Realistic Web Environment for Building Autonomous Agents\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2023/file/5950bf290a1570ea401bf98882128160-Paper-Datasets_and_Benchmarks.pdf\"\u003eMind2web: Towards a generalist agent for the web\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2024/file/5d413e48f84dc61244b6be550f1cd8f5-Paper-Datasets_and_Benchmarks_Track.pdf\"\u003eOsworld: Benchmarking multimodal agents for open-ended tasks in real computer environments\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://proceedings.mlr.press/v70/shi17a/shi17a.pdf\"\u003e(MiniWob) World of Bits: An Open-Domain Platform for Web-Based Agents\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eandroid\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2023/file/bbbb6308b402fe909c39dd29950c32e0-Paper-Datasets_and_Benchmarks.pdf\"\u003eAndroid in the Wild: A Large-Scale Dataset for Android Device Control\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2402.06596\"\u003e(AndroidArena) Understanding the weakness of large language model agents within a complex android environment\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"deepresearch\"\u003eDeepResearch\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003esurvey\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2506.18096\"\u003eDeep Research Agents: A Systematic Examination And Roadmap\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2506.17188v1\"\u003eTowards AI Search Paradigm\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003emodels\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2501.05366\"\u003eSearch-o1: Agentic search-enhanced large reasoning models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2503.09516\"\u003eSearch-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2503.05592\"\u003eR1-searcher: Incentivizing the search capability in llms via reinforcement learning\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/PeterGriffinJin/Search-R1\"\u003erepo\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/jina-ai/node-DeepResearch\"\u003e(Jina) node-DeepResearch Public\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://moonshotai.github.io/Kimi-Researcher/\"\u003eKimi-Researcher: End-to-End RL Training for Emerging Agentic Capabilities\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2506.20249\"\u003eLanguage Modeling by Language Models\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"agentic-rl\"\u003eAgentic RL\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/1913905349284591240\"\u003eReasoning LLM（四）：Agentic RL\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://proceedings.neurips.cc/paper_files/paper/2024/file/c848b7d3adc08fcd0bf1df3101ba6728-Paper-Conference.pdf\"\u003eFine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","title":"LLM Agents"},{"content":"\u0026#x1f4a1; This post is initially focused on interpretability for multimodal models, while later a lot of papers in other fields are included, just for convenience.\nResource Interpretability for MLLMs survey A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models Sparks of Explainability Recent Advancements in Explaining Large Vision Models Awesome LMMs Mechanistic Interpretability probing Probing Multimodal Large Language Models for Global and Local Semantic Representations representation Zoom in: An introduction to circuits Multimodal Neurons in Artificial Neural Networks Interpreting CLIP\u0026rsquo;s Image Representation via Text-Based Decomposition Interpreting the Second-Order Effects of Neurons in CLIP CLIP不同层 Multimodal Neurons in Pretrained Text-Only Transformers circuit **(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models Automatic Discovery of Visual Circuits Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP SAE Case study: Interpreting, manipulating, and controlling clip with sparse autoencoders Towards multimodal interpretability: Learning sparse interpretable features in vision transformers Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery visualization Visualizer！简化你的Vision Transformer可视化！ (DVT) Denoising Vision Transformers Token Activation Map to Visually Explain Multimodal LLMs LVLM-Intrepret: An Interpretability Tool for Large Vision Language Models Transformer Interpretability Beyond Attention Visualization others **Towards interpreting visual information processing in vision-language models demo (dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability \u0026amp; Open Problems Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space information flow **Cross-modal Information Flow in Multimodal Large Language Models *From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks *What\u0026rsquo;s in the Image? A Deep-Dive into the Vision of Vision Language Models The Narrow Gate: Localized Image-Text Communication in Vision-Language Models Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference analyses on MLLMs Words or Vision: Do Vision-Language Models Have Blind Faith in Text? Forgotten Polygons: Multimodal Large Language Models are Shape-Blind Vision Transformers Need Registers On the rankability of visual embeddings Other fields of MLLMs visual pretraining\nScaling Language-Free Visual Representation Learning spatial\ngood\nBeyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas (ViT+LLM \u0026gt; ViT) Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision Encoder ! Learning Visual Composition through Improved Semantic Guidance (prompt-based) Things not Written in Text: Exploring Spatial Commonsense from Visual Signals (prompt-based) Does CLIP Bind Concepts? Probing Compositionality in Large Image Models Probing the Role of Positional Information in Vision-Language Models evaluation\nCan Multimodal Large Language Models Understand Spatial Relations? SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data SPHERE: Unveiling Spatial Blind Spots in Vision-Language Models Through Hierarchical Evaluation REC\n(看 related work) Exploring Spatial Language Grounding Through Referring Expressions (文本引导) ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension attention\nLooking Beyond Text: Reducing Language bias in Large Vision-Language Models via Multimodal Dual-Attention and Soft-Image Guidance MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs Lost in the middle: How language models use long contexts Efficient streaming language models with attention sinks (delimiters) Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and Joint Low-Rank Projection MagicPIG: LSH Sampling for Efficient LLM Generation Label words are anchors: An information flow perspective for understanding in-context learning ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features POS “闭门造车”之多模态思路浅谈（三）：位置编码 社区供稿 | 图解RoPE旋转位置编码及其特性 Base of RoPE Bounds Context Length Sensitivity Meets Sparsity: The Impact of Extremely Sparse Parameter Patterns on Theory-of-Mind of Large Language Models Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding NoPE Transformer Language Models without Positional Encodings Still Learn Positional Information Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings The Impact of Positional Encoding on Length Generalization in Transformers (NoPE limits) Length Generalization of Causal Transformers without Position Encoding Long context Extending Context Window of Large Language Models via Positional Interpolation YaRN: Efficient Context Window Extension of Large Language Models hallucination\nsurvey Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models Hallucination of Multimodal Large Language Models: A Survey *Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs Interpreting and editing vision-language representations to mitigate hallucinations Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference Debiasing Multimodal Large Language Models token compression\nsurvey When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios methods (CDPruner) Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs *AdaFV: Rethinking of Visual-Language alignment for VLM acceleration (FasterVLM) [CLS] Attention is All You Need for Training-FreeVisual Token Pruning: Make VLM Inference Faster Sparsevlm: Visual token sparsification for efficient vision-language Model Inference (FastV) An image is worth 1/2 tokens after layer 2: Plug-and-PLay Acceleration for VLLM Inference LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token *Inference Optimal VLMs Need Only One Visual Token but Larger Models TokenPacker: Efficient Visual Projector for Multimodal LLM Matryoshka Multimodal Models Matryoshka Query Transformer for Large Vision-Language Models FlashSloth: Lightning Multimodal Large Language Models via Embedded Visual Compression FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding active perception Please refer to this post.\nDataset general VQA: Visual Question Answering repo download [(VQA v2.0) Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering] (GQA) GQA:ANewDataset for Real-World Visual Reasoning and Compositional Question Answering [website]https://cs.stanford.edu/people/dorarad/gqa/index.html (TextVQA) Towards VQA Models That Can Read website repo MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models repo MMBench: Is Your Multi-modal Model an All-around Player? spatial (ARO) When and why vision-language models behave like bags-of-words, and what to do about it? (Whatsup) What’s “up” with vision-language models? Investigating their struggle with spatial reasoning [repo]https://github.com/amitakamath/whatsup_vlms (VSR) Visual Spatial Reasoning [repo]https://github.com/cambridgeltl/visual-spatial-reasoning SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data (COMFORT) Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities hallucination (POPE) Evaluating object hallucination in large vision-language models repo (CHAIR) Object hallucination in image captioning repo (OpenCHAIR) Mitigating Open-Vocabulary Caption Hallucinations website Models llm\ngpt-oss Introducing gpt-oss | OpenAI OpenAI开源模型gpt-oss-120b的妙妙小观察 关于gpt-oss那些值得关注的点 vlm\nbasic (Transformer) Attention is all you need (VIT) An Image is Worth 16x16 Words: Transformers for Image Recognition fat Scale (CLIP) Learning Transferable Visual Models From Natural Language Supervision (SigLIP) Sigmoid Loss for Language Image Pre-Training (PACL) Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning (BLIP-2) BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond Qwen2-VL: Enhancing Vision-Language Model\u0026rsquo;s Perception of the World at Any Resolution Qwen2.5-VL Technical Report 万字长文图解Qwen2.5-VL实现细节 (DFN) Data Filtering Networks LLaVA系列 (LLaVA) Visual Instruction Tuning (LLaVA-1.5) Improved Baselines with Visual Instruction Tuning InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks (InternVL 1.5) How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites (InternVL 2.5) Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling visual contrastive learning (SimCLR) A Simple Framework for Contrastive Learning of Visual Representations (MoCo) Momentum Contrast for Unsupervised Visual Representation Learning BeiT (MAE) Masked Autoencoders Are Scalable Vision Learners (iBOT) Image BERT: A New Vision-Language Pre-training Framework (BYOL) Bootstrap Your Own Latent A New Approach to Self-Supervised Learning (DINO) Emerging Properties in Self-Supervised Vision Transformers DINOv2: Learning Robust Visual Features without Supervision DINOv3 Self-Supervised Learning 超详细解读 (目录) 万字长文超详解读之DINO全系列—视觉表征对比学习的高峰 万字长文超详解之DINO-V3（DINO全系列之补充篇） DINO\u0026amp;DINO v2：颠覆自监督视觉特征表示学习 resolution Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution Swin Transformer: Hierarchical Vision Transformer using Shifted Windows DualFocus: A Unified Framework for Integrating Positive and Negative Descriptors in Text-based Person Retrieval generative models\n视觉生成超详细解读 (目录)\nimage\nbasic (T2I)\n(VAE) Auto-Encoding Variational Bayes vae和重参数化技巧 (DDPM) Denoising Diffusion Probabilistic Models (DDIM) Denoising Diffusion Implicit Models (Classifier-guided) Diffusion Models Beat GANs on Image Synthesis (Classifier-free) Classifier-free diffusion guidance (VQGAN) Taming transformers for high-resolution image synthesis VQGAN 论文与源码解读：前Diffusion时代的高清图像生成模型 (DIT) Scalable Diffusion Models with Transformers (LDM) High-resolution image synthesis with latent diffusion models (GLIDE) GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models (Imagen) (DALL-E) Zero-Shot Text-to-Image Generation (DALL-E-2) Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E-3) Improving Image Generation with Better Captions (ControlNet) Adding Conditional Control to Text-to-Image Diffusion Models Consistency Models (LCM) Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference (SDXL Turbo) Adversarial Diffusion Distillation (EDM) Elucidating the Design Space of Diffusion-Based Generative Models Flow Matching for Generative Modeling (Stable Diffusion 3) Scaling Rectified Flow Transformers for High-Resolution Image Synthesis Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction Stable Diffusion技术路线发展历程回顾 image editing\nInstructPix2Pix: Learning to Follow Image Editing Instructions Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry Diffusion Model-Based Image Editing: A Survey video\nsurvey\nThe Dawn of Video Generation: Preliminary Explorations with SORA-like Models generation\nVideo Diffusion Models Latte: Latent Diffusion Transformer for Video Generation Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning I2V\nvideo editing\nAnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks TokenFlow: Consistent Diffusion Features for Consistent Video Editing STABLEV2V: Stablizing Shape Consistency in Video-to-Video Editing VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing autogressive image generation\nAutoregressive Model Beats Diffusion: Llama for Scalable Image Generation AGI\nunified generation and comprehension papers Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities Janus-Pro Janus+Janus-Pro 论文解析 Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding (BAGEL) Emerging Properties in Unified Multimodal Pretraining 生成理解统一模型解读 (十一)：万字长文解读生成理解统一模型 BAGEL：高质量多模态交织预训练 SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation Emu: Generative Pretraining in Multimodality (Emu2) Generative Multimodal Models are In-Context Learners Emu3: Next-Token Prediction is All You Need NExT-GPT： Any-to-Any Multimodal LLM blogs 2025 年，生成和理解多模态大模型一些思考 new mllm archs Fuyu-8B: A Multimodal Architecture for AI Agents (SAIL) The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer 字节 SAIL 论文解读 world models blogs\n3D/4D World Model（WM）近期发展的总结和思考 VLA\n对VLA的RL最新进展梳理 VLA扩充数据来源方法进展梳理 JEPA\nJEPA：自主机器智能的“蛋糕胚”——JEPA模型发展脉络梳理（第一弹） (I-JEPA) Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning world simulator\nA Summary \u0026ldquo;Embodied AI\u0026rdquo;\nA Survey on Vision-Language-Action Models: An Action Tokenization Perspective navigation\nNavigation World Models physical reasoning\nDenoising Hamiltonian Network for Physical Reasoning ","permalink":"https://Siriuslala.github.io/posts/mm_interp/","summary":"\u003cp\u003e\u0026#x1f4a1;\nThis post is initially focused on interpretability for multimodal models, while later a lot of papers in other fields are included, just for convenience.\u003c/p\u003e\n\u003ch2 id=\"resource\"\u003eResource\u003c/h2\u003e\n\u003ch3 id=\"interpretability-for-mllms\"\u003eInterpretability for MLLMs\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003esurvey\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2502.17516\"\u003eA Survey on Mechanistic Interpretability for Multi-Modal Foundation Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2502.01048v1\"\u003eSparks of Explainability Recent Advancements in Explaining Large Vision Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/itsqyh/Awesome-LMMs-Mechanistic-Interpretability?tab=readme-ov-file#-blog\"\u003eAwesome LMMs Mechanistic Interpretability\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eprobing\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2402.17304\"\u003eProbing Multimodal Large Language Models for Global and Local Semantic Representations\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003erepresentation\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://distill.pub/2020/circuits/zoom-in/?ref=cold-takes\"\u003eZoom in: An introduction to circuits\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://distill.pub/2021/multimodal-neurons/\"\u003eMultimodal Neurons in Artificial Neural Networks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2310.05916\"\u003eInterpreting CLIP\u0026rsquo;s Image Representation via Text-Based Decomposition\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2406.04341\"\u003eInterpreting the Second-Order Effects of Neurons in CLIP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.cnblogs.com/LittleHenry/p/18688886\"\u003eCLIP不同层\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://openaccess.thecvf.com/content/ICCV2023W/CLVL/papers/Schwettmann_Multimodal_Neurons_in_Pretrained_Text-Only_Transformers_ICCVW_2023_paper.pdf\"\u003eMultimodal Neurons in Pretrained Text-Only Transformers\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ecircuit\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2406.04236\"\u003e**(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2404.14349\"\u003eAutomatic Discovery of Visual Circuits\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://openaccess.thecvf.com/content/ICCV2023W/CLVL/papers/Palit_Towards_Vision-Language_Mechanistic_Interpretability_A_Causal_Tracing_Tool_for_BLIP_ICCVW_2023_paper.pdf\"\u003eTowards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSAE\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.lesswrong.com/posts/iYFuZo9BMvr6GgMs5/case-study-interpreting-manipulating-and-controlling-clip\"\u003eCase study: Interpreting, manipulating, and controlling clip with sparse autoencoders\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.lesswrong.com/posts/bCtbuWraqYTDtuARg/towards-multimodal-interpretability-learning-sparse-2\"\u003eTowards multimodal interpretability: Learning sparse interpretable features in vision transformers\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2407.14499\"\u003eDiscover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003evisualization\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/398408338\"\u003eVisualizer！简化你的Vision Transformer可视化！\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2401.02957\"\u003e(DVT) Denoising Vision Transformers\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2506.23270\"\u003eToken Activation Map to Visually Explain Multimodal LLMs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://openaccess.thecvf.com/content/CVPR2024W/XAI4CV/papers/Stan_LVLM-Intrepret_An_Interpretability_Tool_for_Large_Vision-Language_Models_CVPRW_2024_paper.pdf\"\u003eLVLM-Intrepret: An Interpretability Tool for Large Vision Language Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2012.09838\"\u003eTransformer Interpretability Beyond Attention Visualization\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eothers\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2410.07149\"\u003e**Towards interpreting visual information processing in vision-language models\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://clementneo.com/llava_logit_lens/\"\u003edemo\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.alignmentforum.org/posts/kobJymvvcvhbjWFKe/laying-the-foundations-for-vision-and-multimodal-mechanistic\"\u003e(dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability \u0026amp; Open Problems\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2203.14680\"\u003eTransformer feed-forward layers build predictions by promoting concepts in the vocabulary space\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003einformation flow\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2411.18620\"\u003e**Cross-modal Information Flow in Multimodal Large Language Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2406.06579\"\u003e*From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2411.17491\"\u003e*What\u0026rsquo;s in the Image? A Deep-Dive into the Vision of Vision Language Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2412.06646\"\u003eThe Narrow Gate: Localized Image-Text Communication in Vision-Language Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"\"\u003ePerformance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2503.13108\"\u003eLifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eanalyses on MLLMs\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2503.02199\"\u003eWords or Vision: Do Vision-Language Models Have Blind Faith in Text?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2502.15969\"\u003eForgotten Polygons: Multimodal Large Language Models are Shape-Blind\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2309.16588\"\u003eVision Transformers Need Registers\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2507.03683\"\u003eOn the rankability of visual embeddings\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"other-fields-of-mllms\"\u003eOther fields of MLLMs\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003evisual pretraining\u003c/strong\u003e\u003c/p\u003e","title":"Interpretability for Multimodal Models"},{"content":" ArXiv(old version): https://arxiv.org/pdf/2502.06106\n","permalink":"https://Siriuslala.github.io/posts/circuit_tuning/","summary":"\u003c!-- The [paper](/pdfs/circuit_tuning) is here. --\u003e\n\u003cp\u003eArXiv(old version): \u003ca href=\"https://arxiv.org/pdf/2502.06106\"\u003ehttps://arxiv.org/pdf/2502.06106\u003c/a\u003e\u003c/p\u003e","title":"Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks"},{"content":"This post is written in Chinese. If you don\u0026rsquo;t know Chinese, you can learn it lol. (Sorry for this because simply translating the post into English may not be enough for you to understand).\n语言学乐子 皮钦语 (pidgin) 大家对那些 1.言语中不时夹杂着英文单词 2.装/凡尔赛 的人表现出一种厌恶。例如，下面是某恋综里的一段留子对话的名场面：\n————————————————————————————————\n\u0026hellip;\n男A：\u0026ldquo;可能因为我学校在伦敦，所以\u0026hellip;\u0026rdquo; 女A：\u0026ldquo;哦我也是。我是高中在 York (Yorkshire) 附近，然后本科研究生在伦敦\u0026rdquo;\n男B：\u0026ldquo;我喜欢 nə —— uhh —— Southampton\u0026rdquo;\n女A：\u0026ldquo;在海边！\u0026rdquo;\n男B：\u0026ldquo;对超美！\u0026rdquo;\n男A：\u0026ldquo;然后，我也挺喜欢曼 —— emmm —— Manchester 的\u0026rdquo;\n\u0026hellip;\n————————————————————————————————\n好的嘲讽完毕，点到为止。\n实际上语言学里的语言演化领域有一个名词叫皮钦语（pidgin），也被称作混杂语，是语言发展的一个阶段，指在没有共同语言而又急于进行交流的人群中间产生的一种混合语言，属于不同语言人群的联系语言。感兴趣详见：皮克特语到四阶皮钦语——苏格兰语言迭代史。\n笔者印象里比较深的，一个是洋泾浜语，即流行在20世纪初的上海滩的一种英汉混杂语；另一个是协和语，指流行于伪满洲国的汉语、日语的混合语言。两者都是殖民地和半殖民地文化的产物。以下是一些例子：\n洋泾浜语：\u0026ldquo;来是\u0026rsquo;康姆\u0026rsquo;（come），去是\u0026rsquo;狗\u0026rsquo;（go）\u0026quot;（详见汪仲贤所著《上海俗话图说》；dbq耳边想起赵丽蓉老师的小品） 协和语：\u0026ldquo;你的帮我，我的钱的大大的给。\u0026quot;（汉语语序为主谓宾（SVO），日语为主宾谓（SOV）） 还有一种语言叫经堂语，指的是中国回族内信仰伊斯兰教的群体在讲授和学习经文时所用的语言，多见于清真寺内，故称经堂语。具体来说，《古兰经》等经书最早是用阿拉伯语、波斯语等语言编写的，国内的穆斯林将其翻译成汉语时，保留了一些音译词汇。例如下面这段：\n\u0026ldquo;都闪白（星期一）那天，阿斯玛（天）突然就黑了，说变就变，忒吓人，要说人在顿因（这辈子、世界）活着，就是要有敬畏心，时时刻刻讨白（忏悔）自己罪，多多给乜贴（施舍）穷人，后世过随拉托（冥河桥）、到了垛子海（地狱），也能知道自己的本性。事事知感（感恩体会）就好了。\n上面这段摘自知乎网友的回答。”都闪白“对应波斯语里的星期一（دوشنبه），”阿斯玛“对应天空（آسمان）\u0026hellip;。更多的介绍请移步于这篇文章。经堂语严格上来说不算皮钦语，因为皮钦语是为了不同语言的使用者之间相互交流而产生的，而经堂语是宗教讲习的时候使用的。\n此外，刘慈欣《三体》中也提到了由汉语和英语结合而成的新语言（舰队混合语）。\n其实，正常的、真诚的表达往往不会惹人厌烦，例如：\n\u0026ldquo;我希望能够做点兼职，至少能够 cover 掉我一部分的学费\u0026rdquo; \u0026ldquo;这个 checkpoint 我测了一下，跟 baseline 相比有很大的优势，但是跟 SOTA 比还是有很大的差距\u0026rdquo; \u0026ldquo;哈喽哈喽，你好呀~\u0026rdquo; 语言磨蚀 蒙古（méng gǔ）还是蒙古（měng gǔ）？ 汉语的音变现象。 古代汉语的发音。平上去入：”天子圣哲“\n”一个“\n”卡塔尔“\n语言学知识 语言相对论 语音和音韵 词法和句法 颜色词\n句法结构\n语义和语用 ","permalink":"https://Siriuslala.github.io/posts/%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E5%AD%A6%E7%9A%84%E6%A2%97%E5%92%8C%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%9F%A5%E8%AF%86/","summary":"\u003cp\u003eThis post is written in Chinese. If you don\u0026rsquo;t know Chinese, you can learn it lol. (Sorry for this because simply translating the post into English may not be enough for you to understand).\u003c/p\u003e\n\u003ch2 id=\"语言学乐子\"\u003e语言学乐子\u003c/h2\u003e\n\u003ch3 id=\"皮钦语-pidgin\"\u003e皮钦语 (pidgin)\u003c/h3\u003e\n\u003cp\u003e大家对那些 1.言语中不时夹杂着英文单词 2.装/凡尔赛 的人表现出一种厌恶。例如，下面是某恋综里的一段留子对话的名场面：\u003c/p\u003e","title":"一些语言学的梗和有意思的知识"},{"content":"\u0026#x1f4a1; This post is mainly focused on text models. For multi-modal models, please refer to this post.\nThe Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.\nCircuit Discovery Methods basic activation patching (causal mediation/interchange interventions\u0026hellip;) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? \u0026#x1f52d; resources inspirition Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned (ROME) Locating and Editing Factual Associations in GPT Attribution patching: Activation patching at industrial scale (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability Attribution Patching Outperforms Automated Circuit Discovery AtP*: An efficient and scalable method for localizing llm behaviour to components Causal Scrubbing: a method for rigorously testing interpretability hypotheses new Using SAE Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models Automatically Identifying Local and Global Circuits with Linear Computation Graphs Contextual Decomposition Mechanistic Interpretation through Contextual Decomposition in Transformers Edge Pruning ? Finding Transformer Circuits with Edge Pruning Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning Evaluation lack of ground truth\nhuman interpretability automatic faithfulness (see this)\nhow much of the full model’s performance can a circuit account for.\ncompleteness\ncomputational efficiency Hypothesis Testing the Circuit Hypothesis in LLMs Issues ablation methods: dropout out is also an ablation, so does zero ablation work? superposition, need the help of SAE? Resources nnpatch path patching (by callummcdougall) Dictionary Learning SAE\nTraining and optimization proper SAE width dead neurons \u0026#x1f52d; resources A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models Circuit Tracing: Revealing Computational Graphs in Language Models Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders Sparse Crosscoders for Cross-Layer Features and Model Diffing Open Source Replication of Anthropic’s Crosscoder paper for model-diffing Transcoders Find Interpretable LLM Feature Circuits Scaling and evaluating sparse autoencoders Improving Dictionary Learning with Gated Sparse Autoencoders Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Evaluation human auto \u0026#x1f52d; resources SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability website (Anthropic, 2024-8, contrastive eval \u0026amp; sort eval) Interpretability Evals for Dictionary Learning (RAVEL) RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations Language models can explain neurons in language models Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control Analysis A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders Are Sparse Autoencoders Useful? A Case Study in Sparse Probing Applications SAE + feature discovery \u0026#x1f52d; resources Sparse Autoencoders Find Highly Interpretable Features in Language Models Anthropic research Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet SAE + circuit discovery \u0026#x1f52d; resources Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models SAE + explain model components Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors Sparse Autoencoders Work on Attention Layer Outputs Interpreting Attention Layer Outputs with Sparse Autoencoders SAE + explain model behaviors The Geometry of Concepts: Sparse Autoencoder Feature Structure SAE + model steering SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders Steering vectors activation steering Steering Language Models With Activation Engineering (lesswrong) Steering GPT-2-XL by adding an activation vector Steering Llama 2 via Contrastive Activation Addition Inference-Time Intervention: Eliciting Truthful Answers from a Language Model Function vectors in large language models feature steering Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Evaluating feature steering: A case study in mitigating social biases representation engineering Representation Engineering: A Top-Down Approach to AI Transparency Others LUNAR: LLM Unlearning via Neural Activation Redirection Mechanistically Eliciting Latent Behaviors in Language Models Model Diffing Stage-wise fine-tuning fine-tuning interp fine-tuning\nscaling Stage-Wise Model Diffing Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks Understanding Catastrophic Forgetting in Language Models via Implicit Inference Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To! intrinsic dimension Measuring the Intrinsic Dimension of Objective Landscapes Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning Dataset-wise Modeldiff: A framework for comparing learning algorithms Algorithm-wise Adversarial robustness as a prior for learned representations Representation equivariance meta-SNE Visualizing Representations: Deep Learning and Human Beings model stitching Understanding image representations by measuring their equivariance and equivalence Revisiting model stitching to compare neural representations SVCCA and similar methods SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability others analyses The Platonic Representation Hypothesis theory When Representations Align: Universality in Representation Learning Dynamics Explain Model Components explain neurons, attention heads and circuits\nExplain neurons \u0026#x1f52d; resources Finding Neurons In A Haystack LatentQA: Teaching LLMs to Decode Activations Into Natural Language Language models can explain neurons in language models Multimodal Neurons in Artificial Neural Networks Finding Safety Neurons in Large Language Models Explain attention heads different heads in one layer/heads in different layer -\u0026gt; grammar/semantic feats\nattention pattern\nAnalyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention 为什么Transformer 需要进行 Multi-head Attention？ special heads\nCopy Suppression: Comprehensively Understanding An Attention Head Explain circuits understand specific circuits on the subspace level\n(IOI) Interpretability in The Wild: A Circuit For Indirect Object Identification in GPT-2 Small What Do the Circuits Mean? A Knowledge Edit View (DAS) Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations LLM Circuit Analyses Are Consistent Across Training and Scale Explain layernorm On the Nonlinearity of Layer Normalization Others The Quantization Model of Neural Scaling Explain Model Behaviors Representations linear representations theory Linear Explanations for Individual Neurons multilingual representations Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs Emerging Cross-lingual Structure in Pretrained Language Models Probing the Emergence of Cross-lingual Alignment during LLM Training Exploring Alignment in Shared Cross-lingual Spaces mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models? Probing LLMs for Joint Encoding of Linguistic Categories Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure multimodal representations Interpreting the Second-Order Effects of Neurons in CLIP Interpreting CLIP\u0026rsquo;s Image Representation via Text-Based Decomposition safety reprs \u0026#x1f52d; resources Linear Representations of sentiment in large language models nonlinear representations Not All Language Model Features Are Linear other analyses The Origins of Representation Manifolds in Large Language Models Layer by Layer: Uncovering Hidden Representations in Language Models From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning Inference-Time Decomposition of Activations (ITDA): A Scalable Approach to Interpreting Large Language Models SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability (CKA) Similarity of Neural Network Representations Revisited 神经网络表征度量（一） Model capabilities training (learning) dynamics\nin-context learning\nbasic In-context Learning and Induction Heads What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation bad in-context learning (learn wrong things) Overthinking The Truth: Understanding How language Models Process False Demonstrations chain of thought (COT)\nhow and why step by step?\nzero-shot COT ???\nanalyses Iteration Head: A Mechanistic Study of Chain-of-Thought How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation Do Large Language Models Latently Perform Multi-Hop Reasoning? unfaithful COT Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting Does the model already know the answer while reasoning, or the model really has a goal? reasoning\nUnveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization How Do LLMs Perform Two-Hop Reasoning in Context? planning\nEvaluating Cognitive Maps and Planning in Large Language Models with CogEval instruction following\nhow does reinforcement learning change the inside of a model? understand RL at mechanistic level high efficient RLxF SAIF: A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models knowledge\n*Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models *(Entity Recognition and Hallucinations) On the Biology of a Large Language Model *Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level (Post 1) Dissecting Recall of Factual Associations in Auto-Regressive Language Models Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts Locating and Editing Factual Associations in GPT website Characterizing Mechanisms for Factual Recall in Language Models Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans The Geometry of Concepts: Sparse Autoencoder Feature Structure memorization \u0026amp; generalization phase transition\nWhat Do Learning Dynamics Reveal About Generalization in LLM Reasoning? In-context Learning and Induction Heads grokking Progress Measures For Grokking Via Mechanistic Interpretability Towards Understanding Grokking: An Effective Theory of Representation Learning learning dynamics\nLearning Dynamics of LLM Finetuning LensLLM: Unveiling Fine-Tuning Dynamics for LLM Selection Loss Landscape Degeneracy Drives Stagewise Development in Transformers Loss landscape geometry reveals stagewise development of transformers Multi-Component Learning and S-Curves Deep double descent duplication\nself-repair\nThe Hydra Effect: Emergent Self-repair in Language Model Computations Explorations of Self-Repair in Language Models massive activations\nMassive Activations in Large Language Models Systematic Outliers in Large Language Models Narrow tasks counting greater-than How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model Indirect Object Indentification (IOI) Interpretability in The Wild: A Circuit For Indirect Object Identification in GPT-2 Small gender Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias Concept-based interpretability Concept-Based Mechanistic Interpretability Using Structured Knowledge Graphs Intrinsic Interpretability Modifying Model Components (SoLU) Softmax Linear Units Reengineering Model Architecture (Except interpretable model architectures, I also list some fresh-new architectures.)\nCBM (Concept Bottleneck Models)\nConcept Bottleneck Models Concept Bottleneck Large Language Models Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization Backpack Language Models\nBackpack Language Models others\nSeeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability SEER: Self-Explainability Enhancement of Large Language Models’ Representations Modular Training of Neural Networks aids Interpretability Compositional attention networks for machine reasoning new architetures\nnew structures Transformers without normalization new modes V-jepa: Latent video prediction for visual representation learning V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning A Path Towards Autonomous Machine Intelligence Large Concept Models: Language Modeling in a Sentence Representation Space Large Language Diffusion Models Fractal Generative Models (VLM single transformer) The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer JEPA Brain Inspired Findings Creations Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration Other Methods \u0026amp; Analysis decomposing a model (APD) Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition (SPD) Stochastic Parameter Decomposition Interesting We Can’t Understand AI Using our Existing Vocabulary Applications AI alignment 3H (helpful, honest, harmless) Avoid bias and harmful behaviors\nconcept-based interpretability representation-based interpretability red-teaming perturbations\nbackdoor detection, red-teaming, capability discovery\nanomaly detection\nbackdoor detection\nSleeper Agents: Training Deceptive LLMs that Persist Through Safety Training SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks Mechanistic anomaly detection and ELK A gentle introduction to mechanistic anomaly detection Concrete empirical research projects in mechanistic anomaly detection jailbreak circuit; SAE; steering vector (anti-refusal)\nmeasures (repr engineering) Improving Alignment and Robustness with Circuit Breakers (steering) Refusal in Language Models Is Mediated by a Single Direction (steering) Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models (training) Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications analyses Universal and Transferable Adversarial Attacks on Aligned Language Models Many-shot jailbreaking Jailbroken: How Does LLM Safety Training Fail? Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs (vlm) Visual Adversarial Examples Jailbreak Aligned Large Language Models (vlm) Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models (vlm) Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything evaluation \u0026amp; benchmark Jailbreak prompts finding on Twitter JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models power-seeking\nParametrically Retargetable Decision-Makers Tend To Seek Power social injustice prejudice, gender bias: doctor \u0026amp; nurse, discrimination\ntraining dynamics; dataset; gradient descent; SAE circuits\nEvaluating feature steering: A case study in mitigating social biases Prejudice and Volatility: A Statistical Framework for Measuring Social Discrimination in Large Language Models Unveiling Gender Bias in Large Language Models: Using Teacher’s Evaluation in Higher Education As an Example deception\ndishonesty\nAlignment faking in large language models Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training Language Models Learn To Mislead Humans Via RLHF How do Large Language Models Navigate Conflicts between Honesty and Helpfulness? reward hacking\nReward Hacking in Reinforcement Learning measurement tampering\nThe AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome.\nBenchmarks for Detecting Measurement Tampering persona drift\nMeasuring and Controlling Persona Drift in Language Model Dialogs Towards Interpreting Visual Information Processing in Vision-Language Models other human values\nSycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models Reducing sycophancy and improving honesty via activation steering Modulating sycophancy in an RLHF model via activation steering Agency\nUnderstanding and Controlling a Maze-Solving Policy Network Parametrically Retargetable Decision-Makers Tend To Seek Power Avoiding Side Effects in Complex Environments Alignmnet theory\nBoth alignment and interpretability are related to AI safety, so the mech interp tools are widely used in alignment research. I\u0026rsquo;ll put some good resources of alignment work here.\nqualitative work (findings, analyses, concepts, \u0026hellip;) * alignment representation * instrumental convergence * shard theory\nProposed by Alex Turner (TurnTrout) and Quintin Pope The Shard Theory of Human Values (lesswrong) The Shard Theory of Human Values \u0026#x1f52d; resources AI Alignment: A Comprehensive Survey Representation Engineering: A Top-Down Approach to AI Transparency Mechanistic Interpretability for AI Safety A Review Improved Algorithms ReFT: Representation Finetuning for Language Models Harmonic Loss Trains Interpretable AI Models influence functions Studying Large Language Model Generalization with Influence Functions Influence Functions for Preference Dataset Pruning Research Limitations Current work mainly focuses on Transformer-based models. Is transformer a inevitable model structure for generative language models? How can we use post-hoc methods as a guide for training a more interpretable and controllable model? Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable? Other Interpretability Fields Neural network interpretability Not necessarily a transformer-based model, maybe an lstm or simply a toy model\nTheories for DL foocker/deeplearningtheory Feature Learning Huang wei\u0026rsquo;s repo game (chess) Evidence of Learned Look-Ahead in a Chess-Playing Neural Network (Sokoban, planning) Planning behavior in a recurrent neural network that plays Sokoban geometry Reasoning in Large Language Models: A Geometric Perspective Theories for Transformer A Mathematical Theory of Attention Bertology A Primer in BERTology: What we know about how BERT works What do you mean, BERT? Assessing BERT as a Distributional Semantics Model Other Surveys Open Problems in Mechanistic Interpretability A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models Mechanistic Interpretability for AI Safety: A Review A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models Towards Uncovering How Large Language Model Works: An Explainability Perspective Math Blogs 让人惊叹的Johnson-Lindenstrauss引理：理论篇 让人惊叹的Johnson-Lindenstrauss引理：应用篇 n维空间下两个随机向量的夹角分布 最小熵原理（六）：词向量的维度应该怎么选择？ 为什么transformer的FFN需要先升维再降维？ ","permalink":"https://Siriuslala.github.io/posts/mech_interp_research/","summary":"\u003cp\u003e\u0026#x1f4a1;\nThis post is mainly focused on text models. For multi-modal models, please refer to \u003ca href=\"https://Siriuslala.github.io/posts/mm_interp/\"\u003ethis post\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"the-purpose-i-write-this-blog\"\u003eThe Purpose I Write This Blog\u003c/h2\u003e\n\u003cp\u003e   To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.\u003c/p\u003e\n\u003ch2 id=\"circuit-discovery\"\u003eCircuit Discovery\u003c/h2\u003e\n\u003ch3 id=\"methods\"\u003eMethods\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ebasic\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eactivation patching (causal mediation/interchange interventions\u0026hellip;)\u003c/li\u003e\n\u003cli\u003epath patching\u003c/li\u003e\n\u003cli\u003escaling techinques: attribution patching\u003c/li\u003e\n\u003cli\u003eDAS (distributed alignment search)   directional activation patching?\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"telescope-resources\"\u003e\u0026#x1f52d; resources\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003einspirition\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/1905.09418\"\u003eAnalyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2202.05262\"\u003e(ROME) Locating and Editing Factual Associations in GPT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.neelnanda.io/mechanistic-interpretability/attribution-patching\"\u003eAttribution patching: Activation patching at industrial scale\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2304.14997\"\u003e(ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2310.10348\"\u003eAttribution Patching Outperforms Automated Circuit Discovery\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2403.00745\"\u003eAtP*: An efficient and scalable method for localizing llm behaviour to components\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing\"\u003eCausal Scrubbing: a method for rigorously testing interpretability hypotheses\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003enew\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eUsing SAE\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2403.19647\"\u003eSparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2405.13868\"\u003eAutomatically Identifying Local and Global Circuits with Linear Computation Graphs\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eContextual Decomposition\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2407.00886\"\u003eMechanistic Interpretation through Contextual Decomposition in Transformers\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eEdge Pruning ?\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/pdf/2406.16778\"\u003eFinding Transformer Circuits with Edge Pruning\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2407.03779\"\u003eFunctional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"\"\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"evaluation\"\u003eEvaluation\u003c/h3\u003e\n\u003cp\u003elack of ground truth\u003c/p\u003e","title":"Possible Research Areas in Mechanistic Interpretability"},{"content":"\u0026#x1f3b6;Code in this post can be found at the jupyter notebook in my \u0026ldquo;saeExploration\u0026rdquo; repo.\nFind features that reflect positive emotions To find the features related to a specific emotion, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:\n1 2 3 4 5 prompt_happy = [\u0026#34;I\u0026#39;ll be on a vacation tomorrow and I\u0026#39;m so happy.\u0026#34;, \u0026#34;My mombrings home a new puppy and I\u0026#39;m so happy.\u0026#34;, \u0026#34;I\u0026#39;m so glad I got the job I wanted.\u0026#34;, \u0026#34;I feel so happy when I\u0026#39;m with my friends.\u0026#34;, \u0026#34;I\u0026#39;m so happy I got the promotion I wanted.\u0026#34;,] I choose to look for features that reflect happiness and sadness. Apart from that, I also wonder if the feature that reflects excitedness has something to do with the one that reflects happiness (they are alike from the semantic level at least.)\nFor a start, I inspected the residual stream in layer_7. The SAE we choose is gpt2-small-res-jb which hooks at the residual stream at the entrance of a layer. The prompts were fed into the model and the outputs were SAE activations. I checked the activations at the word “happy” for all the prompts and calculated the mean value of them. I visualized them as below:\nFigure 1: Feature activations at the keywords for happy emotion Obviously there are three SAE features that activate most actively on the happy emotion, and their feature ids are 2392, 9840 and 21753. I checked the feature 2392 in Neuronpedia and got its feature dashboard:\nFigure 2: SAE feature 2393 on the dashboard (https://neuronpedia.org/gpt2-small/7-res-jb/2392?embed=true\u0026embedexplanation=true\u0026embedplots=true\u0026embedtest=true\u0026height=300). Later I found features for sadness and excitedness in the same way. The top-3 features activating for “sad” and “excited” are [2045, 23774, 10866] and [8935, 9840, 3247] respectively.\nCompare the features related to happiness and excitedness I want to see the difference between \u0026ldquo;happy features\u0026rdquo; and \u0026ldquo;excited features\u0026rdquo; since they are both positive emotions. So I compared their top-3 features and only kept the features that activate on both “happy” and “excited”. They are visualized as below:\nFigure 3: Feature activations on both happy and excited emotions at the residual pre stream in layer_7. From the figure above, we can easily find out the two features shared by happiness and excitedness. It seems that these two features contain positive emotion concepts, which means they are close to each other on the semantic level.\nA deeper investigation into the features related to happiness From the previous result, we can see that there are 3 features that fire quite actively on happiness. Since they share similar semantic meanings, I guess that the representations of features activating on the same emotion (i.e. 2392, 9840 and 21753) have high similarities. To prove this, I try to inspect the representation of one prompt which expresses happiness and calculate the cosine similarities among representations of different features. Here the representations of different layers are calculated as below: $$ feat \\_ repr = W_{dec} * SAE \\_ activations $$ Note that the * operation is a dot product. The W_dec in the formula above is the decoder matrix of SAE with a shape of (d_sae, d_model). We know that according to the definition of dictionary learning, each vector in the dimension of d_sae corresponds to a base vector for an SAE feature. Each element in the SAE_activations can be seen as the intensity of the feature at that position. So by multiplying W_dec and SAE_activations we can get a representation of shape $ (d \\_ model,) $ which expresses the features in the vector space of SAE that is sparser and more interpretable than that of the original model.\nTo visualize the similarities among features, I choose to use the heatmap. Note that in order to make it clear, I mainly focus on features 2392, 9840 and 21753 and using negative sampling to get some other features for comparison. I randomly pick 6 features except for the three features mentioned above. The similarities are calculated and shown as below:\nFigure 4: Feature similarities on the word “happy” at the residual pre stream in layer_7. Obviously we can find that the three features that fire most actively on “happy” are more similar to each other (the top left 3*3 square), thus my hypothesis is proved intuitively.\nFeatures in different layers Figure 5: Feature activations on the word “happy” at the residual pre stream in 12 layers. \u0026ensp;\u0026ensp;Previously I inspected in the 8th layer of gpt2-small and found some emotional features. Now I want to know if similar features exist in other layers, and how they are related to each other. I inspect the autoencoder features in each layer and observe their activations on the word “happy”. The result is shown in Fig 5. We can find that: * Eachlayer has more than 3 features that activate on the happy emotion. * Thepositions of activating features are different from those in other layers. Though the positions of activating features are different, I guess it\u0026rsquo;s just the problem of feature orders. For example, the feature 7683 in layer_1 may be the same kind or even exactly the same feature as feature 13928 in layer_3, though the positions are different. In order to prove this, I choose to visualize the feature representations of SAE outputs. The result is shown below:\nFigure 6: Feature similarities on the word “happy” at the residual pre stream in 12 layers. From the figure above, I got some interesting points about the feature distribution:\nThe features in the first layer have lower similarities with those in later layers. I guess this is because the first layer gets limited information from previous layers, so it cannot express relatively complicated concepts like emotions. I think maybe the first layer contains some low level concepts that are not shown in this figure, which I will explore in the future. Afeature in a layer often corresponds to a feature in another layer. For example, feature 7683 in layer_1 corresponds to feature 13928 in layer_2, which has a similarity of 0.939. This means they are related to each other across different layers, sharing similar semantic meanings. Afeature in a layer tends to be more alike to features in nearby layers. For example, feature 7683 in layer_1 is more similar to feature 13928 in layer_2 than feature 2392 in layer_7, with a similarity of 0.939 to 0.825. I think it\u0026rsquo;s because the vector spaces of nearby layers are relatively close to each other. When two layers are far from each other, the difference between their vector spaces is significant due to a lot of linear and nonlinear manipulations between layers. Thus the features would share low similarities regardless of similar semantic meanings. ","permalink":"https://Siriuslala.github.io/posts/happy_feats/","summary":"\u003cp\u003e\u0026#x1f3b6;Code in this post can be found at \u003ca href=\"https://github.com/Siriuslala/saeExploration/blob/main/multilingual_study.ipynb\"\u003ethe jupyter notebook in my \u0026ldquo;saeExploration\u0026rdquo; repo\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"find-features-that-reflect-positive-emotions\"\u003eFind features that reflect positive emotions\u003c/h2\u003e\n\u003cp\u003eTo find the features related to a specific emotion, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\" id=\"hl-0-1\"\u003e\u003ca class=\"lnlinks\" href=\"#hl-0-1\"\u003e1\u003c/a\u003e\n\u003c/span\u003e\u003cspan class=\"lnt\" id=\"hl-0-2\"\u003e\u003ca class=\"lnlinks\" href=\"#hl-0-2\"\u003e2\u003c/a\u003e\n\u003c/span\u003e\u003cspan class=\"lnt\" id=\"hl-0-3\"\u003e\u003ca class=\"lnlinks\" href=\"#hl-0-3\"\u003e3\u003c/a\u003e\n\u003c/span\u003e\u003cspan class=\"lnt\" id=\"hl-0-4\"\u003e\u003ca class=\"lnlinks\" href=\"#hl-0-4\"\u003e4\u003c/a\u003e\n\u003c/span\u003e\u003cspan class=\"lnt\" id=\"hl-0-5\"\u003e\u003ca class=\"lnlinks\" href=\"#hl-0-5\"\u003e5\u003c/a\u003e\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eprompt_happy\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;I\u0026#39;ll be on a vacation tomorrow and I\u0026#39;m so happy.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\u0026#34;My mombrings home a new puppy and I\u0026#39;m so happy.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\u0026#34;I\u0026#39;m so glad I got the job I wanted.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\u0026#34;I feel so happy when I\u0026#39;m with my friends.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"s2\"\u003e\u0026#34;I\u0026#39;m so happy I got the promotion I wanted.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\u003cp\u003eI choose to look for features that reflect happiness and sadness. Apart from that, I also wonder if the feature that reflects excitedness has something to do with the one that reflects happiness (they are alike from the semantic level at least.)\u003c/p\u003e","title":"Exploring Emotional Features in GPT2-Small"},{"content":"\u0026#x26a0;\u0026#xfe0f; Warnings\nThis post was written when I first delved into this area, and it hasn\u0026rsquo;t been updated for a long time. Thus there might be a lot of errors. Now I\u0026rsquo;ve changed my attitude to this area. The area is not well-defined, and most of the research in this area is of low quality and is not appealing to me. Besides, I think the study of interpretability should be applied to pratical use, though we can also study it for fun. I\u0026rsquo;m still interested in interpretability and its applications. I\u0026rsquo;ll write something new and interesting later ~ \u0026#x1f4a1; This post is accompanied with another post, which contains specific content in this area.\nThe purpose I write this blog Mechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challenges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc). Thus it\u0026rsquo;s a little bit difficult for people new to this area to figure out what researchers are really doing.\nTherefore I write this blog to give a brief introduction to mechanistic interpretability without so much of horrible concepts. The blog aims to help you understand the basic ideas, main directions and latest achievements of this field, providing a list of resources to help you get started at the same time!\nIf you really want to do some cool research as a beginner, I highly recommend the guide by Neel Nanda.\nWhat is Mechanistic interpretability? Speaking of AI research, neural network is the tool that is used most widely nowadays for its excellent representation and generalization ability. What does a neural network do? It receives an input and gives an output after some calculations. Specifically speaking, it usually gets the representations of an input and maps it to an expected output under a predefined computation graph. From my perspective, neural networks mainly care about two things: create representations to extract features and establish the relationship between the representations and the output.\nWhy is neural network so popular? An important reason is that the neural network can save a lot of time for researchers to manually design features. For example, for natural language processing (NLP) people often designed features like \u0026ldquo;the frequency of a word that appears\u0026rdquo; or \u0026ldquo;the co-occurrence probabilities\u0026rdquo; in the past. Manually designing features caused too much labor, so people choose to use neural networks to find features automatically. As for optimizing, they set a goal of minimizing the loss function and using backward propagation (BP) to update the parameters of the model. Thus neural networks free our hands and improve performance at the same time.\nAll is well, so why do we concern about interpretability? Though neural networks can extract a lot of features with a high efficiency, we cannot have a clear understanding of what the features really are. For example, we know that a filter with Laplacian operator can extract the edge of an image, but we don\u0026rsquo;t know what the features extracted by a convolution layer mean because the parameters of the filters inside are often randomly initialized and optimized using BP algorithm. As a result, features in neural networks are often ambiguous.\nWhy is interpretability important? Actually this statement is controversial because some people say interpretability is bullshit\u0026#x1f4a9;. I\u0026rsquo;m not angry about this. Anyway, people\u0026rsquo;s taste varies, just like many people enjoy Picasso\u0026rsquo;s abstract paintings while I don\u0026rsquo;t. Interpretability still lacks exploring so it\u0026rsquo;s now far from application, and that\u0026rsquo;s why some people look down on it. While it is this lack of exploration that excites me most because there are a lot of unknown things waiting for me to discover! Actually, Interpretability is a key component in the AI alignment cycle (see Figure 0). The goal of alignment is to \u0026ldquo;make AI systems behave in line with human intentions and values\u0026rdquo; and interpretability plays an important role in ensuring AI safety. For example, unwanted things like malicious text generated by a language model may be avoided using model steering (a trick played on the activations during the forward propagation). Besides, having a clear understanding of neural networks enables us to focus on the relevant part of a model to a specific task and perform fine-tuning in a more precise way (haha here is an ad for my project: circuit-tuning).\nFigure 0: The position of interpretability in the AI alignment cycle (from this survey)\rLast question: what is mechanistic interpretability? Let\u0026rsquo;s call it mech interp first because I\u0026rsquo;m really tired of typing the full name\u0026#x1f4a6;. There seems not to be a rigorous definition, but I here I want to quote the explanation by Chris Olah:\nMechanistic interpretability seeks to reverse engineer neural networks, similar to how one might reverse engineer a compiled binary computer program.\nAnother thing: there are various of categories of interpretability, such as studies from the geometry perspective or from the game theory and symbol system perspective, which can be found at ICML, ICLR, NeurlPS, etc. When we say mech interp, we often refer to the studies on Transformer-based generative language models now (though the research started before 2017) which will be introduced briefly in the next section. So before we start, let\u0026rsquo;s briefly go over the structure of Transformer first!\nFigure 1: The structure of Transformer (from Arena)\rFigure 2: The structure of the self-attention block in a Transformer block (from Arena)\rFigure 3: The structure of the MLP layer in a Transformer block (from Arena)\rFigure 4: The structure of the layer normalization in a Transformer block (from Arena)\rBasic ideas and research topics In this section, I\u0026rsquo;m gonna explain some terms for mech interp and help you understand the basic ideas of doing mech interp research. I\u0026rsquo;ll try to make it easy!\nNote that I\u0026rsquo;ll only introduce something that I think is important. If you wanna learn more about the concepts in mech interp, please refer to: A Comprehensive Mechanistic Interpretability Explainer \u0026amp; Glossary which is a very comprehensive guide for beginners that I strongly recommend!\nImportant concepts Features\nThere are a lot of definitions for features. Unfortunately none of the definitions above can be widely recognized, so it\u0026rsquo;s open for anyone who wants to seek for the essence of the features. Generally speaking, a feature is a property of an input which is interpretable or cannot be understand by humans. Practically speaking, a feature could be an activation value of a hidden state in a model (at least lots of work is focusing on this).\nHow to find a feature? Or how to know that the thing you find is likely to be a feature? Here I want to quote the concept of \u0026ldquo;the signal of structure\u0026rdquo; proposed by Chris Olah in the post of his thoughts on qualitative research:\nThe signal of structure is any structure in one\u0026rsquo;s qualitative observations which cannot be an artifact of measurement or have come from another source, but instead must reflect some kind of structure in the object of inquiry, even if we don\u0026rsquo;t understand it.\nJust like the discovery of cells under a microscope. The shape of the cells cannot be random noise but strong evidence for the structure of them.\nCircuits\nIf we view a language model as a directed acyclic graph (DAG) $M$ where nodes are terms in its forward pass (neurons, attention heads, embeddings, etc.) and edges are the interactions between those terms (residual connections, attention, projections, etc.), a circuit $C$ is a subgraph of $M$ responsible for some behavior. That means the components inside the circuit have a big influence on the output of the task, while the components outside the subgraph have almost no influence.\nFrom my perspective, a circuit is a path from an input to an output, just like the way between two hosts in the routing networks.\nFigure 5: The computational graph of a model (from Arena)\rSuperposition\nSuperposition is a hypothesis that models can represent more features than the dimensions they have.\nIdeally we expect that each neuron only corresponds to one feature, so we can investigate or even control the feature using the neuron reserved for it. But in practice we find that a neuron fires for more than one features, which is called the phenomenon of polysemanticity in neurons. We believe that we have more features than model dimensions, so we can also say that more than one neurons fire when a feature appears. That is to say, there is not a one-to-one correspondence between neurons and features.\nPrivileged basis\nIt\u0026rsquo;s a weird idea that I have some doubt on it (maybe I haven\u0026rsquo;t grasp the core idea of it\u0026hellip;).\nMy understanding: There are many vector spaces in a model, for example, the residual stream in a layer, the output of the ReLU in a MLP layer, etc. Each vector space can be seen as a representation. Given an input, we can get the hidden states in different vector spaces during the forward propagation of the model. If we could view neurons as directions which may correspond to features in a vector space, then we say there is a privileged basis in this vector space. That is to say, each value at a specific dimension is aligned with a neuron, and that value may be a interpretable feature (maybe not).\nNot all vector spaces in a model have privileged basis. The most accepted view is that privileged bases exist in attention patterns and MLP activations, but not in residual streams. A general law is that a privilege basis often appears with a elementwise nonlinear operation, for instance, ReLu, Softmax, etc. If the operations around a representation are all linear, then we say the basis in the representation is non-privileged. For example, the operations around a residual stream are often non-linear (e.g. $W_{in}$ and $W_{out}$ of a MLP layer which correspond to the \u0026ldquo;read\u0026rdquo; and \u0026ldquo;write\u0026rdquo; operation on the residual stream). If we apply a rotation matrix to the original operations to change the basis, then the result will be unchanged because In other words, something is a privileged basis if it is not rotation-independent, i.e. the nature of computation done on it means that the basis directions have some special significance. A privileged basis is a meaningful basis for a vector space. That is, the coordinates in that basis have some meaning, that coordinates in an arbitrary basis do not have. It does not, necessarily, mean that this is an interpretable basis.\na space can have an interpretable basis without having a privileged basis. In order to be privileged, a basis needs to be interpretable a priori - i.e. we can predict it solely from the structure of the network architecture.\nResearch techniques Circuits Discovery\nFinding the circuit for a specific task attracts the attention of lots of researchers. The thing we wanna do is to get the relevant components for a specific task. A naive idea is to test the components one by one using causal intervention: change the value of one component while keeping others unchanged, and check if it influences the output. This technique is also called ablation or knockout.\nTo achieve this, we have two possible ways: denoising (find useful components) and noising (delete unuseful components). We usually prepare a clean prompt which is relevant to the task (results in a correct answer) and a corrupted prompt which has nothing to do with the task. Before finding circuits, the two prompts are fed into the model separately to get a clean run and a corrupted run.\nIf we use denoising, at each step we replace (patch) the value of a component in the corrupted run with that in the clean run. If the output is closer to the correct answer under a specific metric (e.g. KL divergence or logit difference), then we add the component into the circuit. If we use noising, then we should replace a component in the clean run with that in the corrupted run. If the output is almost unchanged under a threshold, then we regard the component as useless and delete it. Generally speaking, denoising is better than noising. To understand this, I want to quote a line in Arena: noising tells you what is necessary, denoising tells you what is sufficient.\nSeveral techniques in this area:\nactivation patching (aka causal mediation/interchange interventions\u0026hellip;) A method for circuits discovery that take nodes into consideration. path patching A variant of activation patching that also take edges into consideration to study which connections between components matter. For a pair of components A and B, we patch in the clean output of A, but only along paths that affect the input of component B. While in activation patching, all the subsequent components after A are affected. attribution patching An approximation of activation patching using a first-order Taylor expansion on the metric. This method is used to speed up circuits finding. Figure 6: Comparison of activation patching and path patching (from Arena)\rThe difference between activation patching and path patching are shown in Figure 6. In activation patching, we simply patch the node $D$ with $D\u0026rsquo;$, so the nodes after $D$ ($H, G$ and $F$) are affected. While in path patching, we patch edges rather than nodes. For example, we only want to patch the edge $D \\to G$, which means the only change is the information from node $D$ to node $G$. As a result, only $G$ and $F$ are affected while $H$ isn\u0026rsquo;t.\n\u0026#x1f52d; Recommended papers: (ROME) Locating and Editing Factual Associations in GPT (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability (attribution patching) Attribution patching: Activation patching at industrial scale (IOI) INTERPRETABILITY IN THE WILD: A CIRCUIT FOR INDIRECT OBJECT IDENTIFICATION IN GPT-2 SMALL Dictionary Learning\nDictionary Learning aims to deal with the problem of superposition. The idea is like compression sensing in the field of signal processing and is discussed in this article. The implementation of dictionary learning is to train a sparse autoencoder (SAE).\nAn autoencoder consists of an encoder and a decoder. The encoder receives an input and compresses it to a lower dimension, and the decoder maps the hidden representation to the original input. The goal of the autoencoder is to get the representation of the input while compressing it. The autoencoder is optimized using the reconstruction loss.\nCompared with the autoencoedr, the dimension of the hidden representation in SAE is always higher than that of the input, which means the SAE does something completely opposite to the autoencoder. The idea behind is that the model dimension is smaller than the number of features to represent. The model may use superposition to make full use of limited neurons to represent more features. To get one-to-one correspondence between neurons and features, we map the representation to a higher dimensional vector space with SAE encoder. Once we get the representation in SAE (let\u0026rsquo;s call it sparse features), we maps it back to the original input with SAE decoder.\nIn practice, any hidden state in a model can be studied using SAE. For example, when we want to get the sparse features of the activations $h$ in a MLP layer. We can do as follows:\n$$ z = ReLU(W_{enc}h + b_{enc}) $$ $$ h^{\\prime} = W_{dec}z + b_{dec} $$\n$$ loss = \\mathbb{E}_{h}\\left[||h-h^{\\prime}||_{2}^{2} + \\lambda||z||\\right] $$\nNote that $ h = [h_{1}, h_{2},\u0026hellip;,h_{n}]^{T} \\in \\mathbb{R}^{n\\times1} $ is a hidden state with $n$ dimensions, and each $h_{i} \\in H$ is the value of a specific dimension $i$. $W_{enc} \\in \\mathbb{R}^{m\\times n}$ maps the hidden state to a new vector space with dimension $m\u0026gt;n$, $W_{dec} \\in \\mathbb{R}^{n\\times m}$ maps the sparse features back to the original shape, $ b_{enc} \\in \\mathbb{R}^{n} $ and $ b_{dec} \\in \\mathbb{R}^{n} $ are learned bias. The loss function consists of two parts: the MSE loss as the reconstruction loss and L1 norm with a coefficient $\\lambda$ to encourage the sparsity of feature activations. It is the regularization term that separates SAE from ordinary autoencoders, so as to discourage superposition and encourage monosemanticity.\nTo better understanding the encoder and decoder in SAE, we can write a sparse feature $ f_{i} $ as an element of $ z $ :\n$$ f_{i}(h) = z_{i} = ReLU(W^{enc}_{i,.}\\cdot h + b^{enc}_{i}) $$\nEach sparse feature $ f_{i} $ is calculated using row $i$ of the encoder weight matrix. As for decoder, we can write $ h^{\\prime} $ as:\n$$ h^{\\prime} = \\sum_{i=1}^{m}f_{i}(h) \\cdot W^{dec}_{.,i} + b_{dec} $$\n;The reconstructed activation $h^\\prime$ can been seen as a linear addition of all the features. Each column of the decoder matrix corresponds to a feature, so we call it a \u0026ldquo;feature direction\u0026rdquo;. Note that sometimes the L1 norm term in the loss function can be replaced by $ \\lambda\\sum_{i=1}^{m}f_{i}(h)||W^{dec}_{.,i}||_{2} $ which places a constraint to the decoder weights to reduce ambiguity in the addition operation (we want only one or a few features to be large).\nFor simplicity, The details of the model structure, training method and evaluation will not be shown here.\n\u0026#x1f52d; Recommended papers: Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Model Steering\nA useful technique for eliciting certain model behaviors in a mechanistic way.\n\u0026#x1f52d; Recommended papers: Activation Addition: Steering Language Models Without Optimization (lesswrong) Steering GPT-2-XL by adding an activation vector Steering Llama 2 via Contrastive Activation Addition Mechanistically Eliciting Latent Behaviors in Language Models Research Areas Figure 7: The route of mech interp (from transformer-circuits.pub/2024/july-update/)\rTheory\nUnderstand model components Understand model behaviors Application\ninterpretable model structure AI alignment\nAvoid bias and harmful behaviors Some Useful Resources Here I list some resources that would be helpful for you to get started quickly in the field.\nTutorials Arena A tutorial created and maintained by Callum McDougall et al, providing a guided path for anyone who finds themselves overwhelmed by the amount of technical AI safety content out there. Neel Nanda\u0026rsquo;s Tutorial Neel\u0026rsquo;s tutorial for mech interp. Neel Nanda\u0026rsquo;s Quickstart Guide A quick start for mech interp. Neel Nanda\u0026rsquo;s remommended papers Some classic and important papers for mech interp. Neel Nanda\u0026rsquo;s problems v1 Neel\u0026rsquo;s old questions for mech interp. Neel Nanda\u0026rsquo;s problems v2 Neel\u0026rsquo;s 200 new questions for mech interp. Alignment Research Field Guide (by the MIRI team) Frameworks and Libraries TransformerLens A library maintained by Bryce Meyer and created by Neel Nanda. SAELens Originates from TransformerLens, and is separated from it because of the popularity and importance of SAE. CircuitsVis A good tool for visualizing LLMs. Plotly A good tool for plotting. Forums and Communities Transformer Circuits Thread The research posts of Anthropic alignment group. Lesswrong AI Alignment Forum Companies, Institutes, Labs and Programs Anthropic DeepMind FAR Apollo RedWood CHAI (UC Berkeley) MIRI (UC Berkeley) Alignment Research Center (ARC) MATS The ML Alignment \u0026amp; Theory Scholars, an independent research and educational seminar program that connects talented scholars with top mentors in the fields of AI alignment, interpretability, and governance. SPAR Supervised Program for Alignment Research Blogs Chris Olah Neel Nanda * Neel Nanda at the Alignment Forum\nArthur Conmy Andy Zou Jacob Steinhardt David Bau Max Tegmark Trenton Bricken Callum Mcdougall Alex Turner(TurnTrout) \u0026hellip;\n","permalink":"https://Siriuslala.github.io/posts/mech_interp_resource/","summary":"\u003cp\u003e\u0026#x26a0;\u0026#xfe0f; \u003cfont color=\"red\"\u003e\u003cem\u003e\u003cstrong\u003eWarnings\u003c/strong\u003e\u003c/em\u003e\u003c/font\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cem\u003eThis post was written when I first delved into this area, and it hasn\u0026rsquo;t been updated for a long time. Thus there might be a lot of errors.\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eNow I\u0026rsquo;ve changed my attitude to this area. The area is not well-defined, and most of the research in this area is of low quality and is not appealing to me. Besides, I think the study of interpretability should be applied to pratical use, though we can also study it for fun.\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003cem\u003eI\u0026rsquo;m still interested in interpretability and its applications. I\u0026rsquo;ll write something new and interesting later ~\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u0026#x1f4a1;\nThis post is accompanied with \u003ca href=\"https://Siriuslala.github.io/posts/mech_interp_research/\"\u003eanother post\u003c/a\u003e, which contains specific content in this area.\u003c/p\u003e","title":"A Brief Introduction to Mechanistic Interpretability Research"},{"content":"Who am I? Hi~ I\u0026rsquo;m Yueyan Li, a researcher(still a student now) in China. I\u0026rsquo;m now at the Center of Intelligence Science and Technology, BUPT. My research focuses on machine learning, deep learning and natural language processing, mainly interpretability for neural networks and cognitive science now! Here is my CV.\nExcept from my research area, I\u0026rsquo;m also interested in communication engineering\u0026#x1f4fb; which was my major when I was an undergraduate. If you like that, feel free to share something interesting together~\nApart from technologies, I\u0026rsquo;m a lover for nature. I like the mountains, the rivers, the forests\u0026hellip;if you like hiking outdoors, don\u0026rsquo;t forget me! Also, I\u0026rsquo;m a Bboy\u0026#x270c;\u0026#xfe0f;\u0026#x1f918;. If you like breaking or any kind of street dance, just call me\u0026#x1f44b;!\nSometimes I paint as a waste of time, though I\u0026rsquo;m not professional.\nBelow are some of my interests. If you are interested in some of them, please reach me at any time~\nmachine learning, deep learning communication engineering street dance music (Buyi Mao, Eason, Huazhou, Shen Zhou / Avicii, Coldplay, / \u0026hellip;) Linguistics and Languages hiking, mountain climbing\u0026hellip; Street fitness (push-ups, muscle-ups\u0026hellip;) \u0026hellip; About my nickname I use the name Sirius/Sirius Jr./siriuslala\u0026hellip; everywhere on the Internet. This originates from Sirius Black - my favourite character in the Harry Potter series.\nHe is not only an extraodinary wizard with wild and intractable appearence but also the godfather of Harry Potter - the only family alive for Harry who had brought him warmth that is hard to replace. That\u0026rsquo;s why I admire him.\n","permalink":"https://Siriuslala.github.io/about/","summary":"About myself","title":"About Myself"},{"content":"\rPrevious\rNext \u0026nbsp; \u0026nbsp;\r/ [pdf]\rView the PDF file here.\r","permalink":"https://Siriuslala.github.io/pdfs/circuit_tuning/","summary":"\u003cscript crossorigin=\"anonymous\" src=\"/pdf-js/build/pdf.js\"\u003e\u003c/script\u003e\r\n\r\n\r\n\u003cstyle\u003e\r\n  #embed-pdf-container {\r\n    position: relative;\r\n    width: 100%;\r\n    height: auto;\r\n    min-height: 20vh;\r\n     \r\n  }\r\n  \r\n  .pdf-canvas {\r\n    border: 1px solid black;\r\n    direction: ltr;\r\n    width: 100%;\r\n    height: auto;\r\n    display: none;\r\n  }\r\n  \r\n  #the-canvas {\r\n    border: 1px solid black;\r\n    direction: ltr;\r\n    width: 100%;\r\n    height: auto;\r\n    display: none;\r\n  }\r\n  \r\n  \r\n  .pdf-loadingWrapper {\r\n    display: none;\r\n    justify-content: center;\r\n    align-items: center;\r\n    width: 100%;\r\n    height: 350px;\r\n  }\r\n  \r\n  .pdf-loading {\r\n    display: inline-block;\r\n    width: 50px;\r\n    height: 50px;\r\n    border: 3px solid #d2d0d0;;\r\n    border-radius: 50%;\r\n    border-top-color: #383838;\r\n    animation: spin 1s ease-in-out infinite;\r\n    -webkit-animation: spin 1s ease-in-out infinite;\r\n  }\r\n  \r\n  \r\n  \r\n  \r\n  \r\n  #overlayText {\r\n    word-wrap: break-word;\r\n    display: grid;\r\n    justify-content: end;\r\n  }\r\n  \r\n  #overlayText a {\r\n    position: relative;\r\n    top: 10px;\r\n    right: 4px;\r\n    color: #000;\r\n    margin: auto;\r\n    background-color: #eeeeee;\r\n    padding: 0.3em 1em;\r\n    border: solid 2px;\r\n    border-radius: 12px;\r\n    border-color: #00000030;\r\n    text-decoration: none;\r\n  }\r\n  \r\n  #overlayText svg {\r\n    height: clamp(1em, 2vw, 1.4em);\r\n    width:  clamp(1em, 2vw, 1.4em);\r\n  }\r\n  \r\n  \r\n  \r\n  @keyframes spin {\r\n    to { -webkit-transform: rotate(360deg); }\r\n  }\r\n  @-webkit-keyframes spin {\r\n    to { -webkit-transform: rotate(360deg); }\r\n  }\r\n  \u003c/style\u003e\u003cdiv class=\"embed-pdf-container\" id=\"embed-pdf-container-fe6807af\"\u003e\r\n    \u003cdiv class=\"pdf-loadingWrapper\" id=\"pdf-loadingWrapper-fe6807af\"\u003e\r\n        \u003cdiv class=\"pdf-loading\" id=\"pdf-loading-fe6807af\"\u003e\u003c/div\u003e\r\n    \u003c/div\u003e\r\n    \u003cdiv id=\"overlayText\"\u003e\r\n      \u003ca href=\"./Circuit_tuning.pdf\" aria-label=\"Download\" download\u003e\r\n        \u003csvg aria-hidden=\"true\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 18 18\"\u003e\r\n            \u003cpath d=\"M9 13c.3 0 .5-.1.7-.3L15.4 7 14 5.6l-4 4V1H8v8.6l-4-4L2.6 7l5.7 5.7c.2.2.4.3.7.3zm-7 2h14v2H2z\" /\u003e\r\n        \u003c/svg\u003e\r\n      \u003c/a\u003e\r\n    \u003c/div\u003e\r\n    \u003ccanvas class=\"pdf-canvas\" id=\"pdf-canvas-fe6807af\"\u003e\u003c/canvas\u003e\r\n\u003c/div\u003e\r\n\r\n\u003cdiv class=\"pdf-paginator\" id=\"pdf-paginator-fe6807af\"\u003e\r\n    \u003cbutton id=\"pdf-prev-fe6807af\"\u003ePrevious\u003c/button\u003e\r\n    \u003cbutton id=\"pdf-next-fe6807af\"\u003eNext\u003c/button\u003e \u0026nbsp; \u0026nbsp;\r\n    \u003cspan\u003e\r\n      \u003cspan class=\"pdf-pagenum\" id=\"pdf-pagenum-fe6807af\"\u003e\u003c/span\u003e / \u003cspan class=\"pdf-pagecount\" id=\"pdf-pagecount-fe6807af\"\u003e\u003c/span\u003e\r\n    \u003c/span\u003e\r\n    \u003ca class=\"pdf-source\" id=\"pdf-source-fe6807af\" href=\"./Circuit_tuning.pdf\"\u003e[pdf]\u003c/a\u003e\r\n\u003c/div\u003e\r\n\r\n\u003cnoscript\u003e\r\nView the PDF file \u003ca class=\"pdf-source\" id=\"pdf-source-noscript-fe6807af\" href=\"./Circuit_tuning.pdf\"\u003ehere\u003c/a\u003e.\r\n\u003c/noscript\u003e\r\n\r\n\u003cscript type=\"text/javascript\"\u003e\r\n    (function(){\r\n    var url = '.\\/Circuit_tuning.pdf';\r\n\r\n    var hidePaginator = \"\" === \"true\";\r\n    var hideLoader = \"\" === \"true\";\r\n    var selectedPageNum = parseInt(\"\") || 1;\r\n\r\n    \r\n    var pdfjsLib = window['pdfjs-dist/build/pdf'];\r\n\r\n    \r\n    if (pdfjsLib.GlobalWorkerOptions.workerSrc == '')\r\n      \r\n      pdfjsLib.GlobalWorkerOptions.workerSrc = \"\\/pdf-js\\/build\\/pdf.worker.js\";\r\n\r\n\r\n\r\n    \r\n    var pdfDoc = null,\r\n        pageNum = selectedPageNum,\r\n        pageRendering = false,\r\n        pageNumPending = null,\r\n        scale = 3,\r\n        canvas = document.getElementById('pdf-canvas-fe6807af'),\r\n        ctx = canvas.getContext('2d'),\r\n        paginator = document.getElementById(\"pdf-paginator-fe6807af\"),\r\n        loadingWrapper = document.getElementById('pdf-loadingWrapper-fe6807af');\r\n\r\n\r\n    \r\n    showPaginator();\r\n    showLoader();\r\n\r\n    \n\r\n    function renderPage(num) {\r\n      pageRendering = true;\r\n      \r\n      pdfDoc.getPage(num).then(function(page) {\r\n        var viewport = page.getViewport({scale: scale});\r\n        canvas.height = viewport.height;\r\n        canvas.width = viewport.width;\r\n\r\n        \r\n        var renderContext = {\r\n          canvasContext: ctx,\r\n          viewport: viewport\r\n        };\r\n        var renderTask = page.render(renderContext);\r\n\r\n        \r\n        renderTask.promise.then(function() {\r\n          pageRendering = false;\r\n          showContent();\r\n\r\n          if (pageNumPending !== null) {\r\n            \r\n            renderPage(pageNumPending);\r\n            pageNumPending = null;\r\n          }\r\n        });\r\n      });\r\n\r\n      \r\n      document.getElementById('pdf-pagenum-fe6807af').textContent = num;\r\n    }\r\n\r\n    \n\r\n    function showContent() {\r\n      loadingWrapper.style.display = 'none';\r\n      canvas.style.display = 'block';\r\n    }\r\n\r\n    \n\r\n    function showLoader() {\r\n      if(hideLoader) return\r\n      loadingWrapper.style.display = 'flex';\r\n      canvas.style.display = 'none';\r\n    }\r\n\r\n    \n\r\n    function showPaginator() {\r\n      if(hidePaginator) return\r\n      paginator.style.display = 'block';\r\n    }\r\n\r\n    \n\r\n    function queueRenderPage(num) {\r\n      if (pageRendering) {\r\n        pageNumPending = num;\r\n      } else {\r\n        renderPage(num);\r\n      }\r\n    }\r\n\r\n    \n\r\n    function onPrevPage() {\r\n      if (pageNum \u003c= 1) {\r\n        return;\r\n      }\r\n      pageNum--;\r\n      queueRenderPage(pageNum);\r\n    }\r\n    document.getElementById('pdf-prev-fe6807af').addEventListener('click', onPrevPage);\r\n\r\n    \n\r\n    function onNextPage() {\r\n      if (pageNum \u003e= pdfDoc.numPages) {\r\n        return;\r\n      }\r\n      pageNum++;\r\n      queueRenderPage(pageNum);\r\n    }\r\n    document.getElementById('pdf-next-fe6807af').addEventListener('click', onNextPage);\r\n\r\n    \n\r\n    pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) {\r\n      pdfDoc = pdfDoc_;\r\n      var numPages = pdfDoc.numPages;\r\n      document.getElementById('pdf-pagecount-fe6807af').textContent = numPages;\r\n\r\n      \r\n      if(pageNum \u003e numPages) {\r\n        pageNum = numPages\r\n      }\r\n\r\n      \r\n      renderPage(pageNum);\r\n    });\r\n    })();\r\n\u003c/script\u003e","title":"Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks"},{"content":"","permalink":"https://Siriuslala.github.io/faq/","summary":"faq","title":"faq"},{"content":"\rPrevious\rNext \u0026nbsp; \u0026nbsp;\r/ [pdf]\rView the PDF file here.\r","permalink":"https://Siriuslala.github.io/pdfs/helper/","summary":"\u003cscript crossorigin=\"anonymous\" src=\"/pdf-js/build/pdf.js\"\u003e\u003c/script\u003e\r\n\r\n\r\n\u003cstyle\u003e\r\n  #embed-pdf-container {\r\n    position: relative;\r\n    width: 100%;\r\n    height: auto;\r\n    min-height: 20vh;\r\n     \r\n  }\r\n  \r\n  .pdf-canvas {\r\n    border: 1px solid black;\r\n    direction: ltr;\r\n    width: 100%;\r\n    height: auto;\r\n    display: none;\r\n  }\r\n  \r\n  #the-canvas {\r\n    border: 1px solid black;\r\n    direction: ltr;\r\n    width: 100%;\r\n    height: auto;\r\n    display: none;\r\n  }\r\n  \r\n  \r\n  .pdf-loadingWrapper {\r\n    display: none;\r\n    justify-content: center;\r\n    align-items: center;\r\n    width: 100%;\r\n    height: 350px;\r\n  }\r\n  \r\n  .pdf-loading {\r\n    display: inline-block;\r\n    width: 50px;\r\n    height: 50px;\r\n    border: 3px solid #d2d0d0;;\r\n    border-radius: 50%;\r\n    border-top-color: #383838;\r\n    animation: spin 1s ease-in-out infinite;\r\n    -webkit-animation: spin 1s ease-in-out infinite;\r\n  }\r\n  \r\n  \r\n  \r\n  \r\n  \r\n  #overlayText {\r\n    word-wrap: break-word;\r\n    display: grid;\r\n    justify-content: end;\r\n  }\r\n  \r\n  #overlayText a {\r\n    position: relative;\r\n    top: 10px;\r\n    right: 4px;\r\n    color: #000;\r\n    margin: auto;\r\n    background-color: #eeeeee;\r\n    padding: 0.3em 1em;\r\n    border: solid 2px;\r\n    border-radius: 12px;\r\n    border-color: #00000030;\r\n    text-decoration: none;\r\n  }\r\n  \r\n  #overlayText svg {\r\n    height: clamp(1em, 2vw, 1.4em);\r\n    width:  clamp(1em, 2vw, 1.4em);\r\n  }\r\n  \r\n  \r\n  \r\n  @keyframes spin {\r\n    to { -webkit-transform: rotate(360deg); }\r\n  }\r\n  @-webkit-keyframes spin {\r\n    to { -webkit-transform: rotate(360deg); }\r\n  }\r\n  \u003c/style\u003e\u003cdiv class=\"embed-pdf-container\" id=\"embed-pdf-container-af35b2b1\"\u003e\r\n    \u003cdiv class=\"pdf-loadingWrapper\" id=\"pdf-loadingWrapper-af35b2b1\"\u003e\r\n        \u003cdiv class=\"pdf-loading\" id=\"pdf-loading-af35b2b1\"\u003e\u003c/div\u003e\r\n    \u003c/div\u003e\r\n    \u003cdiv id=\"overlayText\"\u003e\r\n      \u003ca href=\"./cv.pdf\" aria-label=\"Download\" download\u003e\r\n        \u003csvg aria-hidden=\"true\" xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 18 18\"\u003e\r\n            \u003cpath d=\"M9 13c.3 0 .5-.1.7-.3L15.4 7 14 5.6l-4 4V1H8v8.6l-4-4L2.6 7l5.7 5.7c.2.2.4.3.7.3zm-7 2h14v2H2z\" /\u003e\r\n        \u003c/svg\u003e\r\n      \u003c/a\u003e\r\n    \u003c/div\u003e\r\n    \u003ccanvas class=\"pdf-canvas\" id=\"pdf-canvas-af35b2b1\"\u003e\u003c/canvas\u003e\r\n\u003c/div\u003e\r\n\r\n\u003cdiv class=\"pdf-paginator\" id=\"pdf-paginator-af35b2b1\"\u003e\r\n    \u003cbutton id=\"pdf-prev-af35b2b1\"\u003ePrevious\u003c/button\u003e\r\n    \u003cbutton id=\"pdf-next-af35b2b1\"\u003eNext\u003c/button\u003e \u0026nbsp; \u0026nbsp;\r\n    \u003cspan\u003e\r\n      \u003cspan class=\"pdf-pagenum\" id=\"pdf-pagenum-af35b2b1\"\u003e\u003c/span\u003e / \u003cspan class=\"pdf-pagecount\" id=\"pdf-pagecount-af35b2b1\"\u003e\u003c/span\u003e\r\n    \u003c/span\u003e\r\n    \u003ca class=\"pdf-source\" id=\"pdf-source-af35b2b1\" href=\"./cv.pdf\"\u003e[pdf]\u003c/a\u003e\r\n\u003c/div\u003e\r\n\r\n\u003cnoscript\u003e\r\nView the PDF file \u003ca class=\"pdf-source\" id=\"pdf-source-noscript-af35b2b1\" href=\"./cv.pdf\"\u003ehere\u003c/a\u003e.\r\n\u003c/noscript\u003e\r\n\r\n\u003cscript type=\"text/javascript\"\u003e\r\n    (function(){\r\n    var url = '.\\/cv.pdf';\r\n\r\n    var hidePaginator = \"\" === \"true\";\r\n    var hideLoader = \"\" === \"true\";\r\n    var selectedPageNum = parseInt(\"\") || 1;\r\n\r\n    \r\n    var pdfjsLib = window['pdfjs-dist/build/pdf'];\r\n\r\n    \r\n    if (pdfjsLib.GlobalWorkerOptions.workerSrc == '')\r\n      \r\n      pdfjsLib.GlobalWorkerOptions.workerSrc = \"\\/pdf-js\\/build\\/pdf.worker.js\";\r\n\r\n\r\n\r\n    \r\n    var pdfDoc = null,\r\n        pageNum = selectedPageNum,\r\n        pageRendering = false,\r\n        pageNumPending = null,\r\n        scale = 3,\r\n        canvas = document.getElementById('pdf-canvas-af35b2b1'),\r\n        ctx = canvas.getContext('2d'),\r\n        paginator = document.getElementById(\"pdf-paginator-af35b2b1\"),\r\n        loadingWrapper = document.getElementById('pdf-loadingWrapper-af35b2b1');\r\n\r\n\r\n    \r\n    showPaginator();\r\n    showLoader();\r\n\r\n    \n\r\n    function renderPage(num) {\r\n      pageRendering = true;\r\n      \r\n      pdfDoc.getPage(num).then(function(page) {\r\n        var viewport = page.getViewport({scale: scale});\r\n        canvas.height = viewport.height;\r\n        canvas.width = viewport.width;\r\n\r\n        \r\n        var renderContext = {\r\n          canvasContext: ctx,\r\n          viewport: viewport\r\n        };\r\n        var renderTask = page.render(renderContext);\r\n\r\n        \r\n        renderTask.promise.then(function() {\r\n          pageRendering = false;\r\n          showContent();\r\n\r\n          if (pageNumPending !== null) {\r\n            \r\n            renderPage(pageNumPending);\r\n            pageNumPending = null;\r\n          }\r\n        });\r\n      });\r\n\r\n      \r\n      document.getElementById('pdf-pagenum-af35b2b1').textContent = num;\r\n    }\r\n\r\n    \n\r\n    function showContent() {\r\n      loadingWrapper.style.display = 'none';\r\n      canvas.style.display = 'block';\r\n    }\r\n\r\n    \n\r\n    function showLoader() {\r\n      if(hideLoader) return\r\n      loadingWrapper.style.display = 'flex';\r\n      canvas.style.display = 'none';\r\n    }\r\n\r\n    \n\r\n    function showPaginator() {\r\n      if(hidePaginator) return\r\n      paginator.style.display = 'block';\r\n    }\r\n\r\n    \n\r\n    function queueRenderPage(num) {\r\n      if (pageRendering) {\r\n        pageNumPending = num;\r\n      } else {\r\n        renderPage(num);\r\n      }\r\n    }\r\n\r\n    \n\r\n    function onPrevPage() {\r\n      if (pageNum \u003c= 1) {\r\n        return;\r\n      }\r\n      pageNum--;\r\n      queueRenderPage(pageNum);\r\n    }\r\n    document.getElementById('pdf-prev-af35b2b1').addEventListener('click', onPrevPage);\r\n\r\n    \n\r\n    function onNextPage() {\r\n      if (pageNum \u003e= pdfDoc.numPages) {\r\n        return;\r\n      }\r\n      pageNum++;\r\n      queueRenderPage(pageNum);\r\n    }\r\n    document.getElementById('pdf-next-af35b2b1').addEventListener('click', onNextPage);\r\n\r\n    \n\r\n    pdfjsLib.getDocument(url).promise.then(function(pdfDoc_) {\r\n      pdfDoc = pdfDoc_;\r\n      var numPages = pdfDoc.numPages;\r\n      document.getElementById('pdf-pagecount-af35b2b1').textContent = numPages;\r\n\r\n      \r\n      if(pageNum \u003e numPages) {\r\n        pageNum = numPages\r\n      }\r\n\r\n      \r\n      renderPage(pageNum);\r\n    });\r\n    })();\r\n\u003c/script\u003e","title":"Yueyan Li"}]