[{"content":"Active Perception Sources Papers mmlm basic\n(VIT) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (CLIP) Learning Transferable Visual Models From Natural Language Supervision (DIT) Scalable Diffusion Models with Transformers (BLIP-2) BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models Qwen2-VL: Enhancing Vision-Language Model\u0026rsquo;s Perception of the World at Any Resolution resolution\nSwin Transformer: Hierarchical Vision Transformer using Shifted Windows DualFocus: A Unified Framework for Integrating Positive and Negative Descriptors in Text-based Person Retrieval generative models image\nbasic (T2I)\n(DDPM) Denoising Diffusion Probabilistic Models (DDIM) Denoising Diffusion Implicit Models (Classifier-guided) Diffusion Models Beat GANs on Image Synthesis (Classifier-free) Classifier-free diffusion guidance (VQVAE) Taming transformers for high-resolution image synthesis (LDM) High-resolution image synthesis with latent diffusion models (GLIDE) (Imagen) (DALL-E) Zero-Shot Text-to-Image Generation (DALL-E-2) Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E-3) Improving Image Generation with Better Captions (ControlNet) Adding Conditional Control to Text-to-Image Diffusion Models Consistency Models (LCM) Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference (SDXL Turbo) Adversarial Diffusion Distillation (EDM) Elucidating the Design Space of Diffusion-Based Generative Models Flow Matching for Generative Modeling (Stable Diffusion 3) Scaling Rectified Flow Transformers for High-Resolution Image Synthesis image editing\nInstructPix2Pix: Learning to Follow Image Editing Instructions Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry Diffusion Model-Based Image Editing: A Survey video\nsurvey\nThe Dawn of Video Generation: Preliminary Explorations with SORA-like Models generation\nVideo Diffusion Models Latte: Latent Diffusion Transformer for Video Generation Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning I2V\nvideo editing\nAnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks TokenFlow: Consistent Diffusion Features for Consistent Video Editing STABLEV2V: Stablizing Shape Consistency in Video-to-Video Editing VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing\\ interp survey\nA Survey on Mechanistic Interpretability for Multi-Modal Foundation Models Analysis\nConceptAttention: Diffusion Transformers Learn Highly Interpretable Features Model capabilities\nspatial reasoning Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas ","permalink":"http://localhost:1313/posts/mm_interp/","summary":"Active Perception Sources Papers mmlm basic\n(VIT) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (CLIP) Learning Transferable Visual Models From Natural Language Supervision (DIT) Scalable Diffusion Models with Transformers (BLIP-2) BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models Qwen2-VL: Enhancing Vision-Language Model\u0026rsquo;s Perception of the World at Any Resolution resolution\nSwin Transformer: Hierarchical Vision Transformer using Shifted Windows DualFocus: A Unified Framework for Integrating Positive and Negative Descriptors in Text-based Person Retrieval generative models image","title":"MM_Interp"},{"content":"The paper is here.\nArXiv: https://arxiv.org/pdf/2502.06106\n","permalink":"http://localhost:1313/posts/circuit_tuning/","summary":"The paper is here.\nArXiv: https://arxiv.org/pdf/2502.06106","title":"Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks"},{"content":"国外 Anthropic (MATS) MATS 项目里 Anthropic 的 mentors 发起的 coding screen，不是Leetcode算法题，而是一个小的工程项目。项目一般是要实现一个系统（一个类）的功能（类的属性及方法），只涉及逻辑上的，不会涉及具体的前后端，也就是说只要有 Python 编程基础即可（MATS的 coding screen 只允许 Python），无需掌握特定领域的知识（不涉及第三方库的使用）。\n我参加过两次（因为第一次信息上传有误，所以重新申请了一次），第一次是写一个员工管理系统，第二次是写一个银行账户管理系统，大同小异。这里我以银行账户管理系统为例进行介绍。 限时 1.5h 的 coding screen 分为4个 level，每一个 level 需要用到前面 level 的内容，也就是一步一步实现整个系统，难度依次递增。\nLevel I\n内容 实现账户注册 def register 实现存钱 def deposit 实现转钱 def transfer 分析 首先要设置一个实例属性self.account: dict，键为账户id，值为账户信息，例如： self.account[account_id] = {\u0026quot;balance\u0026quot;: 0, \u0026quot;income\u0026quot;: 0, \u0026quot;withdraw\u0026quot;: 0, \u0026quot;pay\u0026quot;: []} 账户注册就是往self.account里加元素 存钱就是balance += amount 转钱要注意留存一下支出记录，也就是转钱账户的withdraw += amount Level II\n内容 实现支出最多的账户的获取 def top_spenders：返回列表，每个元素是字符串\u0026quot;account_id(withdraw)\u0026quot;，按支出从大到小排序，如果支出相同，按字母表顺序排序 分析\n首先获取一个列表\naccounts = [{\u0026quot;id\u0026quot;: account, **info} for account, info in self.account.items()]\n然后对accounts进行排序：\naccounts.sort(key=lambda x: (-x[\u0026quot;balance\u0026quot;], x[\u0026quot;id\u0026quot;])) Level III\n内容 实现支付 def pay：支付后的24小时后，银行会按支出的2%返钱给账户 实现支付状态查询 def get_payment_status 分析 这里有一个动态的逻辑，就是返钱的操作不是立即生效的，所以要在任何操作之前查询一下账户状态。例如在每次支付前，要看看余额是否大于要支出的金额，而查询余额时又要考虑上一次支出后银行的返现是否已经到账。根据上述分析，要先实现一个状态查询的功能。 首先我们规定，每一次支出都要有记录，例如：\nself.account[account_id][\u0026quot;pay\u0026quot;] = {\u0026quot;payment_id\u0026quot;: \u0026quot;payment_1\u0026quot;, \u0026quot;amount\u0026quot;: 200, \u0026quot;start\u0026quot;: 4, \u0026quot;cashback\u0026quot;: False}\n这里start指的是支出操作发生时的timestamp，单位为毫秒，cashback是银行返现的状态。每次查询状态时，要看当前的时间戳与支付时的时间戳之差是否大于24小时，然后决定是否往账户里打钱。 支付不再赘述 Level IV\n内容 实现账户合并 实现余额查询 分析 都很简单，但是手慢，第二个函数没写完 得分技巧与注意事项：\n具体功能可以先不实现，先把特殊情况处理掉，如if account_id not in self.account: return None这种情况，能够cover很多测例了； 测例通过数跟最终得分没有必然联系。例如我前三个level满分，第四个level得分800/1000，但最终得分为520（分数范围为 200~600）。这与 CodeSignal 的评价体系有关，具体见官网说明。 国内 智谱AI（日常实习） 没有代表性。\n","permalink":"http://localhost:1313/posts/%E9%9D%A2%E7%BB%8F/","summary":"国外 Anthropic (MATS) MATS 项目里 Anthropic 的 mentors 发起的 coding screen，不是Leetcode算法题，而是一个小的工程项目。项目一般是要实现一个系统（一个类）的功能（类的","title":"面经"},{"content":"This post is written in Chinese. If you don\u0026rsquo;t know Chinese, you can learn it lol. (Sorry for this because simply translating the post into English may not be enough for you to understand).\n纯玩梗 语言现象背后蕴含的知识 皮钦语 (pidgin) 大家对那些 1.言语中不时夹杂着英文单词 2.装/凡尔赛 的人表现出一种厌恶。例如，下面是某恋综里的一段名场面：\n————————————————————————————————\n\u0026hellip;\n男A：\u0026ldquo;可能因为我学校在伦敦，所以\u0026hellip;\u0026rdquo; 女A：\u0026ldquo;哦我也是。我是高中在 York (Yorkshire) 附近，然后本科研究生在伦敦\u0026rdquo;\n男B：\u0026ldquo;我喜欢 nə —— uhh —— Southampton\u0026rdquo;\n女A：\u0026ldquo;在海边！\u0026rdquo;\n男B：\u0026ldquo;对超美！\u0026rdquo;\n男A：\u0026ldquo;然后，我也挺喜欢曼 —— emmm —— Manchester 的\u0026rdquo;\n\u0026hellip;\n————————————————————————————————\n好的嘲讽完毕，点到为止。\n实际上语言学里的语言演化领域有一个名词叫皮钦语（pidgin），也被称作混杂语，是语言发展的一个阶段，指在没有共同语言而又急于进行交流的人群中间产生的一种混合语言，属于不同语言人群的联系语言。感兴趣详见：皮克特语到四阶皮钦语——苏格兰语言迭代史。\n笔者印象里比较深的，一个是洋泾浜语，即流行在20世纪初的上海滩的一种英汉混杂语；另一个是协和语，指流行于伪满洲国的汉语、日语的混合语言。两者都是殖民地和半殖民地文化的产物。以下是一些例子：\n洋泾浜语：\u0026ldquo;来是\u0026rsquo;康姆\u0026rsquo;（come），去是\u0026rsquo;狗\u0026rsquo;（go）\u0026quot;（详见汪仲贤所著《上海俗话图说》；dbq耳边想起赵丽蓉老师的小品） 协和语：\u0026ldquo;你的帮我，我的钱的大大的给。\u0026rdquo; 此外，刘慈欣《三体》中也提到了由汉语和英语结合而成的新语言（舰队混合语）。\n其实，正常的、真诚的表达往往不会惹人厌烦，例如：\n\u0026ldquo;我希望能够做点兼职，至少能够 cover 掉我一部分的学费\u0026rdquo; \u0026ldquo;这个 checkpoint 我测了一下，跟 baseline 相比有很大的优势，但是跟 SOTA 比还是有很大的差距\u0026rdquo; \u0026ldquo;哈喽哈喽，你好呀~\u0026rdquo; 语言磨蚀 ","permalink":"http://localhost:1313/posts/%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E5%AD%A6%E7%9A%84%E6%A2%97%E5%92%8C%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%9F%A5%E8%AF%86/","summary":"This post is written in Chinese. If you don\u0026rsquo;t know Chinese, you can learn it lol. (Sorry for this because simply translating the post into English may not be enough for you to understand). 纯玩梗 语言现象背后蕴含的知识 皮钦语 (pidgin) 大家对那些 1.言语中不时夹杂着英文单","title":"一些语言学的梗和有意思的知识"},{"content":"mechanistic interpretability Computational Linguistics Circuit-tuning: A Mechanistic Approach for Understanding Instrinsic Dimension and Fine-tuning Neural Networks 袁老师建议\n理论 重新审视可解释性，如何给出一个漂亮的解释（思考mech interp局限性，是否可以突破一下） 上手实验，实践指导理论 实验 实验数据 因为目标不是提性能，所以可以设置一些独特的评价体系和指标，突出可解释性 因果推断的数据（hzy, xsy） 为什么做这个？\n现有问题/motivation 全量微调耗费计算资源，对于某些特定领域的任务，只有一部分参数需要微调； 缺乏对微调机理的探究，不知道在微调的过程中发生了什么，不知道微调哪些参数，导致出现很多问题，如灾难性遗忘等； intrinsic dimension的概念在LoRA中得到印证，而mech interp领域的ciruit discovery也有类似的想法，在做法上与LoRA也有共通之处 lora只是借用了 intrinsic dimension 的思想，而 intrinsic dimension 究竟是什么缺乏深入的研究； 方法 机理可解释性 贡献 从mech interp的角度入手，提供一种更深入的理解intrinsic dimension的新方法； 提出表征冗余和前向冗余的概念，并分析了因果干预方法（IE 指标）的合理性； 提出广义 intrinsic dimension 的定义； 分析circuit discovery与AdaLoRA在特征选取上殊途同归； 结合神经科学，提出circuit-tuning，先剪枝(pruning)后微调，作为一种PEFT的新方法，并通过实验证明有效性； 缓解灾难性遗忘； 提供了理解微调过程（Hebbian Learning）(以及训练动力学（训练动态？）?) 的新视角，理解人脑的学习过程 ； 整体逻辑 intrinsic dimension 首次提出的原因是作者认为模型参数存在冗余。在 mech interp 的理论框架下，我认为冗余指的是计算图中的节点冗余。那么： 节点冗余的定义是什么？怎么判断一个节点是否冗余？ 冗余的分类有哪些，各自的来源是什么？ 冗余的检测方法是什么？ 论证思路 前提假设（特征表征空间，特征向量；特定任务下特征冗余） 提出两个概念：表征冗余和前向冗余。阐述各自冗余的来源和冗余的判定方法 定义: 节点冗余 = 表征冗余 + 前向冗余 定义：intrinsic dimension 为其映射得到的表征空间中不冗余节点的个数 论述 circuit discovery 中用到的因果干预方法满足节点冗余的判定条件 由于LoRA是应用intrinsic dimension这一概念进行实践的典型案例，所以从mech interp的角度解释LoRA，并将LoRA与circuit discovery作类比，论证共通之处，最终证明最初猜想的合理性 在理论猜想的指导下，实现circuit-tuning，从而反向论证猜想的合理性 前置（背景）\nmech interp neuron -\u0026gt; feature circuit circuit discovery intrinsic dimension intrinsic dimension The Lottery Ticket Hypothesis \u0026hellip; neuron science sparse coding v.s. population coding（为 superposition 提供依据） population \u0026hellip; 2000 Information processing with population codes 2015 Neural population coding: combining insights from microscopic and mass signals sparse \u0026hellip; BAAI 神经系统的 5 种编码方式与解码 人脑中不同区域分管不同功能；建模成图结构 hebbian learning 及反例（为后续分析边的增强以及为何用 edge patching 做准备） non-Hebbian volume learning mechanism volume learning 的科普（定义很好）：Neural eavesdropping -\u0026gt; 用来解释使用 edge patching 的缘由 volume learning 的定义和模型 (2005) Study of Nitric Oxide Effect in the Hebbian Learning: Towards a Diffusive Hebb’s Law volume learning 的定义和模型 (2009) Can Hebbian Volume Learning Explain Discontinuities in Cortical Maps? (DOI: 10.1162/089976699300016115) volume learning 形式化和机理探究 (2009) A COMPUTATIONAL STUDY OF THE DIFFUSE NEIGHBOURHOODS IN BIOLOGICAL AND ARTIFICIAL NEURAL NETWORKS \u0026ldquo;An intrinsic feature of the NO diffusion is the formation of not-wired neighbourhoods, diffuse neighbourhoods (DNB), which supports the emerging of complex structures.\u0026rdquo; volume learning 的定义和模型 (2015) Nitric Oxide Diffusion and Multi-compartmental Systems: Modeling and Implications ??? volume learning 的定义和模型 Volume Learning: Signaling Covariance Through Neural Tissue 用进废退（为后续分析引入额外参数拟合任务会造成其他能力的遗忘） \u0026hellip; Finetuning full PEFT Lora, AdaLora mask操作的定义 理论 Provide an understanding of intrinsic dimension using mech interp\n猜想：The computational graph of a model is $G$. Given a weight matrix $W$ and a circuit $C \\subseteq G$ corresponding to a specific task $T$ with a granularity at the neuron level, the intrinsic dimenison of $W$ is equivalent to the number of nodes in $C$ which exist in the vector space projected by $W$.\n假设\n把模型看出计算图\nLinear feature hypothesis: \u0026ldquo;Let\u0026rsquo;s call a neural network representation linear if features correspond to directions in activation space.\u0026rdquo; \u0026ndash; toy model\n表征的定义：representation -\u0026gt; vector space SLU-3.2 两个原则：Composition as Addition \u0026amp; Intensity as Scaling \u0026ndash; July Updates Superposition\nthe phenomenon of polysemanticity 在特征空间正交基集合 $\\mathbf{E}$ 的定义 假设表征空间维度为m，则 $\\mathbf{E} = ({\\vec{e_{1}}, \\vec{e_{2}}, \u0026hellip;, \\vec{e_{m}}})$ 为一组相互正交的基底 因为存在 superposition，所以特征往往不会和神经元对应的basis对齐，由此引出特征向量 SLU-3.4 特征向量的定义 特征 $f$ 是向量空间中的一个方向，其特征向量 $\\vec{v_{f}}$ 是正交基底的线性组合，即 $$ \\vec{v_{f}} = \\sum_{i=1}^{m}c_{i}\\vec{e_{i}} = c_{1}\\vec{e_{1}} + c_{2}\\vec{e_{2}} +\u0026hellip;+ c_{m}\\vec{e_{m}} $$ ($c_{i}$为系数，$|ci|≤1$ 且 $\\sum_{i=1}^{k}ci^{2}=1$)。 All features represented in a model is $\\mathcal{F}$. Given a task $T$ with $\\mathcal{D_{T}} = ({x_{1}, x_{2}, \u0026hellip;, x_{t}})$ which consists of $t$ samples that follows a specific data distribution, the features of $T$ is $\\mathcal{F_{T}}\\subsetneqq \\mathcal{F}$.\nintrinsic dimension exists in a model when it comes to a specific task.\n定理/猜想/命题/定义\n讨论\n最终的目的是提出一种新的 intrinsic dimension 的定义和理解方法，对于某个参数 W，希望将 intrinsic dimension 理解为通过 W 映射得到的表征空间中不冗余节点的个数。 可从 AdaLoRA的重要性分数（IE指标）的形式出发：$|w\\nabla_{w}\\mathcal{L}|$。两部分：参数原有的大小和损失函数关于参数的梯度。由此提出两个概念：表征冗余和前向冗余。 表征冗余指的是表征空间的维度存在冗余，某一维度的值很小，则该维度冗余； 前向冗余指的是表征空间某一维度上的值在参与后续计算的过程中影响力被削弱，对最终结果没什么影响，所以也是冗余； 提出节点冗余的判据：需要表征冗余和前向冗余同时存在。 讨论 IE 作为节点贡献判断的合理性 正式定义 intrinsic dimension 论述 AdaLoRA 中重要性分数与 circuit discovery 中 IE 指标的等价性。 ReLU的特征选择机理（后续在实验中证明）\n在模型中设置 ReLu，会鼓励激活值变大？ 前置\n讨论 intrinsic dimension，针对的是神经网络中的某一个参数矩阵中参数的冗余性。我们可以通过考察经过该矩阵映射后得到的表征$H$来研究其冗余性。我们若将表征 $H=({h_{1}, h_{2}, \u0026hellip;, h_{m}}) \\in \\mathbb{R}^{D}(D=m)$视为研究主体，则可将考察的参数矩阵记为$W_{pre}\\in \\mathbb{R}^{m \\times n}$. I（定义 特定任务下表征存在冗余：representation redundancy）\nGiven a task $T$ and an input $x\\in T$. 假设$x$所含的特征 $\\mathcal{F_{x}} = (f_{1},f_{2}, \u0026hellip;, f_{t}) \\subseteq F_{T}$ 对应的特征方向为 $ \\mathbf{V_{x}} = ({ \\vec{v_{f_{1}}}, \\vec{v_{f_{2}}}, \u0026hellip;, \\vec{v_{f_{t}}} }) $, 且特征的激活值为 $ A_{x} = ({ a_{f_{1}}, a_{f_{2}}, \u0026hellip;, a_{f_{t}} }) $, 则表征空间内 $\\vec{e_{i}}$ 方向上的强度 $a_{i}$ 可以表示为： $$ a_{i} = (\\sum_{i=1}^{t} a_{f_{i}}\\vec{v_{f_{i}}} ) \\cdot \\vec{e_{i}} $$\n表征冗余的前提假设\n特征的稀疏性(sparsity): 一条数据往往只包含个别的几个特征 Toy Models of Superposition 特定任务下用到的特征数量有限 (Definition) 特征的语义不变性（semantics preservation of a feature）：Consider a feature $f$ with its feature direction $\\vec{v_{f}} = \\sum_{i=1}^{m}c_{i}\\vec{e_{i}}$. If we set the coefficients in $\\vec{v_{f}}$ which correspond to a set of bases $\\mathbf{E_{r}}$ to zero, then we can get a new vector $\\tilde{\\vec{v_{f}}}$. If the cosine similarity $ cos(\\vec{v_{f}}, \\tilde{\\vec{v_{f}}}) = \\frac{\\vec{v_{f}} \\cdot \\tilde{\\vec{v_{f}}}} {||\\vec{v_{f}}|| \\cdot ||\\tilde{\\vec{v_{f}}}||} \\gt \\delta$ $(\\delta \\in (0, 1])$, then we say $\\tilde{\\vec{v_{f}}}$ has preserved the semantic information in $f$.\n(Definition) 维度冗余（dimension redundancy）：Given a task $T$ with a number of features $\\mathcal{F_{T}}$. Let\u0026rsquo;s consider a single basis $\\vec{e_{i}}$. For any feature $f$ in $\\mathcal{F_{T}}$, we set the coefficient $c_{i}$ in $\\vec{v_{f}}$ to zero and get the deformed feature direction $\\tilde{\\vec{v_{f}}}$. If $\\vec{e_{i}}$ satisfies: $$ min[cos( \\vec{v_{f_{1}}}, \\tilde{\\vec{v_{f_{1}}}} ), cos( \\vec{v_{f_{2}}}, \\tilde{\\vec{v_{f_{2}}}} ), \u0026hellip;, cos( \\vec{v_{f_{|\\mathcal{F_{T}}|}}}, \\tilde{\\vec{v_{f_{|\\mathcal{F_{T}}|}}}} )] \\gt \\delta $$ then $\\vec{e_{i}}$ is called a redundant dimension for representing features in task $T$. Noet that $\\delta \\in (0, 1]$ is the lowerbound for judging whether $\\tilde{\\vec{v_{f}}}$ preserves the semantic information in $f$.\n(Corollary) ：Consider the setup of Definition \u0026hellip;. If $\\vec{e_{i}}$ satisfies: $$ max(|\\vec{v_{f_{1}}} \\vec{e_{i}}|,|\\vec{v_{f_{2}}} \\vec{e_{i}}|, \u0026hellip;, |\\vec{v_{f_{|\\mathcal{F_{T}}|}}} \\vec{e_{i}}|) \\lt \\sqrt{1-\\delta^{2}} $$ then we say $\\vec{e_{i}}$ is redundant for representing features in task $T$. This is an obvious result from the rigorous definition for dimension redundancy of a single basis in a specific task. Noet that $|\\vec{v_{f_{j}}} \\vec{e_{i}}|$ is the projection of $\\vec{v_{f_{j}}}$ on direction $\\vec{e_{i}}$. Higher $|\\vec{v_{f_{j}}} \\vec{e_{i}}|$ means higher dependency of $\\vec{v_{f_{j}}}$ on $\\vec{e_{i}}$. If some tolerance $\\epsilon \\gt 0$ is allowed, then we can redefine the dimension redundancy in a specific task as: $$ \\sum_{j=1}^{|\\mathcal{F_{T}}|} max(|\\vec{v_{f_{j}}} \\vec{e_{i}}| - \\sqrt{1-\\delta^{2}}, 0) \\lt \\epsilon $$ The weak definition of dimension redundancy requires the total deformation in feature directions that exceeds the lowerbound $\\delta$ for semantics preservation should be limited within a threshold $\\epsilon$.\n维度冗余的概率: From Theorem \u0026hellip;, the redundancy probability $P_{r}$ of a single basis $\\vec{e_{i}}$ can be written as: $$ P_{r} = \\prod_{j=1}^{|\\mathcal{F_{T}}|} P(|\\vec{v_{f_{j}}} \\vec{e_{i}}| \\lt \\sqrt{1-\\delta^{2}}) $$ The equation tells us that the probability a basis $\\vec{e_{i}}$ is redundant will decrease with the increasing number of features. On the contrary, the fewer features related to a specific task, the higher the likelihood that a certain basis is redundant, leading to more redundant dimensions in the representation. (Definition) 表征冗余（representation redundancy）：Given a specific task $T$ and its related features $\\mathcal{F_{T}}$, we choose a set of bases $\\mathbf{E_{r}}$ from $\\mathbf{E}$. For every feature $f \\in \\mathcal{F_{T}}$, we set the coefficients in its feature direction $\\vec{v_{f}}$ which correspond to the bases in $\\mathbf{E_{r}}$ to zero and get the deformed feature direction $\\tilde{\\vec{v_{f}}}$. Given a threshold $\\epsilon$ which is the tolerence of deformation which exceeds the upperbound $\\delta$ for semantics preservation. If $\\mathbf{E_{r}}$ satisfies: $$ \\sum_{j=1}^{|\\mathcal{F_{T}}|} max[(\\delta - cos( \\vec{v_{f}}, \\tilde{\\vec{v_{f}}} ) ), 0] \\lt \\epsilon $$ then we say $\\mathbf{E_{r}}$ is a set of redundant bases, and the representation redundancy in $\\mathbb{R}^{m}$ is defined under the choice of $\\mathbf{E_{r}}$. If $|\\mathbf{E_{r}}| = r$, then the value of representation redundancy is equal to $r$.\nThe definition of representation redundancy is actually an extension of Definition \u0026hellip; from one-dimensional redundancy to multidimensional redundancy. The difference $\\delta - cos( \\vec{v_{f}}, \\tilde{\\vec{v_{f}}} )$ is used to measure the degree of deformation of feature directions. If the total deformation in feature directions is accepted under the threshold $\\epsilon$, then the influence from the removal of $\\mathbf{E_{r}}$ on the semantic representation of $\\mathcal{F_{T}}$ can be ignored, and the number of useful dimensions for representing $\\mathcal{F_{T}}$ is $m-r$. Johnson–Lindenstrauss lemma（用不用的）\n特别的，when the dimension $D$ of latent space $\\mathbb{R}^{D}$ satisfies $D \\geq |\\mathcal{F_{t}}| $, we can suppose that there is no polysemanticity in $H$. In this situation, the number of features needed is equal to the number of non-zero singular values which is actually the rank of $W_{pre}$.\nintrinsic dimension 最初的定义是解空间的余维度？？？(只是知乎的说法，还要根据原文判断) 是否也规定了输入服从某一种数据分布，也就是对于特定任务而言？这样就可以利用 intrinsic dimension 最初的定义来顺理成章地引入“特定任务T”的前提假设。\n参考\nLORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS ADALORA: ADAPTIVE BUDGET ALLOCATION FOR PARAMETER-EFFICIENT FINE-TUNING PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning Sparse Low-rank Adaptation of Pre-trained Language Models II（定义：前向冗余 forward redundancy）\nIf the representation $H={h_{1}, h_{2}, \u0026hellip;, h_{m}}$ is followed by another weight matrix $W^{post} \\in \\mathbb{R}^{l\\times m}$, then the following representation is $ H^{\\prime} = W^{post}H \\in \\mathbb{R}^{D^{\\prime}}(D^{\\prime}=l) $. To investigate into the influence of $H$ on $H^{\\prime}$, the representation $H^{\\prime}$ can also be splited into the sum of influnences of the dimensions in $H$: $$ W^{post}H = \\sum_{i=1}^{m}h_{i}W_{:,i}^{post} $$ 我们发现：$h_{i}$ 对 $H^{\\prime}$ 的影响会受到 $W_{:,i}^{post}$ 的制约。如果 $W_{:,i}^{post}$ 参数值很小，则会削弱 $h_{i}$ 的影响，导致其在后续计算中对最终结果的影响降低。 （定义：前向冗余）给定一个表征 $H$ 和后续的参数 $W^{post}$，我们考察 $H$ 中的某个维度 $i$。给定一个影响力的下界 $\\tau_{f} \\gt 0 $。如果： $$ |W_{:,i}^{post} h_{i}| \\lt \\tau_{f} $$ 我们称表征空间 $\\mathbb{R}^{m}$ 中的维度 $i$ 是前向冗余的。即表征空间 $\\mathbb{R}^{m}$ 存在前向冗余。 III（定义：节点冗余 node redundancy）\nGiven a model $\\mathcal{M}$ and its computational graph $\\mathcal{G}=(\\mathcal{V}, \\mathcal{E})$ in which $\\mathcal{V}$ and $\\mathcal{E}$ represent nodes and edges in $\\mathcal{G}$ respectively, consider a node $n \\in \\mathcal{V}$. A batch of inputs $X = (x_{i})_{i=1}^{p} \\subset \\mathcal{D_T}$ is fed into the model and the activation of $n$ as well as the final output $\\mathcal{M}(X)$ are saved. For each input $x_{i}$, if we replace the value of $n$ with another value $n^{\\prime}$ while keeping other activations in the forward propagation of input $x_{i}$ unchanged, we can get another output $\\mathcal{M}(x_{i})^{\\prime}$. Given a metric $f$ for measuring the difference caused by $n$ between two output distribution and a threshold $\\tau$, we can calculate the contribution $c_{i}(n)$ of node $n$ as: $$ c_{i}(n) = f(n; \\mathcal{M}(x_{i}), \\mathcal{M}(x_{i})^{\\prime})$$ If $$ \\mathbb{E}{x{i}~X}[c_{i}(n)] \u0026lt; \\tau$$ then node $n$ is called a redundant node in $\\mathcal{G}$. 定义：patching （节点冗余的检测） 讨论：因果干预指标 IE 能够合理判断节点冗余（从节点冗余的定义出发） indirect effect: 因果推断里的定义 With the concept of indirect effect from causal intervention, we can estimate the contribution of a node as: $$ IE(n; x_{clean}, x_{noise}) = \\mathcal{L_{m}}(\\mathcal{M}(x_{clean}-do(n\\leftarrow n(x_{noise})))) - \\mathcal{L_{m}}(\\mathcal{M}(x_{clean})) $$ To \u0026hellip;, [\u0026hellip;] et al purposed attribution which applies a first-order Taylor expansion to $IE$ at $n = n(x_{clean})$. The $IE$ in attribution patching can be simplified into: $$ IE(n; x_{clean}, x_{noise}) \\approx \\mathcal{L_{m}}(\\mathcal{M}(x_{clean})) + (n_{noise}-n_{clean}) \\nabla_{n} \\mathcal{L_{m}}(\\mathcal{M}(x_{clean})) - \\mathcal{L_{m}}(\\mathcal{M}(x_{clean})) = (n_{noise}-n_{clean}) \\nabla_{n} \\mathcal{L_{m}}(\\mathcal{M}(x_{clean})) $$ If we use zero ablation, then IE of node $n$ can be written as: $$ IE(n; x_{clean}) \\approx n \\nabla_{n} \\mathcal{L_{m}}(\\mathcal{M}(x_{clean})) $$ This form of IE consists of two parts: $n$ the value of node $n$ on the clean input and $ \\nabla_{n} \\mathcal{L_{m}}(\\mathcal{M}(x_{clean})) $ the gradient of the patching metric function $\\mathcal{L_{m}}$ on node $n$ in terms of the clean input. The two parts correspond to the representation redundancy and forward redundancy respectively. Let\u0026rsquo;s consider a representation in $\\mathbb{R}^{m}$ which consists of a set of nodes $N=(n_{1}, n_{2},\u0026hellip;, n_{m}) \\subset \\mathcal{G}$ and focus on the node $n_{i}$: The first part directly serves as a signal for representation redundancy since the maginitude of a neuron is a good indicator for dimension redundancy when we inspect into a representation. The smaller the value of a neuron is, the higher the likelihood that the neuron corresponds to a redundant dimension. This is because the activation of a feature $f$ on a basis $\\vec{e_{i}}$ is directly proportional to the projection of the feature direction $\\vec{v_{f}}$ on $\\vec{e_{i}}$.\nOne thing to note is the case where two features $f_{1}$ and $f_{2}$ are almost in opposite directions and the activations on the shared direction may cancel out each other, which results in a situation that the value of a neuron seems to be small though both of the two features fire on it. While this case may be tricky, Elhage et al show that models prefer to represent anticorrelated features in opposite directions, which means $f_{1}$ and $f_{2}$ may never co-occurr at the same time, so the probability of the above case occurring is very small. As for the second part, it can be written as: $$ \\nabla_{n_{i}} \\mathcal{L_{m}} = \\sum_{j=1}^{l} \\nabla_{n_{j}} \\mathcal{L_{m}} \\cdot \\nabla_{n_{i}} n_{j}(chain rule) $$ in which the node $n_{j}$ in the following vector space $\\mathbb{R}^{l}$ can be written as: $$ n_{j} = \\sum_{i=1}^{m}w_{ji}^{post}n_{i} $$ Thus $$ \\nabla_{n_{i}} \\mathcal{L_{m}} = \\sum_{j=1}^{l} \\nabla_{n_{j}} \\mathcal{L_{m}} \\cdot w_{ji}^{post} $$ 讨论： 判断节点是否冗余，需要度量节点对模型计算过程中的贡献。 我们想判断一个节点是冗余的，如果表征冗余而前向不冗余，则该节点的贡献不容忽视；如果前向冗余而表征不冗余，则该节点的也不能判断为冗余。 From this view, either the represenation redundancy or the forward redundancy is not sufficient when measuring the contribution of a node. 节点冗余的充分条件是表征冗余和前向冗余同时存在。 IV（定义 intrinsic dimension）\nWhether a node is redundant or not is determined by $W^{pre}$ and $W^{post}$, or even parameters several layers away. Circuit discovery 通过 IE 来度量节点的贡献。IE 小于阈值，则说明该节点是冗余的。冗余总量为 $d_{r}$，则 intrinsic dimension 为 $D-d_{r}$. (Definition)（广义的 intrinsic dimension: general intrinsic dimension）: 狭义的 intrinsic dimension 是在 neuron level 的范畴下定义的，而广义的 intrinsic dimension 可以定义在任意 granularity 下。对于某一任务，我们选取一个 granularity ，用 IE 来衡量节点的重要性，并选取一个阈值 $\\tau_{IE}$。对于某一个表征 $H \\in \\mathbb{R}^{D}$, 如果 IE(node) 小于阈值，则说明该节点对于该任务情境下而言是冗余的。如果冗余量是 $d_{r}$，则该表征的冗余量为 $D-d_{r}$，也就意味着该表征的映射参数 $W \\in \\mathbb{R}^{m \\times n}$ 在维度 $m$ 上存在参数冗余。 V (命题？)（LoRA 的特征选择 feature selection）\nBy performing singular value decomposition $\\Delta W = U\\Sigma V = \\sum_{i=1}^{r}u_{i}\\sigma_{i}v_{i}$ on weight matrix $\\Delta W$, feature selection is feasible with the optimization of the orthogonal basis in $U,V$ and the mask of singular value $\\sigma_{i}$.\n输入为$x\\in T$, 则 $$\\Delta Wx = U\\Sigma Vx = \\sum_{i=1}^{r}u_{i}\\sigma_{i}v_{i}x = \\sum_{i=1}^{r}(\\sigma_{i}v_{i}x)\\cdot u_{i} = \\sum_{i=1}^{r}a_{i}u_{i} $$ 我们称$a_{i}$为$x$的特征在表征空间内$e_{i}$方向上的强度。 From the decomposition of $\\Delta W$, we can see that all features are forced to be represented only using bases in $U$. Thus the optimization of $\\Delta W$ is to establish a better orthogonal bases $U$ that could minimize the deformation of feature directions and preserve the semantics of task-related features.\nW特征提取: consider the function of $W$ as feature extraction. 变形程度超过阈值的特征被丢弃。这里要重新定义表征冗余，同时去掉多个维度保证形变不超过阈值。 解释/证明\n假设特征$f$所需正交基集合为$e_{f} = { e_{f_{1}}, e_{f_{2}}, \u0026hellip;, e_{f_{k}} k\\leq m } $是$m$维表征空间的基底集合$e$的子集,，即特征向量$v_{f}=c_{1}e_{f_{1}}+c_{2}e_{f_{2}}+\u0026hellip;+c_{k}e_{f_{k}}$ ($c_{i}$为系数且$|ci|≤1, \\sum_{i=1}^{k}ci^{2}=1$)。若$e_{f}$对应的奇异值集合$\\sigma_{f}={\\sigma_{f_{1}}, \\sigma_{f_{2}}, \u0026hellip;, \\sigma_{f_{k}}} $内的某些元素被mask，且mask后的特征方向为$\\tilde{v_{f}}$，我们可以用余弦相似度定义mask操作前后特征的变形程度： $$ cosine_sim = \\frac{v_{f}\\dot \\tilde{v_{f}}}{|v_{f}||\\tilde{v_{f}}|} $$ 给定一个阈值s, 若$cosine_sim \\lt s$，则该特征被丢弃，反之则被选择。 分析\n奇异值不是直接选取特征，而是选取表征空间里的正交基。 ~~ 特征选择的结果是 维度冗余: there is redundancy in the dimensions of the latent space $\\mathbb{R}^{D}(D=m)$ where the features lie. ~~ （目标等价 objective equivalence）: Selecting singular values based on importance scores in AdaLoRA is equivalent to selecting nodes based on indirect effect in circuit discovery. (除了AdaLoRA，是否还有其他的指标)\n感性理解：重要性分数的思想来源于剪枝，而circuit discovery也在做剪枝的事情 证明 指标衡量的是：contribution of a node $n$ to the model\u0026rsquo;s behavior\n重要性分数\n参考 **Importance estimation for neural network pruning importance score的一般形式，考虑了参数敏感性 Super tickets in pre-trained language models: From model compression to improving generalization transformer里应用importance score Are sixteen heads really better than one? Platon: Pruning large transformer models with upper confidence bound of weight importance importance score的改进：考虑到IS计算结果的不稳定性(AdaLoRA里的滑动平均操作？) **Movement pruning: Adaptive sparsity by fine-tuning 在微调的时候观察参数变化，从而进行剪枝（微调服务于剪枝） 本文是：在剪枝的过程中微调（剪枝服务于微调，剪枝是为了降低微调的计算量） importance score考虑到了参数敏感性，且在推导时用到了泰勒展开；indirect effect在attribution patching里也用到了泰勒展开\n$W^{\\prime}x = (W + \\Delta W)x$ A the j-th neuron $$ n_{j} = \\sum_{i=1}^{r}u_{ij}\\sigma_{i}v_{i}x$$ IS of a parameter $w$: $$I(w) = |w\\nabla_{w}\\mathcal{L}|$$ where $L$ is the loss function(e.g. next token prediction loss). According to this paper, the form of $I$ is actually a approximation of the intervention effect on $w$ when pruning neural networks. Specifically speaking, we can perform sensitivity analysis on a parameter based on the difference in loss induced by removing it. The difference in loss $diff$ can be written as $$|E(D, W)-E(D, W|w=0)|$$ where $E$ is the fitting error, $D$ is the training dataset, $W$ represents the parameters in a model and $w=0$ means removing $w$ by replacing its value with zero. Directly computing $diff$ is inefficient, so [\u0026hellip;] et al regard $diff$ as a function of $w$ and simplify it by making a first-order Taylor expansion at $w=w_{origin}$, where $w_{origin}$ is the original value of $w$. So it comes to the form of importance score $I$, and $I$ is essentially a simpilified form of indirect effect. logits difference 和 log prob difference 都能推导成 difference in loss (见 Streamlit)\nWhen we apply zero ablation which set $n_{noise}$ to zero, equation () takes almost the same form as equation (): $$ IE(n; x_{clean}, x_{noise}=0) \\approx n_{clean} \\nabla_{n} \\mathcal{L_{m}}(\\mathcal{M}(x_{clean})) $$ Thus, when we inspect in the importance of a neuron in $\\mathbb{R}^{m}$, we can divide it into the importances of singular values in $W \\in \\mathbb{R}^{m \\times n}$ since we can split the value of a neuron into terms including singular values (see). So with chain rule we have: $$ I(\\sigma_{i}) = |\\sigma_{i} \\nabla_{\\sigma_{i}}\\mathcal{L}| = |\\sigma_{i} \\sum_{j=1}^{m} (\\nabla_{n_{j}}\\mathcal{L} \\cdot \\nabla_{\\sigma_{i}}n_{j})| = |\\sigma_{i} \\sum_{j=1}^{m} [\\nabla_{n_{j}}\\mathcal{L} \\cdot (u_{ij}v_{i}^{T}x)]| = | \\sum_{j=1}^{m} [\\nabla_{n_{j}}\\mathcal{L} \\cdot (u_{ij}\\sigma_{i}v_{i}^{T}x)]| = | \\sum_{j=1}^{m} (\\nabla_{n_{j}}\\mathcal{L} \\cdot n_{ji})| = | \\sum_{j=1}^{m} (\\nabla_{n_{j}}\\mathcal{L} \\cdot \\nabla_{n_{ji}}n_{j} \\cdot n_{ji})| = | \\sum_{j=1}^{m} (n_{ji} \\cdot \\nabla_{n_{ji}}\\mathcal{L})| \\approx | \\sum_{j=1}^{m} IE(n_{ji}; x_{clean}, x_{noise}=0)| $$\nRepresentation $N$ can be spilited into iterms corresponding to the singular values of $W^{pre}$. Thus we have: $$ I(\\sigma_{i}) \\approx |IE(N_{i})| $$ $$ N = \\sum_{i=1}^{r}N_{i} $$ We can see the importance score of $\\sigma_{i}$ is approximately equal to the $IE$ of the $i$-th component of the representation $N$ in terms of the SVD on $W^{pre}$.\n参考\nINTRINSIC DIMENSIONALITY EXPLAINS THE EFFECTIVENESS OF LANGUAGE MODEL FINE-TUNING MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES 其他理论分析\nlora A Kernel-Based View of Language Model Fine-Tuning The Impact of LoRA on the Emergence of Clusters in Transformers LoRA Training in the NTK Regime has No Spurious Local Minima Asymmetry in low rank adapters of foundation models The expressive power of low-rank adaptation feature learning Neural Networks can Learn Representations with Gradient Descent 算法\n算法设计\n算法论述\n重要性分数的平滑 $$IE(n)^{(t)} = E_{X^{(t)}}(IE(n, x, x_{noise}))$$ $$IE(n)^{*(t)} = \\overline{IE(n)}^{(t)} \\cdot \\overline{U(n)}^{(t)}$$ $$\\overline{IE(n)}^{(t)} = \\beta_{1}IE(n)^{(t-1)} + (1-\\beta_{1})IE(n)^{(t)}$$ $$\\overline{U(n)}^{(t)} = \\beta_{2}U(n)^{(t-1)} + (1-\\beta_{2})|IE(n)^{(t)} - \\overline{IE(n)}^{(t)}|$$ \u0026ldquo;随机激活\u0026rdquo; 细节\n**aggregation method 简单任务（如主谓不一致）：patch activation后，用logit difference/log prob difference John has a pencil John have a pencil 复杂任务（如数学）：整段话输进去，先算IE在一句话中的平均值，再算在样本间的均值 weighted aggregation circuit剪枝的方法 是大于$\\tau$就保留，还是按top_b？ tau如何设置？事先在数据集上求平均？ 是否需要设置类似于budget scheduler的东?(AdaLoRA逐步增加top_b，也就是逐渐增加增量$\\delta W$) **滑动计算IE：抵抗方差，算是一种对circuit discovery的优化 node or edge, or both?(参考 sparse features circuits, AtP*) activation patching path patching 微调哪些部分，是否要与LoRA对应？ soft mask(\u0026ldquo;随机激活\u0026rdquo;)? 就是C以外的components随机选取进行更新（参考：SoftNet） 正则化？（灾难性遗忘） MOE routing? 算法流程\nInput: $\\mathcal{D}$: dataset, $\\mathcal{M}$: model, $G$: the computing graph of $\\mathbb{M}$, $\\mathcal{L_{m}}$: metric for measuring indirect effect, $\\tau_{n}$: threshold for nodes in circuit discovery, $\\tau_{e}$: threshold for edges in circuit discovery, T: total number of training steps Output: Fine-tuned model $\\mathcal{M^{\\prime}}$ Set circuit $C \\leftarrow G$\nfor $i=1$ to $T$ do\nsample a mini-batch $X={x_{1}, x_{2}, \u0026hellip;, x_{n}} \\sim \\mathcal{D}$\n// circuit discovery\n// patch nodes\nfor node $n \\in C$ do\nif $E_{X}(IE(n, x, x_{noise})) \\lt \\tau_{n}$ then\n$C \\leftarrow C \\backslash n$\nend\nend\n// patch edges\nfor edge $e \\in C$ do\nif $E_{X}(IE(e, x, x_{noise})) \\lt \\tau_{e}$ then\n$C \\leftarrow C \\backslash e$\nend\nend\n//edge tuning\nfor edge $e \\in C$ do\nGet parameter $w$ correspond to $e$\nUpdate $w = w - \\eta \\nabla_{w}\\mathcal{L}$\nend\nReset circuit $C \\leftarrow G$\nend\n参考\n(ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability (attribution patching) Attribution Patching Outperforms Automated Circuit Discovery Attribution patching: Activation patching at industrial scale AtP*: An efficient and scalable method for localizing llm behaviour to components How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model Linear Representations of sentiment in large language models 收敛性证明（可选）\nLipschitz continuity（参考Forget-free Continual Learning with Soft-Winning SubNetworks） Lipschitz gradient continuity: Let $\\theta \\in \\mathbb{R}^{D} = (\\theta_{1}, \\theta_{2}, \u0026hellip;, \\theta_{|D|})$. If \u0026hellip;, then we have $$ ||\\nabla_{\\theta}f(\\theta^{\\prime}) - \\nabla_{\\theta}f(\\theta)|| \\leq L||\\theta^{\\prime} - \\theta|| $$ where $L$ is Lipschitz constant. If we mask out a set of parameters $\\theta_{mask} \\subset \\theta$, then we have $$ ||\\nabla_{\\theta \\backslash \\theta_{mask}}f(\\theta^{\\prime}) - \\nabla_{\\theta \\backslash \\theta_{mask}}f(\\theta)|| \\lt ||\\nabla_{\\theta}f(\\theta^{\\prime}) - \\nabla_{\\theta}f(\\theta)|| \\leq L||\\theta^{\\prime} - \\theta|| $$ This is because for each masked parameter $\\theta_{i} \\in \\theta_{mask}$, the gradient of it is equal to 0, which leads to a smaller norm for gradient change. Thus a smaller Lipschitz constant $L_{mask} \\lt L$ serves as the upper bound for the change rate of gradients when we mask out a set of unrelevant parameters, which means the training process will be more stable compared with full fine-tuning. circuit-tuning的特性\n缓解 catastrophic forgetting，interpretable 现有方法 memory replay regularization **parameter isolation 理论上解释如何缓解 参考 微调 Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks Understanding Catastrophic Forgetting in Language Models via Implicit Inference Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking lora *(O-LoRA) Orthogonal Subspace Learning for Language Model Continual Learning *(I-LoRA) Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning InfLoRA GSLoRA 持续学习 Recent Advances of Foundation Language Models-based Continual Learning: A Survey **A Comprehensive Survey of Continual Learning: Theory, Method and Application Brain-inspired learning in artificial neural networks: a review 区别与联系\n与LoRA/AdaLoRA有什么区别？ lora不好对W奇异值分解，所以对增量进行微调，而circuit-tuning直接微调原参数 lora选奇异值的最终结果就是选neuron，也就是说lora直接从参数入手，而circuit-tuning先从mech interp的角度发现circuit，找到edge对应的参数，然后微调 与剪枝相比 相似 用到了lottery ticket的思想 区别 剪枝是模型已经有了某个能力，然后剔除与该能力无关的结构（是这样？），而circuit-tuning是在没有该能力的情况下一步步探索 与continual learning中parameter isolation方法的区别 相似 over-parameterized的假设与intrinsic dimension类似 区别 circuit-tuning可扩展到大于neuron的level，而continual learning大多是neural level circuit-tuning从mech interp的角度考虑，本身是一种微调方法，同时附带了缓解灾难性遗忘的功能 circuit-tuning剪枝的指标是IE；而持续学习如CLNP是计算平均激活值，激活值小会被剪掉，或是SoftNet引入了可学习的weight score，根据weight score设置mask continual learining的场景主要是按顺序进行的t个任务，而circuit-tuning则是提供一种微调的思路 参考（相似成果） (CLNP) **Continual learning via neural pruning Forget-free Continual Learning with Soft-Winning SubNetworks 实验设计\ndataset\ntask for small model subject-verb \u0026ldquo;disagreement\u0026rdquo; 损失函数 加权似然函数: $p\\in(0, 1]$ gender bias for LLM math, instruction following, code, language transfer, \u0026hellip; others safety gender bias(见SAE circuits) model\nsmall model v.s. LLM small: Pythia, GPT-2, Gemma, Phi Pythia: rotary embeddingss large: Mistral, Llama ReLU特征选择\ncircuit-tuning\nsplit granularity: nodes(neurons or heads? 详见SAE circuits, AtP*) acdc? eap: vectors as nodes spc: neurons as nodes (为了跟SAE features作对比) AtP*: neuron level \u0026amp; vector level ablation methods(zero? mean? ) 见 IOI 论文的讨论 $L$选哪个？logit difference / log prob 是下一个token还是什么？(aggregation method) IE选哪个？标准activation patching/AtP/AtP*/ig threshld evaluation SAE? evaluation\n简单任务 主谓不一致相关指标： circuit相关指标：faithfulness, completeness, \u0026hellip;（见sfc） 复杂任务 通用能力 analyses\n主谓一致-\u0026gt;主谓不一致 circuit的变化 intrinsic dimension的验证：看非零奇异值个数以及circuit中neuron的个数 LoRA和circuit-tuning相互验证 通用能力怎么保证？（谁都可以随便选取一部分参数，然后让新的能力占据其原有能力） application\nmodel steering Hebbian Learning\nAI of Brain and Cognitive Sciences: From the Perspective of First Principles 优点和不足\n优点 参数高效 可解释，更精准 缓解灾难性遗忘 不足 需要先找到circuit 计算量大 不像LoRA一样可插拔 难以scale? linguistics 语言习得，二语习得 迁移学习 ","permalink":"http://localhost:1313/posts/my_research_interests/","summary":"mechanistic interpretability Computational Linguistics Circuit-tuning: A Mechanistic Approach for Understanding Instrinsic Dimension and Fine-tuning Neural Networks 袁老师建议 理论 重新审视可解释性，如何给出一个漂亮的解释（思考mech interp局限性，是否可以突破一下） 上","title":"My research interests"},{"content":"The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.\nCircuits Discovery Methods basic activation patching (causal mediation/interchange interventions\u0026hellip;) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? \u0026#x1f52d; resources (ROME) Locating and Editing Factual Associations in GPT Attribution patching: Activation patching at industrial scale (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability Attribution Patching Outperforms Automated Circuit Discovery AtP*: An efficient and scalable method for localizing llm behaviour to components Causal Scrubbing: a method for rigorously testing interpretability hypotheses new Using SAE Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models Automatically Identifying Local and Global Circuits with Linear Computation Graphs Contextual Decomposition Mechanistic Interpretation through Contextual Decomposition in Transformers Edge Pruning ? Finding Transformer Circuits with Edge Pruning Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning Evaluation lack of ground truth\nhuman interpretability automatic faithfulness (see this)\nhow much of the full model’s performance can a circuit account for.\ncompleteness\ncomputational efficiency Hypothesis Testing the Circuit Hypothesis in LLMs Issues ablation methods: dropout out is also an ablation, so does zero ablation work? superposition, need the help of SAE? Dictionary Learning SAE\nTraining and optimization proper SAE width dead neurons \u0026#x1f52d; resources Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders Sparse Crosscoders for Cross-Layer Features and Model Diffing Open Source Replication of Anthropic’s Crosscoder paper for model-diffing Transcoders Find Interpretable LLM Feature Circuits Scaling and evaluating sparse autoencoders Improving Dictionary Learning with Gated Sparse Autoencoders Evaluation human auto \u0026#x1f52d; resources (Anthropic, 2024-8, contrastive eval \u0026amp; sort eval) Interpretability Evals for Dictionary Learning (RAVEL) RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations Language models can explain neurons in language models Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control Analysis A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders Are Sparse Autoencoders Useful? A Case Study in Sparse Probing Applications SAE + feature discovery \u0026#x1f52d; resources Sparse Autoencoders Find Highly Interpretable Features in Language Models Anthropic research Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet SAE + circuit discovery \u0026#x1f52d; resources Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models SAE + explain model components Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors Sparse Autoencoders Work on Attention Layer Outputs Interpreting Attention Layer Outputs with Sparse Autoencoders SAE + explain model behaviors The Geometry of Concepts: Sparse Autoencoder Feature Structure SAE + model steering SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders Steering vectors activation steering Steering Language Models With Activation Engineering (lesswrong) Steering GPT-2-XL by adding an activation vector Steering Llama 2 via Contrastive Activation Addition Inference-Time Intervention: Eliciting Truthful Answers from a Language Model Function vectors in large language models feature steering Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Evaluating feature steering: A case study in mitigating social biases representation engineering Representation Engineering: A Top-Down Approach to AI Transparency Others LUNAR: LLM Unlearning via Neural Activation Redirection Mechanistically Eliciting Latent Behaviors in Language Models Model Diffing Stage-wise fine-tuning\nfine-tuning\nscaling Stage-Wise Model Diffing Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks Understanding Catastrophic Forgetting in Language Models via Implicit Inference Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To! Dataset-wise Modeldiff: A framework for comparing learning algorithms Algorithm-wise Adversarial robustness as a prior for learned representations Representation equivariance meta-SNE Visualizing Representations: Deep Learning and Human Beings model stitching Understanding image representations by measuring their equivariance and equivalence Revisiting model stitching to compare neural representations SVCCA and similar methods SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability others analyses The Platonic Representation Hypothesis theory When Representations Align: Universality in Representation Learning Dynamics Explain Model Components explain neurons, attention heads and circuits\nExplain neurons \u0026#x1f52d; resources Finding Neurons In A Haystack LatentQA: Teaching LLMs to Decode Activations Into Natural Language Language models can explain neurons in language models Multimodal Neurons in Artificial Neural Networks Finding Safety Neurons in Large Language Models Explain attention heads different heads in one layer/heads in different layer -\u0026gt; grammar/semantic feats\nattention pattern\nAnalyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention 为什么Transformer 需要进行 Multi-head Attention？ special heads\nCopy Suppression: Comprehensively Understanding An Attention Head Explain circuits understand specific circuits on the subspace level\n(IOI) Interpretability in The Wild: A Circuit For Indirect Object Identification in GPT-2 Small What Do the Circuits Mean? A Knowledge Edit View (DAS) Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations LLM Circuit Analyses Are Consistent Across Training and Scale Explain layernorm On the Nonlinearity of Layer Normalization Others The Quantization Model of Neural Scaling Explain Model Behaviors Feature representations linear representations theory Linear Explanations for Individual Neurons multilingual representations Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs Emerging Cross-lingual Structure in Pretrained Language Models Probing the Emergence of Cross-lingual Alignment during LLM Training Exploring Alignment in Shared Cross-lingual Spaces mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models? Probing LLMs for Joint Encoding of Linguistic Categories Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure multimodal representations Interpreting the Second-Order Effects of Neurons in CLIP Interpreting CLIP\u0026rsquo;s Image Representation via Text-Based Decomposition safety reprs \u0026#x1f52d; resources Linear Representations of sentiment in large language models nonlinear representations Not All Language Model Features Are Linear Model capabilities training (learning) dynamics\nin-context learning\nbasic In-context Learning and Induction Heads What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation bad in-context learning (learn wrong things) Overthinking The Truth: Understanding How language Models Process False Demonstrations chain of thought (COT)\nhow and why step by step?\nzero-shot COT ???\nanalyses Iteration Head: A Mechanistic Study of Chain-of-Thought How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation Do Large Language Models Latently Perform Multi-Hop Reasoning? unfaithful COT Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting Does the model already know the answer while reasoning, or the model really has a goal? reasoning\nUnveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization How Do LLMs Perform Two-Hop Reasoning in Context? planning\nEvaluating Cognitive Maps and Planning in Large Language Models with CogEval instruction following\nhow does reinforcement learning change the inside of a model? understand RL at mechanistic level high efficient RLxF SAIF: A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models knowledge\nCharacterizing Mechanisms for Factual Recall in Language Models Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts The Geometry of Concepts: Sparse Autoencoder Feature Structure memorization \u0026amp; generalization\nWhat Do Learning Dynamics Reveal About Generalization in LLM Reasoning? learning dynamics\nLoss Landscape Degeneracy Drives Stagewise Development in Transformers Loss landscape geometry reveals stagewise development of transformers duplication\nself-repair\nThe Hydra Effect: Emergent Self-repair in Language Model Computations Explorations of Self-Repair in Language Models grokking\nProgress Measures For Grokking Via Mechanistic Interpretability Towards Understanding Grokking: An Effective Theory of Representation Learning phase transition\nIn-context Learning and Induction Heads double descent\nDeep double descent massive activations\nMassive Activations in Large Language Models Systematic Outliers in Large Language Models Narrow tasks counting greater-than How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model Indirect Object Indentification (IOI) Interpretability in The Wild: A Circuit For Indirect Object Identification in GPT-2 Small Interpretable model structure also called intrinsic interpretability\nModifying Model Components (SoLU) Softmax Linear Units Reengineering Model Architecture (Except interpretable model architectures, I also list some fresh-new architectures.)\nCBM (Concept Bottleneck Models)\nConcept Bottleneck Models Concept Bottleneck Large Language Models Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization Backpack Language Models\nBackpack Language Models others\nSeeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability SEER: Self-Explainability Enhancement of Large Language Models’ Representations Modular Training of Neural Networks aids Interpretability Compositional attention networks for machine reasoning new architetures\nA Path Towards Autonomous Machine Intelligence Large Concept Models: Language Modeling in a Sentence Representation Space Large Language Diffusion Models Fractal Generative Models JEPA Vision models survey Sparks of Explainability Recent Advancements in Explaining Large Vision Models Other Methods \u0026amp; Analysis decomposing a model Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition Application AI alignment 3H (helpful, honest, harmless) Avoid bias and harmful behaviors\nconcept-based interpretability representation-based interpretability red-teaming perturbations\nbackdoor detection, red-teaming, capability discovery\nanomaly detection\nbackdoor detection\nSleeper Agents: Training Deceptive LLMs that Persist Through Safety Training SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks Mechanistic anomaly detection and ELK A gentle introduction to mechanistic anomaly detection Concrete empirical research projects in mechanistic anomaly detection refuse to request \u0026amp; jailbreak circuit; SAE; steering vector (anti-refusal)\nmeasures (repr engineering) Improving Alignment and Robustness with Circuit Breakers (steering) Refusal in Language Models Is Mediated by a Single Direction (steering) Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models (training) Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications analyses Universal and Transferable Adversarial Attacks on Aligned Language Models Many-shot jailbreaking Jailbroken: How Does LLM Safety Training Fail? Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs (vlm) Visual Adversarial Examples Jailbreak Aligned Large Language Models (vlm) Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models (vlm) Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything evaluation \u0026amp; benchmark Jailbreak prompts finding on Twitter JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models power-seeking\nParametrically Retargetable Decision-Makers Tend To Seek Power social injustice prejudice, gender bias: doctor \u0026amp; nurse, discrimination\ntraining dynamics; dataset; gradient descent; SAE circuits\nEvaluating feature steering: A case study in mitigating social biases Prejudice and Volatility: A Statistical Framework for Measuring Social Discrimination in Large Language Models Unveiling Gender Bias in Large Language Models: Using Teacher’s Evaluation in Higher Education As an Example deception\ndishonesty\nAlignment faking in large language models Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training Language Models Learn To Mislead Humans Via RLHF How do Large Language Models Navigate Conflicts between Honesty and Helpfulness? reward hacking\nReward Hacking in Reinforcement Learning measurement tampering\nThe AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome.\nBenchmarks for Detecting Measurement Tampering persona drift\nMeasuring and Controlling Persona Drift in Language Model Dialogs other human values\nSycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models Reducing sycophancy and improving honesty via activation steering Modulating sycophancy in an RLHF model via activation steering Agency\nUnderstanding and Controlling a Maze-Solving Policy Network Parametrically Retargetable Decision-Makers Tend To Seek Power Avoiding Side Effects in Complex Environments Alignmnet theory\nBoth alignment and interpretability are related to AI safety, so the mech interp tools are widely used in alignment research. I\u0026rsquo;ll put some good resources of alignment work here.\nqualitative work (findings, analyses, concepts, \u0026hellip;) * alignment representation * instrumental convergence * shard theory\nProposed by Alex Turner (TurnTrout) and Quintin Pope The Shard Theory of Human Values (lesswrong) The Shard Theory of Human Values \u0026#x1f52d; resources AI Alignment: A Comprehensive Survey Representation Engineering: A Top-Down Approach to AI Transparency Mechanistic Interpretability for AI Safety A Review Improved Algorithms ReFT: Representation Finetuning for Language Models Harmonic Loss Trains Interpretable AI Models Research Limitations Current work mainly focuses on Transformer-based models. Is transformer a inevitable model structure for generative language models? How can we use post-hoc methods as a guide for training a more interpretable and controllable model? Other Interpretability Fields Neural network interpretability Not necessarily a transformer-based model, maybe an lstm or simply a toy model\nTheories for DL foocker/deeplearningtheory Feature Learning Huang wei\u0026rsquo;s repo game (chess) Evidence of Learned Look-Ahead in a Chess-Playing Neural Network (Sokoban, planning) Planning behavior in a recurrent neural network that plays Sokoban geometry Reasoning in Large Language Models: A Geometric Perspective Other Surveys Open Problems in Mechanistic Interpretability A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models Mechanistic Interpretability for AI Safety: A Review A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models Towards Uncovering How Large Language Model Works: An Explainability Perspective ","permalink":"http://localhost:1313/posts/mech_interp_research/","summary":"The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic. Circuits Discovery Methods basic activation patching (causal mediation/interchange interventions\u0026hellip;) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? \u0026#x1f52d; resources (ROME) Locating and","title":"Possible Research Areas in Mechanistic Interpretability"},{"content":"\u0026#x1f3b6;Code in this post can be found at the jupyter notebook in my \u0026ldquo;saeExploration\u0026rdquo; repo.\nFind features that reflect positive emotions To find the features related to a specific emotion, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:\n1 2 3 4 5 prompt_happy = [\u0026#34;I\u0026#39;ll be on a vacation tomorrow and I\u0026#39;m so happy.\u0026#34;, \u0026#34;My mombrings home a new puppy and I\u0026#39;m so happy.\u0026#34;, \u0026#34;I\u0026#39;m so glad I got the job I wanted.\u0026#34;, \u0026#34;I feel so happy when I\u0026#39;m with my friends.\u0026#34;, \u0026#34;I\u0026#39;m so happy I got the promotion I wanted.\u0026#34;,] I choose to look for features that reflect happiness and sadness. Apart from that, I also wonder if the feature that reflects excitedness has something to do with the one that reflects happiness (they are alike from the semantic level at least.)\nFor a start, I inspected the residual stream in layer_7. The SAE we choose is gpt2-small-res-jb which hooks at the residual stream at the entrance of a layer. The prompts were fed into the model and the outputs were SAE activations. I checked the activations at the word “happy” for all the prompts and calculated the mean value of them. I visualized them as below:\nFigure 1: Feature activations at the keywords for happy emotion Obviously there are three SAE features that activate most actively on the happy emotion, and their feature ids are 2392, 9840 and 21753. I checked the feature 2392 in Neuronpedia and got its feature dashboard:\nFigure 2: SAE feature 2393 on the dashboard (https://neuronpedia.org/gpt2-small/7-res-jb/2392?embed=true\u0026embedexplanation=true\u0026embedplots=true\u0026embedtest=true\u0026height=300). Later I found features for sadness and excitedness in the same way. The top-3 features activating for “sad” and “excited” are [2045, 23774, 10866] and [8935, 9840, 3247] respectively.\nCompare the features related to happiness and excitedness I want to see the difference between \u0026ldquo;happy features\u0026rdquo; and \u0026ldquo;excited features\u0026rdquo; since they are both positive emotions. So I compared their top-3 features and only kept the features that activate on both “happy” and “excited”. They are visualized as below:\nFigure 3: Feature activations on both happy and excited emotions at the residual pre stream in layer_7. From the figure above, we can easily find out the two features shared by happiness and excitedness. It seems that these two features contain positive emotion concepts, which means they are close to each other on the semantic level.\nA deeper investigation into the features related to happiness From the previous result, we can see that there are 3 features that fire quite actively on happiness. Since they share similar semantic meanings, I guess that the representations of features activating on the same emotion (i.e. 2392, 9840 and 21753) have high similarities. To prove this, I try to inspect the representation of one prompt which expresses happiness and calculate the cosine similarities among representations of different features. Here the representations of different layers are calculated as below: $$ feat \\_ repr = W_{dec} * SAE \\_ activations $$ Note that the * operation is a dot product. The W_dec in the formula above is the decoder matrix of SAE with a shape of (d_sae, d_model). We know that according to the definition of dictionary learning, each vector in the dimension of d_sae corresponds to a base vector for an SAE feature. Each element in the SAE_activations can be seen as the intensity of the feature at that position. So by multiplying W_dec and SAE_activations we can get a representation of shape $ (d \\_ model,) $ which expresses the features in the vector space of SAE that is sparser and more interpretable than that of the original model.\nTo visualize the similarities among features, I choose to use the heatmap. Note that in order to make it clear, I mainly focus on features 2392, 9840 and 21753 and using negative sampling to get some other features for comparison. I randomly pick 6 features except for the three features mentioned above. The similarities are calculated and shown as below:\nFigure 4: Feature similarities on the word “happy” at the residual pre stream in layer_7. Obviously we can find that the three features that fire most actively on “happy” are more similar to each other (the top left 3*3 square), thus my hypothesis is proved intuitively.\nFeatures in different layers Figure 5: Feature activations on the word “happy” at the residual pre stream in 12 layers. \u0026ensp;\u0026ensp;Previously I inspected in the 8th layer of gpt2-small and found some emotional features. Now I want to know if similar features exist in other layers, and how they are related to each other. I inspect the autoencoder features in each layer and observe their activations on the word “happy”. The result is shown in Fig 5. We can find that: * Eachlayer has more than 3 features that activate on the happy emotion. * Thepositions of activating features are different from those in other layers. Though the positions of activating features are different, I guess it\u0026rsquo;s just the problem of feature orders. For example, the feature 7683 in layer_1 may be the same kind or even exactly the same feature as feature 13928 in layer_3, though the positions are different. In order to prove this, I choose to visualize the feature representations of SAE outputs. The result is shown below:\nFigure 6: Feature similarities on the word “happy” at the residual pre stream in 12 layers. From the figure above, I got some interesting points about the feature distribution:\nThe features in the first layer have lower similarities with those in later layers. I guess this is because the first layer gets limited information from previous layers, so it cannot express relatively complicated concepts like emotions. I think maybe the first layer contains some low level concepts that are not shown in this figure, which I will explore in the future. Afeature in a layer often corresponds to a feature in another layer. For example, feature 7683 in layer_1 corresponds to feature 13928 in layer_2, which has a similarity of 0.939. This means they are related to each other across different layers, sharing similar semantic meanings. Afeature in a layer tends to be more alike to features in nearby layers. For example, feature 7683 in layer_1 is more similar to feature 13928 in layer_2 than feature 2392 in layer_7, with a similarity of 0.939 to 0.825. I think it\u0026rsquo;s because the vector spaces of nearby layers are relatively close to each other. When two layers are far from each other, the difference between their vector spaces is significant due to a lot of linear and nonlinear manipulations between layers. Thus the features would share low similarities regardless of similar semantic meanings. ","permalink":"http://localhost:1313/posts/happy_feats/","summary":"\u0026#x1f3b6;Code in this post can be found at the jupyter notebook in my \u0026ldquo;saeExploration\u0026rdquo; repo.\nFind features that reflect positive emotions To find the features related to a specific emotion, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:\n1 2 3 4 5 prompt_happy = [\u0026#34;I\u0026#39;ll be on a vacation tomorrow and I\u0026#39;m so happy.\u0026#34;, \u0026#34;My mombrings home a new puppy and I\u0026#39;m so happy.","title":"Exploring Emotional Features in GPT2-Small"},{"content":"The purpose I write this blog Mechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challenges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc). Thus it\u0026rsquo;s a little bit difficult for people new to this area to figure out what researchers are really doing.\nTherefore I write this blog to give a brief introduction to mechanistic interpretability without so much of horrible concepts. The blog aims to help you understand the basic ideas, main directions and latest achievements of this field, providing a list of resources to help you get started at the same time!\nIf you really want to do some cool research as a beginner, I highly recommend the guide by Neel Nanda.\nWhat is Mechanistic interpretability? Speaking of AI research, neural network is the tool that is used most widely nowadays for its excellent representation and generalization ability. What does a neural network do? It receives an input and gives an output after some calculations. Specifically speaking, it usually gets the representations of an input and maps it to an expected output under a predefined computation graph. From my perspective, neural networks mainly care about two things: create representations to extract features and establish the relationship between the representations and the output.\nWhy is neural network so popular? An important reason is that the neural network can save a lot of time for researchers to manually design features. For example, for natural language processing (NLP) people often designed features like \u0026ldquo;the frequency of a word that appears\u0026rdquo; or \u0026ldquo;the co-occurrence probabilities\u0026rdquo; in the past. Manually designing features caused too much labor, so people choose to use neural networks to find features automatically. As for optimizing, they set a goal of minimizing the loss function and using backward propagation (BP) to update the parameters of the model. Thus neural networks free our hands and improve performance at the same time.\nAll is well, so why do we concern about interpretability? Though neural networks can extract a lot of features with a high efficiency, we cannot have a clear understanding of what the features really are. For example, we know that a filter with Laplacian operator can extract the edge of an image, but we don\u0026rsquo;t know what the features extracted by a convolution layer mean because the parameters of the filters inside are often randomly initialized and optimized using BP algorithm. As a result, features in neural networks are often ambiguous.\nWhy is interpretability important? Actually this statement is controversial because some people say interpretability is bullshit\u0026#x1f4a9;. I\u0026rsquo;m not angry about this. Anyway, people\u0026rsquo;s taste varies, just like many people enjoy Picasso\u0026rsquo;s abstract paintings while I don\u0026rsquo;t. Interpretability still lacks exploring so it\u0026rsquo;s now far from application, and that\u0026rsquo;s why some people look down on it. While it is this lack of exploration that excites me most because there are a lot of unknown things waiting for me to discover! Actually, Interpretability is a key component in the AI alignment cycle (see Figure 0). The goal of alignment is to \u0026ldquo;make AI systems behave in line with human intentions and values\u0026rdquo; and interpretability plays an important role in ensuring AI safety. For example, unwanted things like malicious text generated by a language model may be avoided using model steering (a trick played on the activations during the forward propagation). Besides, having a clear understanding of neural networks enables us to focus on the relevant part of a model to a specific task and perform fine-tuning in a more precise way (haha here is an ad for my project: circuit-tuning).\nFigure 0: The position of interpretability in the AI alignment cycle (from this survey)\rLast question: what is mechanistic interpretability? Let\u0026rsquo;s call it mech interp first because I\u0026rsquo;m really tired of typing the full name\u0026#x1f4a6;. There seems not to be a rigorous definition, but I here I want to quote the explanation by Chris Olah:\nMechanistic interpretability seeks to reverse engineer neural networks, similar to how one might reverse engineer a compiled binary computer program.\nAnother thing: there are various of categories of interpretability, such as studies from the geometry perspective or from the game theory and symbol system perspective, which can be found at ICML, ICLR, NeurlPS, etc. When we say mech interp, we often refer to the studies on Transformer-based generative language models now (though the research started before 2017) which will be introduced briefly in the next section. So before we start, let\u0026rsquo;s briefly go over the structure of Transformer first!\nFigure 1: The structure of Transformer (from Arena)\rFigure 2: The structure of the self-attention block in a Transformer block (from Arena)\rFigure 3: The structure of the MLP layer in a Transformer block (from Arena)\rFigure 4: The structure of the layer normalization in a Transformer block (from Arena)\rBasic ideas and research topics In this section, I\u0026rsquo;m gonna explain some terms for mech interp and help you understand the basic ideas of doing mech interp research. I\u0026rsquo;ll try to make it easy!\nNote that I\u0026rsquo;ll only introduce something that I think is important. If you wanna learn more about the concepts in mech interp, please refer to: A Comprehensive Mechanistic Interpretability Explainer \u0026amp; Glossary which is a very comprehensive guide for beginners that I strongly recommend!\nImportant concepts Features\nThere are a lot of definitions for features. Unfortunately none of the definitions above can be widely recognized, so it\u0026rsquo;s open for anyone who wants to seek for the essence of the features. Generally speaking, a feature is a property of an input which is interpretable or cannot be understand by humans. Practically speaking, a feature could be an activation value of a hidden state in a model (at least lots of work is focusing on this).\nHow to find a feature? Or how to know that the thing you find is likely to be a feature? Here I want to quote the concept of \u0026ldquo;the signal of structure\u0026rdquo; proposed by Chris Olah in the post of his thoughts on qualitative research:\nThe signal of structure is any structure in one\u0026rsquo;s qualitative observations which cannot be an artifact of measurement or have come from another source, but instead must reflect some kind of structure in the object of inquiry, even if we don\u0026rsquo;t understand it.\nJust like the discovery of cells under a microscope. The shape of the cells cannot be random noise but strong evidence for the structure of them.\nCircuits\nIf we view a language model as a directed acyclic graph (DAG) $M$ where nodes are terms in its forward pass (neurons, attention heads, embeddings, etc.) and edges are the interactions between those terms (residual connections, attention, projections, etc.), a circuit $C$ is a subgraph of $M$ responsible for some behavior. That means the components inside the circuit have a big influence on the output of the task, while the components outside the subgraph have almost no influence.\nFrom my perspective, a circuit is a path from an input to an output, just like the way between two hosts in the routing networks.\nFigure 5: The computational graph of a model (from Arena)\rSuperposition\nSuperposition is a hypothesis that models can represent more features than the dimensions they have.\nIdeally we expect that each neuron only corresponds to one feature, so we can investigate or even control the feature using the neuron reserved for it. But in practice we find that a neuron fires for more than one features, which is called the phenomenon of polysemanticity in neurons. We believe that we have more features than model dimensions, so we can also say that more than one neurons fire when a feature appears. That is to say, there is not a one-to-one correspondence between neurons and features.\nPrivileged basis\nIt\u0026rsquo;s a weird idea that I have some doubt on it (maybe I haven\u0026rsquo;t grasp the core idea of it\u0026hellip;).\nMy understanding: There are many vector spaces in a model, for example, the residual stream in a layer, the output of the ReLU in a MLP layer, etc. Each vector space can be seen as a representation. Given an input, we can get the hidden states in different vector spaces during the forward propagation of the model. If we could view neurons as directions which may correspond to features in a vector space, then we say there is a privileged basis in this vector space. That is to say, each value at a specific dimension is aligned with a neuron, and that value may be a interpretable feature (maybe not).\nNot all vector spaces in a model have privileged basis. The most accepted view is that privileged bases exist in attention patterns and MLP activations, but not in residual streams. A general law is that a privilege basis often appears with a elementwise nonlinear operation, for instance, ReLu, Softmax, etc. If the operations around a representation are all linear, then we say the basis in the representation is non-privileged. For example, the operations around a residual stream are often non-linear (e.g. $W_{in}$ and $W_{out}$ of a MLP layer which correspond to the \u0026ldquo;read\u0026rdquo; and \u0026ldquo;write\u0026rdquo; operation on the residual stream). If we apply a rotation matrix to the original operations to change the basis, then the result will be unchanged because In other words, something is a privileged basis if it is not rotation-independent, i.e. the nature of computation done on it means that the basis directions have some special significance. A privileged basis is a meaningful basis for a vector space. That is, the coordinates in that basis have some meaning, that coordinates in an arbitrary basis do not have. It does not, necessarily, mean that this is an interpretable basis.\na space can have an interpretable basis without having a privileged basis. In order to be privileged, a basis needs to be interpretable a priori - i.e. we can predict it solely from the structure of the network architecture.\nResearch techniques Circuits Discovery\nFinding the circuit for a specific task attracts the attention of lots of researchers. The thing we wanna do is to get the relevant components for a specific task. A naive idea is to test the components one by one using causal intervention: change the value of one component while keeping others unchanged, and check if it influences the output. This technique is also called ablation or knockout.\nTo achieve this, we have two possible ways: denoising (find useful components) and noising (delete unuseful components). We usually prepare a clean prompt which is relevant to the task (results in a correct answer) and a corrupted prompt which has nothing to do with the task. Before finding circuits, the two prompts are fed into the model separately to get a clean run and a corrupted run.\nIf we use denoising, at each step we replace (patch) the value of a component in the corrupted run with that in the clean run. If the output is closer to the correct answer under a specific metric (e.g. KL divergence or logit difference), then we add the component into the circuit. If we use noising, then we should replace a component in the clean run with that in the corrupted run. If the output is almost unchanged under a threshold, then we regard the component as useless and delete it. Generally speaking, denoising is better than noising. To understand this, I want to quote a line in Arena: noising tells you what is necessary, denoising tells you what is sufficient.\nSeveral techniques in this area:\nactivation patching (aka causal mediation/interchange interventions\u0026hellip;) A method for circuits discovery that take nodes into consideration. path patching A variant of activation patching that also take edges into consideration to study which connections between components matter. For a pair of components A and B, we patch in the clean output of A, but only along paths that affect the input of component B. While in activation patching, all the subsequent components after A are affected. attribution patching An approximation of activation patching using a first-order Taylor expansion on the metric. This method is used to speed up circuits finding. Figure 6: Comparison of activation patching and path patching (from Arena)\rThe difference between activation patching and path patching are shown in Figure 6. In activation patching, we simply patch the node $D$ with $D\u0026rsquo;$, so the nodes after $D$ ($H, G$ and $F$) are affected. While in path patching, we patch edges rather than nodes. For example, we only want to patch the edge $D \\to G$, which means the only change is the information from node $D$ to node $G$. As a result, only $G$ and $F$ are affected while $H$ isn\u0026rsquo;t.\n\u0026#x1f52d; Recommended papers: (ROME) Locating and Editing Factual Associations in GPT (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability (attribution patching) Attribution patching: Activation patching at industrial scale (IOI) INTERPRETABILITY IN THE WILD: A CIRCUIT FOR INDIRECT OBJECT IDENTIFICATION IN GPT-2 SMALL Dictionary Learning\nDictionary Learning aims to deal with the problem of superposition. The idea is like compression sensing in the field of signal processing and is discussed in this article. The implementation of dictionary learning is to train a sparse autoencoder (SAE).\nAn autoencoder consists of an encoder and a decoder. The encoder receives an input and compresses it to a lower dimension, and the decoder maps the hidden representation to the original input. The goal of the autoencoder is to get the representation of the input while compressing it. The autoencoder is optimized using the reconstruction loss.\nCompared with the autoencoedr, the dimension of the hidden representation in SAE is always higher than that of the input, which means the SAE does something completely opposite to the autoencoder. The idea behind is that the model dimension is smaller than the number of features to represent. The model may use superposition to make full use of limited neurons to represent more features. To get one-to-one correspondence between neurons and features, we map the representation to a higher dimensional vector space with SAE encoder. Once we get the representation in SAE (let\u0026rsquo;s call it sparse features), we maps it back to the original input with SAE decoder.\nIn practice, any hidden state in a model can be studied using SAE. For example, when we want to get the sparse features of the activations $h$ in a MLP layer. We can do as follows:\n$$ z = ReLU(W_{enc}h + b_{enc}) $$ $$ h^{\\prime} = W_{dec}z + b_{dec} $$\n$$ loss = \\mathbb{E}_{h}\\left[||h-h^{\\prime}||_{2}^{2} + \\lambda||z||\\right] $$\nNote that $ h = [h_{1}, h_{2},\u0026hellip;,h_{n}]^{T} \\in \\mathbb{R}^{n\\times1} $ is a hidden state with $n$ dimensions, and each $h_{i} \\in H$ is the value of a specific dimension $i$. $W_{enc} \\in \\mathbb{R}^{m\\times n}$ maps the hidden state to a new vector space with dimension $m\u0026gt;n$, $W_{dec} \\in \\mathbb{R}^{n\\times m}$ maps the sparse features back to the original shape, $ b_{enc} \\in \\mathbb{R}^{n} $ and $ b_{dec} \\in \\mathbb{R}^{n} $ are learned bias. The loss function consists of two parts: the MSE loss as the reconstruction loss and L1 norm with a coefficient $\\lambda$ to encourage the sparsity of feature activations. It is the regularization term that separates SAE from ordinary autoencoders, so as to discourage superposition and encourage monosemanticity.\nTo better understanding the encoder and decoder in SAE, we can write a sparse feature $ f_{i} $ as an element of $ z $ :\n$$ f_{i}(h) = z_{i} = ReLU(W^{enc}_{i,.}\\cdot h + b^{enc}_{i}) $$\nEach sparse feature $ f_{i} $ is calculated using row $i$ of the encoder weight matrix. As for decoder, we can write $ h^{\\prime} $ as:\n$$ h^{\\prime} = \\sum_{i=1}^{m}f_{i}(h) \\cdot W^{dec}_{.,i} + b_{dec} $$\n;The reconstructed activation $h^\\prime$ can been seen as a linear addition of all the features. Each column of the decoder matrix corresponds to a feature, so we call it a \u0026ldquo;feature direction\u0026rdquo;. Note that sometimes the L1 norm term in the loss function can be replaced by $ \\lambda\\sum_{i=1}^{m}f_{i}(h)||W^{dec}_{.,i}||_{2} $ which places a constraint to the decoder weights to reduce ambiguity in the addition operation (we want only one or a few features to be large).\nFor simplicity, The details of the model structure, training method and evaluation will not be shown here.\n\u0026#x1f52d; Recommended papers: Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Model Steering\nA useful technique for eliciting certain model behaviors in a mechanistic way.\n\u0026#x1f52d; Recommended papers: Activation Addition: Steering Language Models Without Optimization (lesswrong) Steering GPT-2-XL by adding an activation vector Steering Llama 2 via Contrastive Activation Addition Mechanistically Eliciting Latent Behaviors in Language Models Research Areas Figure 7: The route of mech interp (from transformer-circuits.pub/2024/july-update/)\rTheory\nUnderstand model components Understand model behaviors Application\ninterpretable model structure AI alignment\nAvoid bias and harmful behaviors Some Useful Resources Here I list some resources that would be helpful for you to get started quickly in the field.\nTutorials Arena A tutorial created and maintained by Callum McDougall et al, providing a guided path for anyone who finds themselves overwhelmed by the amount of technical AI safety content out there. Neel Nanda\u0026rsquo;s Tutorial Neel\u0026rsquo;s tutorial for mech interp. Neel Nanda\u0026rsquo;s Quickstart Guide A quick start for mech interp. Neel Nanda\u0026rsquo;s remommended papers Some classic and important papers for mech interp. Neel Nanda\u0026rsquo;s problems v1 Neel\u0026rsquo;s old questions for mech interp. Neel Nanda\u0026rsquo;s problems v2 Neel\u0026rsquo;s 200 new questions for mech interp. Alignment Research Field Guide (by the MIRI team) Frameworks and Libraries TransformerLens A library maintained by Bryce Meyer and created by Neel Nanda. SAELens Originates from TransformerLens, and is separated from it because of the popularity and importance of SAE. CircuitsVis A good tool for visualizing LLMs. Plotly A good tool for plotting. Forums and Communities Transformer Circuits Thread The research posts of Anthropic alignment group. Lesswrong AI Alignment Forum Companies, Institutes, Labs and Programs Anthropic DeepMind FAR Apollo RedWood CHAI (UC Berkeley) MIRI (UC Berkeley) Alignment Research Center (ARC) MATS The ML Alignment \u0026amp; Theory Scholars, an independent research and educational seminar program that connects talented scholars with top mentors in the fields of AI alignment, interpretability, and governance. SPAR Supervised Program for Alignment Research Blogs Chris Olah Neel Nanda * Neel Nanda at the Alignment Forum\nArthur Conmy Andy Zou Jacob Steinhardt David Bau Max Tegmark Trenton Bricken Callum Mcdougall Alex Turner(TurnTrout) \u0026hellip;\n","permalink":"http://localhost:1313/posts/mech_interp_resource/","summary":"The purpose I write this blog Mechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challenges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc).","title":"A Brief Introduction to Mechanistic Interpretability Research"},{"content":"","permalink":"http://localhost:1313/posts/my-first-post/","summary":"","title":"My First Post"},{"content":"Who am I? Hi~ I\u0026rsquo;m Yueyan Li, a researcher(still a student now) in China. I\u0026rsquo;m now at the Center of Intelligence Science and Technology, BUPT. My research focuses on machine learning, deep learning and natural language processing, mainly interpretability for neural networks and cognitive science now! Here is my CV.\nExcept from my research area, I\u0026rsquo;m also interested in communication engineering\u0026#x1f4fb; which was my major when I was an undergraduate. If you like that, feel free to share something interesting together~\nApart from technologies, I\u0026rsquo;m a lover for nature. I like the mountains, the rivers, the forests\u0026hellip;if you like hiking outdoors, don\u0026rsquo;t forget me! Also, I\u0026rsquo;m a Bboy\u0026#x270c;\u0026#xfe0f;\u0026#x1f918;. If you like breaking or any kind of street dance, just call me\u0026#x1f44b;!\nSometimes I paint as a waste of time, though I\u0026rsquo;m not professional.\nBelow are some of my interests. If you are interested in some of them, please reach me at any time~\nmachine learning, deep learning communication engineering street dance music (Buyi Mao, Eason, Huazhou, Shen Zhou / Avicii, Coldplay, / \u0026hellip;) Linguistics and Languages hiking, mountain climbing\u0026hellip; Street fitness (push-ups, muscle-ups\u0026hellip;) \u0026hellip; About my nickname I use the name Sirius/Sirius Jr./siriuslala\u0026hellip; everywhere on the Internet. This originates from Sirius Black - my favourite character in the Harry Potter series.\nHe is not only an extraodinary wizard with wild and intractable appearence but also the godfather of Harry Potter - the only family alive for Harry who had brought him warmth that is hard to replace. That\u0026rsquo;s why I admire him.\n","permalink":"http://localhost:1313/about/","summary":"About myself","title":"About Myself"},{"content":"\rPrevious\rNext \u0026nbsp; \u0026nbsp;\r/ [pdf]\rView the PDF file here.\r","permalink":"http://localhost:1313/pdfs/circuit_tuning/","summary":"\rPrevious\rNext \u0026nbsp; \u0026nbsp;\r/ [pdf]\rView the PDF file here.\r","title":"Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks"},{"content":"","permalink":"http://localhost:1313/faq/","summary":"faq","title":"faq"},{"content":"\rPrevious\rNext \u0026nbsp; \u0026nbsp;\r/ [pdf]\rView the PDF file here.\r","permalink":"http://localhost:1313/pdfs/helper/","summary":"\rPrevious\rNext \u0026nbsp; \u0026nbsp;\r/ [pdf]\rView the PDF file here.\r","title":"Yueyan Li"}]