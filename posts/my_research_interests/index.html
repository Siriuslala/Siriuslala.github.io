<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>My research interests | Siriuslala's Blog!</title>
<meta name=keywords content><meta name=description content="mechanistic interpretability Computational Linguistics Circuit-tuning: A Mechanistic Approach for Understanding Instrinsic Dimension and Fine-tuning Neural Networks 袁老师建议 理论 重新审视可解释性，如何给出一个漂亮的解释（思考mech interp局限性，是否可以突破一下） 上"><meta name=author content="Me"><link rel=canonical href=http://localhost:1313/posts/my_research_interests/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.187da127e3564f8ca7db95950f88b9ebc2d4f26e4c83ebf6b5baafee59ff4ebd.css integrity="sha256-GH2hJ+NWT4yn25WVD4i568LU8m5Mg+v2tbqv7ln/Tr0=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/pig.svg><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/pig.svg><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/pig.svg><link rel=apple-touch-icon href=http://localhost:1313/pig.svg><link rel=mask-icon href=http://localhost:1313/pig.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/my_research_interests/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}.formula{width:100%;overflow-x:auto}</style><meta property="og:title" content="My research interests"><meta property="og:description" content="mechanistic interpretability Computational Linguistics Circuit-tuning: A Mechanistic Approach for Understanding Instrinsic Dimension and Fine-tuning Neural Networks 袁老师建议 理论 重新审视可解释性，如何给出一个漂亮的解释（思考mech interp局限性，是否可以突破一下） 上"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/my_research_interests/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-27T17:07:06+08:00"><meta property="article:modified_time" content="2024-09-27T17:07:06+08:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="My research interests"><meta name=twitter:description content="mechanistic interpretability Computational Linguistics Circuit-tuning: A Mechanistic Approach for Understanding Instrinsic Dimension and Fine-tuning Neural Networks 袁老师建议 理论 重新审视可解释性，如何给出一个漂亮的解释（思考mech interp局限性，是否可以突破一下） 上"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"My research interests","item":"http://localhost:1313/posts/my_research_interests/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"My research interests","name":"My research interests","description":"mechanistic interpretability Computational Linguistics Circuit-tuning: A Mechanistic Approach for Understanding Instrinsic Dimension and Fine-tuning Neural Networks 袁老师建议 理论 重新审视可解释性，如何给出一个漂亮的解释（思考mech interp局限性，是否可以突破一下） 上","keywords":[],"articleBody":"mechanistic interpretability Computational Linguistics Circuit-tuning: A Mechanistic Approach for Understanding Instrinsic Dimension and Fine-tuning Neural Networks 袁老师建议\n理论 重新审视可解释性，如何给出一个漂亮的解释（思考mech interp局限性，是否可以突破一下） 上手实验，实践指导理论 实验 实验数据 因为目标不是提性能，所以可以设置一些独特的评价体系和指标，突出可解释性 因果推断的数据（hzy, xsy） 为什么做这个？\n现有问题/motivation 全量微调耗费计算资源，对于某些特定领域的任务，只有一部分参数需要微调； 缺乏对微调机理的探究，不知道在微调的过程中发生了什么，不知道微调哪些参数，导致出现很多问题，如灾难性遗忘等； intrinsic dimension的概念在LoRA中得到印证，而mech interp领域的ciruit discovery也有类似的想法，在做法上与LoRA也有共通之处 方法 机理可解释性 贡献 从mech interp的角度入手，提供一种理解intrinsic dimension的新方法； 分析circuit discovery与AdaLoRA在特征选取上殊途同归 提出circuit-tuning，先剪枝(pruning)后微调，作为一种PEFT的新方法，并通过实验证明有效性； 整体逻辑 从mech interp的角度定义intrinsic rank（一种猜想） 由于LoRA是应用intrinsic dimension这一概念进行实践的典型案例，所以从mech interp的角度解释LoRA，并将LoRA与circuit discovery作类比，论证共通之处，最终证明最初猜想的合理性 在理论猜想的指导下，实现circuit-tuning，从而反向论证猜想的合理性 前置（背景）\nmech interp neuron -\u003e feature circuit circuit discovery Finetuning full PEFT Lora, AdaLora mask操作的定义 intrinsic dimension intrinsic dimension 人脑中不同区域分管不同功能 The Lottery Ticket Hypothesis hebbian learning … 理论 Provide an understanding of intrinsic dimension using mech interp\n猜想：Given a weight matrix $W$ and a circuit $C$ with a granularity at the neuron level, the intrinsic dimenison of $W$ is equivalent to the number of nodes in $C$ which exist in the vector space projected by $W$.\n讨论 定性分析：circuit和AdaLoRA都是试图寻找神经网络中任务相关的部分 比较共同点 表征角度 AdaLoRA动态选取rank，说明每一层负责某一task的components数量不一样 circuit discovery说明只有一部分组件对某一任务有影响； 计算角度 linear addition 假设\nLinear feature hypothesis: “Let’s call a neural network representation linear if features correspond to directions in activation space.” – toy model 两个原则：Composition as Addition \u0026 Intensity as Scaling – July Updates U空间中正交基集合$e$的定义 假设表征空间维度为m，则$e = {e_{1}, e_{2}, …, e_{m} d}$为一组相互正交的基底 正交基并不一定与m维空间的neuron一一对应，可以是neuron所表示的坐标系旋转后的结果 All features represented in a model is $\\mathcal{F}$. Given a task $T={x_{1}, x_{2}, …, x_{t}}$ which consists of $t$ samples that follows a specific data distribution $\\mathcal{D_{T}}$, the features of $T$ is $\\mathcal{F_{T}}\\subsetneqq \\mathcal{F}$. W特征提取: consider the function of $W$ as feature extraction 输入为$x\\in T$, 则 $$\\Delta Wx = U\\Sigma Vx = \\sum_{i=1}^{r}u_{i}\\sigma_{i}v_{i}x = \\sum_{i=1}^{r}(\\sigma_{i}v_{i}x)\\cdot u_{i} = \\sum_{i=1}^{r}a_{i}u_{i} $$ 我们称$a_{i}$为$x$的特征在表征空间内$e_{i}$方向上的强度。 假设输入$x$所含的特征$F_{x} = f_{1}^{x},f_{2}^{x}, …, f_{t}^{x} \\subseteq F_{T}$对应的特征方向为$ V_{x} = { v_{f_{1}}^{x}, v_{f_{2}}^{x}, …, v_{f_{t}}^{x} } $, 且特征的激活值为$ A_{x} = { a_{f_{1}}^{x}, a_{f_{2}}^{x}, …, a_{f_{t}}^{x} } $, 则表征空间内$e_{i}$方向上的强度$a_{i}$可以表示为： $$ a_{i} = (\\sum_{i=1}^{t} a_{f_{i}}^{x}v_{f_{i}}^{x} ) \\cdot e_{i} $$ intrinsic dimension exists in a model when it comes to a specific task. 定理/猜想/命题/定义\nI (命题？)（特征选择 feature selection）: By performing singular value decomposition $\\Delta W = U\\Sigma V = \\sum_{i=1}^{r}u_{i}\\sigma_{i}v_{i}$ on weight matrix $\\Delta W$, feature selection is feasible with the optimization of the orthogonal basis in $U,V$ and the mask of singular value $\\sigma_{i}$.\n解释/证明 假设特征$f$所需正交基集合为$e_{f} = { e_{f_{1}}, e_{f_{2}}, …, e_{f_{k}} k\\leq m } $是$m$维表征空间的基底集合$e$的子集,，即特征向量$v_{f}=c_{1}e_{f_{1}}+c_{2}e_{f_{2}}+…+c_{k}e_{f_{k}}$ ($c_{i}$为系数且$|ci|≤1$)。若$e_{f}$对应的奇异值集合$\\sigma_{f}={\\sigma_{f_{1}}, \\sigma_{f_{2}}, …, \\sigma_{f_{k}} $内的某些元素被mask，且mask后的特征方向为$\\tilde{v_{f}}$，我们可以用余弦相似度定义mask操作前后特征的变形程度： $$ cosine_sim = \\frac{v_{f}\\dot \\tilde{v_{f}}}{|v_{f}||\\tilde{v_{f}}|} $$ 给定一个阈值s, 若$cosine_sim \\lt s$，则该特征被丢弃，反之则被选择。 分析 奇异值不是直接选取特征，而是选取表征空间里的正交基。 特征选择的结果是 维度冗余: there is redundancy in the dimensions of the latent space $\\mathbb{R}^{D}(D=m)$ where the features lie. II（定义 intrinsic dimension）: For any weight matrix $W \\in \\mathbb{R}^{m\\times n}$ in model $M$ and input data $x\\in T$, we can get the representation $H={h_{1}, h_{2}, …, h_{m}} \\in \\mathbb{R}^{D}(D=m)$ of $x$ by multiplying $x$ with $W$. Given a threshold $\\tau \\gt 0$, if the absolute value $|h_{i}| \\in \\mathbb{R}$ is lower than $\\tau$, then we regard the $i$-th dimension as redundant. If the dimension redundancy in $H$ is $d_{r}$, then we define the intrinsic dimension as $D-d_{r}$.\n特别的，when the dimension $D$ of latent space $\\mathbb{R}^{D}$ satisfiies $D \\geq |\\mathcal{F_{t}}| $, we can suppose that there is no polysemanticity in $H$. In this situation, the number of features needed is equal to the number of non-zero singular values which is actually the rank of $W$.\n理论的限制范围——输入服从某一种数据分布\nintrinsic dimension最初的定义是解空间的余维度？？？(只是知乎的说法，还要根据原文判断) 若从superposition的角度考虑：neuron的多语义是因为预训练数据范围较广，神经网络是用来拟合\"世界模型\"，故对于表征整个世界来说，m维是不够的； 但对于具体任务，涉及到的特征有限，所以在微调过程中m维是冗余的；具体体现在更新量$\\Delta W$中； 参考\nLORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS ADALORA: ADAPTIVE BUDGET ALLOCATION FOR PARAMETER-EFFICIENT FINE-TUNING PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning Sparse Low-rank Adaptation of Pre-trained Language Models III（目标等价 objective equivalence）: Selecting singular values based on importance scores in AdaLoRA is equivalent to selecting nodes based on indirect effect in circuit discovery. (除了AdaLoRA，是否还有其他的指标)\n感性理解：重要性分数的思想来源于剪枝，而circuit discovery也在做剪枝的事情\n证明\n指标衡量的是：contribution of a node $n$ to the model’s behavior 重要性分数 参考 **Importance estimation for neural network pruning importance score的一般形式，考虑了参数敏感性 Super tickets in pre-trained language models: From model compression to improving generalization transformer里应用importance score Are sixteen heads really better than one? Platon: Pruning large transformer models with upper confidence bound of weight importance importance score的改进：考虑到IS计算结果的不稳定性(AdaLoRA里的滑动平均操作？) **Movement pruning: Adaptive sparsity by fine-tuning 在微调的时候观察参数变化，从而进行剪枝（微调服务于剪枝） 本文是：在剪枝的过程中微调（剪枝服务于微调，剪枝是为了降低微调的计算量） indirect effect 因果推断里的定义： $IE$在activation patching中的具体形式： $$ IE(n; x_{clean}, x_{noise}) = \\mathcal{L_{m}}(\\mathcal{M}(x_{clean}-do(n\\leftarrow n(x_{noise})))) - \\mathcal{L_{m}}(\\mathcal{M}(x_{clean})) $$ importance score考虑到了参数敏感性，且在推导时用到了泰勒展开；indirect effect在attribution patching里也用到了泰勒展开 $W^{\\prime}x = (W + \\Delta W)x$ $$ Neuron_{j} = \\sum_{i=1}^{r}u_{ij}\\sigma_{i}v_{i}x$$ IS of a parameter $w$: $$I(w) = |w\\nabla_{w}\\mathcal{L}|$$ where $L$ is the loss function(e.g. next token prediction loss). According to this paper, the form of $I$ is actually a approximation of the intervention effect on $w$ when pruning neural networks. Specifically speaking, we can perform sensitivity analysis on a parameter based on the difference in loss induced by removing it. The difference in loss $diff$ can be written as $$|E(D, W)-E(D, W|w=0)|$$ where $E$ is the fitting error, $D$ is the training dataset, $W$ represents the parameters in a model and $w=0$ means removing $w$ by replacing its value with zero. Directly computing $diff$ is inefficient, so […] et al regard $diff$ as a function of $w$ and simplify it by making a first-order Taylor expansion at $w=w_{origin}$, where $w_{origin}$ is the original value of $w$. So it comes to the form of importance score $I$, and $I$ is essentially a simpilified form of indirect effect.\nSimilar to the importance score, such technique is also used in activation patching. […] et al purposed attribution which applies a first-order Taylor expansion to $IE$ at $n = n(x_{clean})$. The $IE$ in attribution patching can be written as: $$ IE(n; x_{clean}, x_{noise}) \\approx \\mathcal{L_{m}}(\\mathcal{M}(x_{clean})) + (n_{noise}-n_{clean})^{T} \\frac{\\partial \\mathcal{L_{m}}(\\mathcal{M}(x_{clean}))}{\\partial n} - \\mathcal{L_{m}}(\\mathcal{M}(x_{clean})) = (n_{noise}-n_{clean})^{T} \\frac{\\partial \\mathcal{L_{m}}(\\mathcal{M}(x_{clean}))}{\\partial n} $$ When we apply zero ablation which set $n_{noise}$ to zero, equation () takes almost the same form as equation (). Thus, when we inspect in the importance of a neuron in $\\mathbb{R}^{m}$, we can divide it into the importances of singular values in $W \\in \\mathbb{R}^{m \\times n}$ since we can split the value of a neuron into terms including singular values (see). 参考\nINTRINSIC DIMENSIONALITY EXPLAINS THE EFFECTIVENESS OF LANGUAGE MODEL FINE-TUNING MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES 讨论\n不严格等价：与circuit中components的细粒度，特征选取的指标(I, IE)，更新策略都有关系 其他理论分析\nlora A Kernel-Based View of Language Model Fine-Tuning The Impact of LoRA on the Emergence of Clusters in Transformers LoRA Training in the NTK Regime has No Spurious Local Minima Asymmetry in low rank adapters of foundation models The expressive power of low-rank adaptation feature learning Neural Networks can Learn Representations with Gradient Descent 算法\n算法设计\n算法论述\n重要性分数的平滑 $$IE(n)^{(t)} = E_{X^{(t)}}(IE(n, x, x_{noise}))$$ $$IE(n)^{*(t)} = \\overline{IE(n)}^{(t)} \\cdot \\overline{U(n)}^{(t)}$$ $$\\overline{IE(n)}^{(t)} = \\beta_{1}IE(n)^{(t-1)} + (1-\\beta_{1})IE(n)^{(t)}$$ $$\\overline{U(n)}^{(t)} = \\beta_{2}U(n)^{(t-1)} + (1-\\beta_{2})|IE(n)^{(t)} - \\overline{IE(n)}^{(t)}|$$ “随机激活” 细节\n**aggregation method 简单任务（如主谓不一致）：patch activation后，用logit difference/log prob difference John has a pencil John have a pencil 复杂任务（如数学）：整段话输进去，先算IE在一句话中的平均值，再算在样本间的均值 weighted aggregation circuit剪枝的方法 是大于$\\tau$就保留，还是按top_b？ tau如何设置？事先在数据集上求平均？ 是否需要设置类似于budget scheduler的东?(AdaLoRA逐步增加top_b，也就是逐渐增加增量$\\delta W$) **滑动计算IE：抵抗方差，算是一种对circuit discovery的优化 activation patching or path patching, or both?(参考 sparse features circuits, AtP*) soft mask(“随机激活”)? 就是C以外的components随机选取进行更新（参考：SoftNet） 正则化？（灾难性遗忘） MOE routing? 算法流程\nInput: $\\mathcal{D}$: dataset, $\\mathcal{M}$: model, $G$: the computing graph of $\\mathbb{M}$, $\\mathcal{L_{m}}$: metric for measuring indirect effect, $\\tau_{n}$: threshold for nodes in circuit discovery, $\\tau_{e}$: threshold for edges in circuit discovery, T: total number of training steps Output: Fine-tuned model $\\mathcal{M^{\\prime}}$ Set circuit $C \\leftarrow G$\nfor $i=1$ to $T$ do\nsample a mini-batch $X={x_{1}, x_{2}, …, x_{n}} \\sim \\mathcal{D}$\n// circuit discovery\n// patch nodes\nfor node $n \\in C$ do\nif $E_{X}(IE(n, x, x_{noise})) \\lt \\tau_{n}$ then\n$C \\leftarrow C \\backslash n$\nend\nend\n// patch edges\nfor edge $e \\in C$ do\nif $E_{X}(IE(e, x, x_{noise})) \\lt \\tau_{e}$ then\n$C \\leftarrow C \\backslash e$\nend\nend\n//edge tuning\nfor edge $e \\in C$ do\nGet parameter $w$ correspond to $e$\nUpdate $w = w - \\eta \\nabla_{w}\\mathcal{L}$\nend\nReset circuit $C \\leftarrow G$\nend\n参考\n(ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability (attribution patching) Attribution Patching Outperforms Automated Circuit Discovery Attribution patching: Activation patching at industrial scale AtP*: An efficient and scalable method for localizing llm behaviour to components How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model Linear Representations of sentiment in large language models 收敛性证明（可选）\nLipschitz continuity（参考Forget-free Continual Learning with Soft-Winning SubNetworks） circuit-tuning的特性\n缓解 catastrophic forgetting，interpretable 现有方法 memory replay regularization **parameter isolation 理论上解释如何缓解 参考 微调 Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks Understanding Catastrophic Forgetting in Language Models via Implicit Inference Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking lora *(O-LoRA) Orthogonal Subspace Learning for Language Model Continual Learning *(I-LoRA) Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning InfLoRA GSLoRA 持续学习 Recent Advances of Foundation Language Models-based Continual Learning: A Survey **A Comprehensive Survey of Continual Learning: Theory, Method and Application Brain-inspired learning in artificial neural networks: a review 区别与联系\n与LoRA/AdaLoRA有什么区别？ lora不好对W奇异值分解，所以对增量进行微调，而circuit-tuning直接微调原参数 lora选奇异值的最终结果就是选neuron，也就是说lora直接从参数入手，而circuit-tuning先从mech interp的角度发现circuit，找到edge对应的参数，然后微调 与剪枝相比 相似 用到了lottery ticket的思想 区别 剪枝是模型已经有了某个能力，然后剔除与该能力无关的结构（是这样？），而circuit-tuning是在没有该能力的情况下一步步探索 与continual learning中parameter isolation方法的区别 相似 over-parameterized的假设与intrinsic dimension类似 区别 circuit-tuning可扩展到大于neuron的level，而continual learning大多是neural level circuit-tuning从mech interp的角度考虑，本身是一种微调方法，同时附带了缓解灾难性遗忘的功能 circuit-tuning剪枝的指标是IE；而持续学习如CLNP是计算平均激活值，激活值小会被剪掉，或是SoftNet引入了可学习的weight score，根据weight score设置mask continual learining的场景主要是按顺序进行的t个任务，而circuit-tuning则是提供一种微调的思路 参考（相似成果） (CLNP) **Continual learning via neural pruning Forget-free Continual Learning with Soft-Winning SubNetworks 实验设计\ndataset task for small model IOI subject-verb “disagreement” gender bias for LLM math, instruction following, code, language transfer, … others safety gender bias(见SAE circuits) model small model v.s. LLM small: Pythia, GPT-2, Gemma, Phi Pythia: rotary embeddingss large: Mistral, Llama circuit-tuning split granularity: nodes(neurons or heads? 详见SAE circuits, AtP*) ablation methods(zero? average? ) $L$选哪个？logit difference / log prob 是下一个token还是什么？(aggregation method) IE选哪个？标准activation patching/AtP/AtP*/ig threshld evaluation SAE? evaluation 简单任务 主谓不一致相关指标： circuit相关指标：faithfulness, completeness, …（见sfc） 复杂任务 analyses 主谓一致-\u003e主谓不一致 circuit的变化 LoRA和circuit-tuning相互验证 application model steering 优点和不足\n优点 参数高效 可解释，更精准 缓解灾难性遗忘 不足 需要先找到circuit 计算量大 不像LoRA一样可插拔 难以scale? linguistics 语言习得，二语习得 迁移学习 ","wordCount":"4723","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-09-27T17:07:06+08:00","dateModified":"2024-09-27T17:07:06+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/my_research_interests/"},"publisher":{"@type":"Organization","name":"Siriuslala's Blog!","logo":{"@type":"ImageObject","url":"http://localhost:1313/pig.svg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/pig.svg alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/about/ title=About><span>About</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/faq/ title=FAQ><span>FAQ</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">My research interests
<span class=entry-hint title=Draft><svg height="35" viewBox="0 -960 960 960" fill="currentcolor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h1><div class=post-meta><span title='2024-09-27 17:07:06 +0800 CST'>September 27, 2024</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;4723 words&nbsp;·&nbsp;Me</div></header><div class=post-content><h2 id=mechanistic-interpretability>mechanistic interpretability<a hidden class=anchor aria-hidden=true href=#mechanistic-interpretability>#</a></h2><h3 id=computational-linguistics>Computational Linguistics<a hidden class=anchor aria-hidden=true href=#computational-linguistics>#</a></h3><ul><li>Circuit-tuning: A Mechanistic Approach for Understanding Instrinsic Dimension and Fine-tuning Neural Networks<ul><li><p>袁老师建议</p><ul><li>理论<ul><li>重新审视可解释性，如何给出一个漂亮的解释（思考mech interp局限性，是否可以突破一下）</li><li>上手实验，实践指导理论</li></ul></li><li>实验<ul><li>实验数据<ul><li>因为目标不是提性能，所以可以设置一些独特的评价体系和指标，突出可解释性</li><li>因果推断的数据（hzy, xsy）</li></ul></li></ul></li></ul></li><li><p>为什么做这个？</p><ul><li>现有问题/motivation<ul><li>全量微调耗费计算资源，对于某些特定领域的任务，只有一部分参数需要微调；</li><li>缺乏对微调机理的探究，不知道在微调的过程中发生了什么，不知道微调哪些参数，导致出现很多问题，如灾难性遗忘等；</li><li>intrinsic dimension的概念在LoRA中得到印证，而mech interp领域的ciruit discovery也有类似的想法，在做法上与LoRA也有共通之处</li></ul></li><li>方法<ul><li>机理可解释性</li></ul></li><li>贡献<ul><li>从mech interp的角度入手，提供一种理解intrinsic dimension的新方法；</li><li>分析circuit discovery与AdaLoRA在特征选取上殊途同归</li><li>提出circuit-tuning，先剪枝(pruning)后微调，作为一种PEFT的新方法，并通过实验证明有效性；</li></ul></li><li>整体逻辑<ul><li>从mech interp的角度定义intrinsic rank（一种猜想）</li><li>由于LoRA是应用intrinsic dimension这一概念进行实践的典型案例，所以从mech interp的角度解释LoRA，并将LoRA与circuit discovery作类比，论证共通之处，最终证明最初猜想的合理性</li><li>在理论猜想的指导下，实现circuit-tuning，从而反向论证猜想的合理性</li></ul></li></ul></li><li><p>前置（背景）</p><ul><li>mech interp<ul><li>neuron -> feature</li><li>circuit</li><li>circuit discovery</li></ul></li><li>Finetuning<ul><li>full</li><li>PEFT<ul><li>Lora, AdaLora<ul><li>mask操作的定义</li></ul></li></ul></li></ul></li><li>intrinsic dimension<ul><li>intrinsic dimension</li><li>人脑中不同区域分管不同功能</li><li>The Lottery Ticket Hypothesis</li><li>hebbian learning</li><li>&mldr;</li></ul></li></ul></li><li><p>理论
Provide an understanding of intrinsic dimension using mech interp</p><ul><li><p>猜想：Given a weight matrix $W$ and a circuit $C$ with a granularity at the neuron level, the intrinsic dimenison of $W$ is equivalent to the number of nodes in $C$ which exist in the vector space projected by $W$.</p><ul><li>讨论<ul><li>定性分析：circuit和AdaLoRA都是试图寻找神经网络中任务相关的部分<ul><li>比较共同点<ul><li>表征角度<ul><li>AdaLoRA动态选取rank，说明每一层负责某一task的components数量不一样</li><li>circuit discovery说明只有一部分组件对某一任务有影响；</li></ul></li><li>计算角度<ul><li>linear addition</li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>假设</p><ul><li>Linear feature hypothesis: &ldquo;Let&rsquo;s call a neural network representation linear if features correspond to directions in activation space.&rdquo; &ndash; <a href=https://transformer-circuits.pub/2022/toy_model/index.html>toy model</a><ul><li>两个原则：Composition as Addition & Intensity as Scaling &ndash; <a href=https://transformer-circuits.pub/2024/july-update/index.html#linear-representations>July Updates</a></li><li>U空间中正交基集合$e$的定义<ul><li>假设表征空间维度为m，则$e = {e_{1}, e_{2}, &mldr;, e_{m} d}$为一组相互正交的基底<ul><li>正交基并不一定与m维空间的neuron一一对应，可以是neuron所表示的坐标系旋转后的结果</li></ul></li></ul></li></ul></li><li>All features represented in a model is $\mathcal{F}$. Given a task $T={x_{1}, x_{2}, &mldr;, x_{t}}$ which consists of $t$ samples that follows a specific data distribution $\mathcal{D_{T}}$, the features of $T$ is $\mathcal{F_{T}}\subsetneqq \mathcal{F}$.</li><li>W特征提取: consider the function of $W$ as feature extraction<ul><li>输入为$x\in T$, 则 $$\Delta Wx = U\Sigma Vx = \sum_{i=1}^{r}u_{i}\sigma_{i}v_{i}x = \sum_{i=1}^{r}(\sigma_{i}v_{i}x)\cdot u_{i} = \sum_{i=1}^{r}a_{i}u_{i} $$ 我们称$a_{i}$为$x$的特征在表征空间内$e_{i}$方向上的强度。<ul><li>假设输入$x$所含的特征$F_{x} = f_{1}^{x},f_{2}^{x}, &mldr;, f_{t}^{x} \subseteq F_{T}$对应的特征方向为$ V_{x} = { v_{f_{1}}^{x}, v_{f_{2}}^{x}, &mldr;, v_{f_{t}}^{x} } $, 且特征的激活值为$ A_{x} = { a_{f_{1}}^{x}, a_{f_{2}}^{x}, &mldr;, a_{f_{t}}^{x} } $, 则表征空间内$e_{i}$方向上的强度$a_{i}$可以表示为：
$$ a_{i} = (\sum_{i=1}^{t} a_{f_{i}}^{x}v_{f_{i}}^{x} ) \cdot e_{i} $$</li></ul></li></ul></li><li>intrinsic dimension exists in a model when it comes to a specific task.</li></ul></li><li><p>定理/猜想/命题/定义</p><ul><li><p>I (命题？)（特征选择 feature selection）: By performing singular value decomposition $\Delta W = U\Sigma V = \sum_{i=1}^{r}u_{i}\sigma_{i}v_{i}$ on weight matrix $\Delta W$, feature selection is feasible with the optimization of the orthogonal basis in $U,V$ and the mask of singular value $\sigma_{i}$.</p><ul><li>解释/证明<ul><li>假设特征$f$所需正交基集合为$e_{f} = { e_{f_{1}}, e_{f_{2}}, &mldr;, e_{f_{k}} k\leq m } $是$m$维表征空间的基底集合$e$的子集,，即特征向量$v_{f}=c_{1}e_{f_{1}}+c_{2}e_{f_{2}}+&mldr;+c_{k}e_{f_{k}}$ ($c_{i}$为系数且$|ci|≤1$)。若$e_{f}$对应的奇异值集合$\sigma_{f}={\sigma_{f_{1}}, \sigma_{f_{2}}, &mldr;, \sigma_{f_{k}} $内的某些元素被mask，且mask后的特征方向为$\tilde{v_{f}}$，我们可以用余弦相似度定义mask操作前后特征的变形程度：
$$ cosine_sim = \frac{v_{f}\dot \tilde{v_{f}}}{|v_{f}||\tilde{v_{f}}|} $$
给定一个阈值s, 若$cosine_sim \lt s$，则该特征被丢弃，反之则被选择。</li></ul></li><li>分析<ul><li>奇异值不是直接选取特征，而是选取表征空间里的正交基。</li><li>特征选择的结果是 维度冗余: there is redundancy in the dimensions of the latent space $\mathbb{R}^{D}(D=m)$ where the features lie.</li></ul></li></ul></li><li><p>II（定义 intrinsic dimension）: For any weight matrix $W \in \mathbb{R}^{m\times n}$ in model $M$ and input data $x\in T$, we can get the representation $H={h_{1}, h_{2}, &mldr;, h_{m}} \in \mathbb{R}^{D}(D=m)$ of $x$ by multiplying $x$ with $W$. Given a threshold $\tau \gt 0$, if the absolute value $|h_{i}| \in \mathbb{R}$ is lower than $\tau$, then we regard the $i$-th dimension as redundant. If the dimension redundancy in $H$ is $d_{r}$, then we define the intrinsic dimension as $D-d_{r}$.</p><ul><li><p>特别的，when the dimension $D$ of latent space $\mathbb{R}^{D}$ satisfiies $D \geq |\mathcal{F_{t}}| $, we can suppose that there is no polysemanticity in $H$. In this situation, the number of features needed is equal to the number of non-zero singular values which is actually the rank of $W$.</p></li><li><p>理论的限制范围——输入服从某一种数据分布</p><ul><li>intrinsic dimension最初的定义是解空间的余维度？？？(只是知乎的说法，还要根据原文判断)</li><li>若从superposition的角度考虑：neuron的多语义是因为预训练数据范围较广，神经网络是用来拟合"世界模型"，故对于表征整个世界来说，m维是不够的；</li><li>但对于具体任务，涉及到的特征有限，所以在微调过程中m维是冗余的；具体体现在更新量$\Delta W$中；</li></ul></li><li><p>参考</p><ul><li><a href=https://arxiv.org/pdf/2106.09685>LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</a></li><li><a href=https://arxiv.org/pdf/2303.10512>ADALORA: ADAPTIVE BUDGET ALLOCATION FOR PARAMETER-EFFICIENT FINE-TUNING</a></li><li><a href=https://arxiv.org/pdf/2404.02948>PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2406.09044>MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning</a></li><li><a href=https://arxiv.org/pdf/2311.11696>Sparse Low-rank Adaptation of Pre-trained Language Models</a></li></ul></li></ul></li><li><p>III（目标等价 objective equivalence）: Selecting singular values based on importance scores in AdaLoRA is equivalent to selecting nodes based on indirect effect in circuit discovery. (除了AdaLoRA，是否还有其他的指标)</p><ul><li><p>感性理解：重要性分数的思想来源于剪枝，而circuit discovery也在做剪枝的事情</p></li><li><p>证明</p><ul><li>指标衡量的是：contribution of a node $n$ to the model&rsquo;s behavior</li><li>重要性分数<ul><li>参考<ul><li><a href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Molchanov_Importance_Estimation_for_Neural_Network_Pruning_CVPR_2019_paper.pdf>**Importance estimation for neural network pruning</a><ul><li>importance score的一般形式，考虑了参数敏感性</li></ul></li><li><a href=https://arxiv.org/pdf/2105.12002>Super tickets in pre-trained language models: From model compression to improving generalization</a><ul><li>transformer里应用importance score</li><li><a href=https://proceedings.neurips.cc/paper_files/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf>Are sixteen heads really better than one?</a></li></ul></li><li><a href=https://proceedings.mlr.press/v162/zhang22ao/zhang22ao.pdf>Platon: Pruning large transformer models with upper confidence bound of weight importance</a><ul><li>importance score的改进：考虑到IS计算结果的不稳定性(AdaLoRA里的滑动平均操作？)</li></ul></li><li><a href=https://proceedings.neurips.cc/paper_files/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf>**Movement pruning: Adaptive sparsity by fine-tuning</a><ul><li>在微调的时候观察参数变化，从而进行剪枝（微调服务于剪枝）</li><li>本文是：在剪枝的过程中微调（剪枝服务于微调，剪枝是为了降低微调的计算量）</li></ul></li></ul></li></ul></li><li>indirect effect<ul><li>因果推断里的定义：</li><li>$IE$在activation patching中的具体形式：
$$ IE(n; x_{clean}, x_{noise}) = \mathcal{L_{m}}(\mathcal{M}(x_{clean}-do(n\leftarrow n(x_{noise})))) - \mathcal{L_{m}}(\mathcal{M}(x_{clean})) $$</li></ul></li><li>importance score考虑到了参数敏感性，且在推导时用到了泰勒展开；indirect effect在attribution patching里也用到了泰勒展开<ul><li>$W^{\prime}x = (W + \Delta W)x$</li><li>$$ Neuron_{j} = \sum_{i=1}^{r}u_{ij}\sigma_{i}v_{i}x$$</li><li>IS of a parameter $w$: $$I(w) = |w\nabla_{w}\mathcal{L}|$$ where $L$ is the loss function(e.g. next token prediction loss). According to <a href=https://openaccess.thecvf.com/content_CVPR_2019/papers/Molchanov_Importance_Estimation_for_Neural_Network_Pruning_CVPR_2019_paper.pdf>this paper</a>, the form of $I$ is actually a approximation of the intervention effect on $w$ when pruning neural networks.
Specifically speaking, we can perform sensitivity analysis on a parameter based on the difference in loss induced by removing it. The difference in loss $diff$ can be written as $$|E(D, W)-E(D, W|w=0)|$$ where $E$ is the fitting error, $D$ is the training dataset, $W$ represents the parameters in a model and $w=0$ means removing $w$ by replacing its value with zero. Directly computing $diff$ is inefficient, so [&mldr;] et al regard $diff$ as a function of $w$ and simplify it by making a first-order Taylor expansion at $w=w_{origin}$, where $w_{origin}$ is the original value of $w$. So it comes to the form of importance score $I$, and $I$ is essentially a simpilified form of indirect effect.<br>Similar to the importance score, such technique is also used in activation patching. [&mldr;] et al purposed attribution which applies a first-order Taylor expansion to $IE$ at $n = n(x_{clean})$. The $IE$ in attribution patching can be written as:</li></ul>$$ IE(n; x_{clean}, x_{noise}) \approx \mathcal{L_{m}}(\mathcal{M}(x_{clean})) + (n_{noise}-n_{clean})^{T} \frac{\partial \mathcal{L_{m}}(\mathcal{M}(x_{clean}))}{\partial n} - \mathcal{L_{m}}(\mathcal{M}(x_{clean})) = (n_{noise}-n_{clean})^{T} \frac{\partial \mathcal{L_{m}}(\mathcal{M}(x_{clean}))}{\partial n} $$
When we apply zero ablation which set $n_{noise}$ to zero, equation () takes almost the same form as equation (). Thus, when we inspect in the importance of a neuron in $\mathbb{R}^{m}$, we can divide it into the importances of singular values in $W \in \mathbb{R}^{m \times n}$ since we can split the value of a neuron into terms including singular values (see).</li></ul></li><li><p>参考</p><ul><li><a href=https://arxiv.org/pdf/2012.13255>INTRINSIC DIMENSIONALITY EXPLAINS THE EFFECTIVENESS OF LANGUAGE MODEL FINE-TUNING</a></li><li><a href=https://arxiv.org/pdf/1804.08838>MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES</a></li></ul></li></ul></li></ul></li><li><p>讨论</p><ul><li>不严格等价：与circuit中components的细粒度，特征选取的指标(I, IE)，更新策略都有关系</li></ul></li><li><p>其他理论分析</p><ul><li>lora<ul><li><a href=https://proceedings.mlr.press/v202/malladi23a/malladi23a.pdf>A Kernel-Based View of Language Model Fine-Tuning</a></li><li><a href=https://arxiv.org/pdf/2402.15415>The Impact of LoRA on the Emergence of Clusters in Transformers</a></li><li><a href=https://arxiv.org/pdf/2402.11867>LoRA Training in the NTK Regime has No Spurious Local Minima</a></li><li><a href=https://arxiv.org/pdf/2402.16842>Asymmetry in low rank adapters of foundation models</a></li><li><a href=https://arxiv.org/pdf/2310.17513>The expressive power of low-rank adaptation</a></li></ul></li><li>feature learning<ul><li><a href=https://arxiv.org/pdf/2206.15144>Neural Networks can Learn Representations with Gradient Descent</a></li></ul></li></ul></li></ul></li><li><p>算法</p><ul><li><p>算法设计</p><ul><li><p>算法论述</p><ul><li>重要性分数的平滑
$$IE(n)^{(t)} = E_{X^{(t)}}(IE(n, x, x_{noise}))$$
$$IE(n)^{*(t)} = \overline{IE(n)}^{(t)} \cdot \overline{U(n)}^{(t)}$$
$$\overline{IE(n)}^{(t)} = \beta_{1}IE(n)^{(t-1)} + (1-\beta_{1})IE(n)^{(t)}$$
$$\overline{U(n)}^{(t)} = \beta_{2}U(n)^{(t-1)} + (1-\beta_{2})|IE(n)^{(t)} - \overline{IE(n)}^{(t)}|$$</li><li>&ldquo;随机激活&rdquo;</li></ul></li><li><p>细节</p><ul><li>**aggregation method<ul><li>简单任务（如主谓不一致）：patch activation后，用logit difference/log prob difference<ul><li>John <strong>has</strong> a pencil</li><li>John <strong>have</strong> a pencil</li></ul></li><li>复杂任务（如数学）：整段话输进去，先算IE在一句话中的平均值，再算在样本间的均值<ul><li>weighted aggregation</li></ul></li></ul></li><li>circuit剪枝的方法<ul><li>是大于$\tau$就保留，还是按top_b？</li><li>tau如何设置？事先在数据集上求平均？</li><li>是否需要设置类似于budget scheduler的东?(AdaLoRA逐步增加top_b，也就是逐渐增加增量$\delta W$)</li></ul></li><li>**滑动计算IE：抵抗方差，算是一种对circuit discovery的优化</li><li>activation patching or path patching, or both?(参考 sparse features circuits, AtP*)</li><li>soft mask(&ldquo;随机激活&rdquo;)? 就是C以外的components随机选取进行更新（参考：<a href=https://arxiv.org/pdf/2303.14962>SoftNet</a>）</li><li>正则化？（灾难性遗忘）</li><li>MOE routing?</li></ul></li><li><p>算法流程<br><strong>Input</strong>: $\mathcal{D}$: dataset, $\mathcal{M}$: model, $G$: the computing graph of $\mathbb{M}$, $\mathcal{L_{m}}$: metric for measuring indirect effect, $\tau_{n}$: threshold for nodes in circuit discovery, $\tau_{e}$: threshold for edges in circuit discovery, T: total number of training steps<br><strong>Output</strong>: Fine-tuned model $\mathcal{M^{\prime}}$<br>Set circuit $C \leftarrow G$<br><strong>for</strong> $i=1$ to $T$ <strong>do</strong><br>  sample a mini-batch $X={x_{1}, x_{2}, &mldr;, x_{n}} \sim \mathcal{D}$</p><p>  // circuit discovery<br>  // patch nodes<br>  <strong>for</strong> node $n \in C$ <strong>do</strong><br>    <strong>if</strong> $E_{X}(IE(n, x, x_{noise})) \lt \tau_{n}$ <strong>then</strong><br>      $C \leftarrow C \backslash n$<br>    <strong>end</strong><br>  <strong>end</strong><br>  // patch edges<br>  <strong>for</strong> edge $e \in C$ <strong>do</strong><br>    <strong>if</strong> $E_{X}(IE(e, x, x_{noise})) \lt \tau_{e}$ <strong>then</strong><br>      $C \leftarrow C \backslash e$<br>    <strong>end</strong><br>  <strong>end</strong></p><p>  //edge tuning<br>  <strong>for</strong> edge $e \in C$ <strong>do</strong><br>    Get parameter $w$ correspond to $e$<br>    Update $w = w - \eta \nabla_{w}\mathcal{L}$<br>  <strong>end</strong></p><p>  Reset circuit $C \leftarrow G$<br><strong>end</strong></p></li><li><p>参考</p><ul><li><a href=https://arxiv.org/abs/2304.14997>(ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability</a></li><li><a href=https://arxiv.org/abs/2310.10348>(attribution patching) Attribution Patching Outperforms Automated Circuit Discovery</a></li><li><a href=https://www.neelnanda.io/mechanistic-interpretability/attribution-patching>Attribution patching: Activation patching at industrial scale</a></li><li><a href=https://arxiv.org/pdf/2403.00745>AtP*: An efficient and scalable method for localizing llm behaviour to components</a></li><li><a href=https://arxiv.org/pdf/2305.00586>How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model</a></li><li><a href=https://arxiv.org/abs/2310.15154>Linear Representations of sentiment in large language models</a></li></ul></li></ul></li><li><p>收敛性证明（可选）</p><ul><li>Lipschitz continuity（参考<a href=https://arxiv.org/pdf/2303.14962>Forget-free Continual Learning with Soft-Winning SubNetworks</a>）</li></ul></li><li><p>circuit-tuning的特性</p><ul><li>缓解 catastrophic forgetting，interpretable<ul><li>现有方法<ul><li>memory replay</li><li>regularization</li><li>**parameter isolation</li></ul></li><li>理论上解释如何缓解</li></ul></li><li>参考<ul><li>微调<ul><li><a href=https://arxiv.org/pdf/2311.12786>Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks</a></li><li><a href=https://arxiv.org/pdf/2309.10105>Understanding Catastrophic Forgetting in Language Models via Implicit Inference</a></li><li><a href=https://arxiv.org/pdf/2402.14811>Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking</a></li></ul></li><li>lora<ul><li>*<a href=https://arxiv.org/pdf/2310.14152>(O-LoRA) Orthogonal Subspace Learning for Language Model Continual Learning</a></li><li>*<a href=https://arxiv.org/pdf/2402.18865>(I-LoRA) Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning</a></li><li><a href>InfLoRA</a></li><li><a href>GSLoRA</a></li></ul></li><li>持续学习<ul><li><a href=https://arxiv.org/pdf/2405.18653>Recent Advances of Foundation Language Models-based Continual Learning: A Survey</a></li><li><a href=https://arxiv.org/pdf/2302.00487>**A Comprehensive Survey of Continual Learning: Theory, Method and Application</a></li><li><a href=https://arxiv.org/pdf/2305.11252>Brain-inspired learning in artificial neural networks: a review</a></li></ul></li></ul></li></ul></li><li><p>区别与联系</p><ul><li>与LoRA/AdaLoRA有什么区别？<ul><li>lora不好对W奇异值分解，所以对增量进行微调，而circuit-tuning直接微调原参数</li><li>lora选奇异值的最终结果就是选neuron，也就是说lora直接从参数入手，而circuit-tuning先从mech interp的角度发现circuit，找到edge对应的参数，然后微调</li></ul></li><li>与剪枝相比<ul><li>相似<ul><li>用到了lottery ticket的思想</li></ul></li><li>区别<ul><li>剪枝是模型已经有了某个能力，然后剔除与该能力无关的结构（是这样？），而circuit-tuning是在没有该能力的情况下一步步探索</li></ul></li></ul></li><li>与continual learning中parameter isolation方法的区别<ul><li>相似<ul><li>over-parameterized的假设与intrinsic dimension类似</li></ul></li><li>区别<ul><li>circuit-tuning可扩展到大于neuron的level，而continual learning大多是neural level</li><li>circuit-tuning从mech interp的角度考虑，本身是一种微调方法，同时附带了缓解灾难性遗忘的功能</li><li>circuit-tuning剪枝的指标是IE；而持续学习如CLNP是计算平均激活值，激活值小会被剪掉，或是SoftNet引入了可学习的weight score，根据weight score设置mask</li><li>continual learining的场景主要是按顺序进行的t个任务，而circuit-tuning则是提供一种微调的思路</li></ul></li><li>参考（相似成果）<ul><li><a href=https://arxiv.org/pdf/1903.04476>(CLNP) **Continual learning via neural pruning</a></li><li><a href=https://arxiv.org/pdf/2303.14962>Forget-free Continual Learning with Soft-Winning SubNetworks</a></li></ul></li></ul></li></ul></li></ul></li><li><p>实验设计</p><ul><li>dataset<ul><li>task<ul><li>for small model<ul><li>IOI</li><li>subject-verb &ldquo;disagreement&rdquo;</li><li>gender bias</li></ul></li><li>for LLM<ul><li>math, instruction following, code, language transfer, &mldr;</li></ul></li><li>others<ul><li>safety</li><li>gender bias(见SAE circuits)</li></ul></li></ul></li></ul></li><li>model<ul><li>small model v.s. LLM<ul><li>small: Pythia, GPT-2, Gemma, Phi<ul><li><a href=https://arxiv.org/pdf/2304.01373>Pythia</a>: rotary embeddingss</li></ul></li><li>large: Mistral, Llama</li></ul></li></ul></li><li>circuit-tuning<ul><li>split granularity: nodes(neurons or heads? 详见SAE circuits, AtP*)</li><li>ablation methods(zero? average? )</li><li>$L$选哪个？logit difference / log prob 是下一个token还是什么？(aggregation method)</li><li>IE选哪个？标准activation patching/AtP/AtP*/ig</li><li>threshld</li><li>evaluation</li><li>SAE?</li></ul></li><li>evaluation<ul><li>简单任务<ul><li>主谓不一致相关指标：</li><li>circuit相关指标：faithfulness, completeness, &mldr;（见sfc）</li></ul></li><li>复杂任务</li></ul></li><li>analyses<ul><li>主谓一致->主谓不一致 circuit的变化</li><li>LoRA和circuit-tuning相互验证</li></ul></li><li>application<ul><li>model steering</li></ul></li></ul></li><li><p>优点和不足</p><ul><li>优点<ul><li>参数高效</li><li>可解释，更精准</li><li>缓解灾难性遗忘</li></ul></li><li>不足<ul><li>需要先找到circuit</li><li>计算量大</li><li>不像LoRA一样可插拔</li><li>难以scale?</li></ul></li></ul></li></ul></li></ul><h3 id=linguistics>linguistics<a hidden class=anchor aria-hidden=true href=#linguistics>#</a></h3><ul><li>语言习得，二语习得<ul><li>迁移学习</li></ul></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E5%AD%A6%E7%9A%84%E6%A2%97%E5%92%8C%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%9F%A5%E8%AF%86/><span class=title>« Prev</span><br><span>一些语言学的梗和有意思的知识</span>
</a><a class=next href=http://localhost:1313/posts/mech_interp_research/><span class=title>Next »</span><br><span>Possible Research Areas in Mechanistic Interpretability</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share My research interests on x" href="https://x.com/intent/tweet/?text=My%20research%20interests&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My research interests on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f&amp;title=My%20research%20interests&amp;summary=My%20research%20interests&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My research interests on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f&title=My%20research%20interests"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My research interests on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My research interests on whatsapp" href="https://api.whatsapp.com/send?text=My%20research%20interests%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My research interests on telegram" href="https://telegram.me/share/url?text=My%20research%20interests&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My research interests on ycombinator" href="https://news.ycombinator.com/submitlink?t=My%20research%20interests&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><div id=tw-comment></div><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="light"?"light":"dark",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"Siriuslala/siriuslala.github.io","data-repo-id":"R_kgDOMo4X2w","data-category":"Announcements","data-category-id":"DIC_kwDOMo4X284CiI_8","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":getStoredTheme(),"data-lang":"en","data-loading":"lazy",crossorigin:"anonymous"},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#tw-comment").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Siriuslala's Blog!</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>