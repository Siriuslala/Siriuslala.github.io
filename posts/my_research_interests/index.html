<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>My research interests | Siriuslala's Blog!</title>
<meta name=keywords content><meta name=description content="mechanistic interpretability Computational Linguistics Circuit-tuning: A Mechanistic Approach for Understanding Instrinsic Rank and Fine-tuning Neural Networks motivation 现有问题 现有的微调包括全量微调和PEFT； 不知道在微调的过程中发生了什么；也正是因为缺乏对机理的探究，导致"><meta name=author content="Me"><link rel=canonical href=http://localhost:1313/posts/my_research_interests/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.187da127e3564f8ca7db95950f88b9ebc2d4f26e4c83ebf6b5baafee59ff4ebd.css integrity="sha256-GH2hJ+NWT4yn25WVD4i568LU8m5Mg+v2tbqv7ln/Tr0=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/pig.svg><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/pig.svg><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/pig.svg><link rel=apple-touch-icon href=http://localhost:1313/pig.svg><link rel=mask-icon href=http://localhost:1313/pig.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/my_research_interests/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}.formula{width:100%;overflow-x:auto}</style><meta property="og:title" content="My research interests"><meta property="og:description" content="mechanistic interpretability Computational Linguistics Circuit-tuning: A Mechanistic Approach for Understanding Instrinsic Rank and Fine-tuning Neural Networks motivation 现有问题 现有的微调包括全量微调和PEFT； 不知道在微调的过程中发生了什么；也正是因为缺乏对机理的探究，导致"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/my_research_interests/"><meta property="og:image" content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-27T17:07:06+08:00"><meta property="article:modified_time" content="2024-09-27T17:07:06+08:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="My research interests"><meta name=twitter:description content="mechanistic interpretability Computational Linguistics Circuit-tuning: A Mechanistic Approach for Understanding Instrinsic Rank and Fine-tuning Neural Networks motivation 现有问题 现有的微调包括全量微调和PEFT； 不知道在微调的过程中发生了什么；也正是因为缺乏对机理的探究，导致"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"My research interests","item":"http://localhost:1313/posts/my_research_interests/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"My research interests","name":"My research interests","description":"mechanistic interpretability Computational Linguistics Circuit-tuning: A Mechanistic Approach for Understanding Instrinsic Rank and Fine-tuning Neural Networks motivation 现有问题 现有的微调包括全量微调和PEFT； 不知道在微调的过程中发生了什么；也正是因为缺乏对机理的探究，导致","keywords":[],"articleBody":"mechanistic interpretability Computational Linguistics Circuit-tuning: A Mechanistic Approach for Understanding Instrinsic Rank and Fine-tuning Neural Networks motivation\n现有问题 现有的微调包括全量微调和PEFT； 不知道在微调的过程中发生了什么；也正是因为缺乏对机理的探究，导致出现很多问题，如灾难性遗忘等； circuit discovey是从结果(activation)出发 方法 提供一个理解微调的新的视角：机理可解释性 贡献 从mech interp的角度理解intrinsic rank、LoRA、AdaLoRA等； 提出circuit-tuning，从原因(weights)出发，类似于剪枝(pruning)，作为一种PEFT的新方法，并通过实验证明； 分析circuit-tuning和LoRA殊途同归 前置（背景）\nmech interp neuron -\u003e feature circuit circuit discovery Finetuning full PEFT Lora, intrinsic rank 理论 Provide an understanding of intrinsic rank using mech interp\n猜想：Given a weight matrix $W$ and a circuit $C$ with a granularity at the neuron level, the intrinsic rank of $W$ is equivalent to the number of nodes in $C$ which exist in the vector space projected by $W$.\n讨论 定性分析：circuit和AdaLoRA都是试图寻找神经网络中任务相关的部分 比较共同点 表征角度 AdaLoRA动态选取rank，说明每一层负责某一task的components数量不一样 circuit discovery说明只有一部分组件对某一任务有影响； 人脑中不同区域分管不同功能 计算角度 linear addition 假设\nnode -\u003e feature Linear feature hypothesis: “Let’s call a neural network representation linear if features correspond to directions in activation space.” – toy model 两个原则：Composition as Addition \u0026 Intensity as Scaling – July Updates W特征提取 consider the function of $W$ as feature extraction all features represented in a model is $\\mathcal{F}$. The features of a specific task is a subset of $\\mathcal{F}$. That is, for a specific task $T$, the related features $\\mathcal{F_{t}}\\subsetneqq \\mathcal{F}$. 定理\nTheorem I（维度冗余 dimension redundancy）: For any weight matrix $W \\in \\mathbb{R}^{m\\times n}$ in model $M$ and input data $x\\in\\mathcal{D_{T}}$ that follows a specific distribution corresponding to a specific task $T$, there is redundancy in the dimensions of the latent space $\\mathbb{R}^{D}(D=m)$ where the features lie. The dimension redundancy $d_{r} \\geq D-rank(W) = m - min(m, n)$.\n思路 从mech interp角度理解参数矩阵$W$的低秩性\n假设\n特征是线性的。尽管整个模型是非线性的，但涉及到$W$的操作是线性的； 假设neuron和feature一一对应 理解$W+\\Delta W$\n分析$W$的映射 -\u003e m维向量空间，是对输入数据的表征 LoRA基于SVD，从SVD角度分析$\\Delta W$的映射，通过rank说明由W映射得到的m维向量空间的维度存在冗余 解释：输入数据的特征有限，即某一task需要用的特征有限，故表征空间维度可降低 特别的，when the dimension $D$ of latent space $\\mathbb{R}^{D}$ satisfiies $D \\geq |\\mathcal{F_{t}}| $, then the dimension redundancy is strictly greater than $m - min(m, n)$(see assumption x). 理论的限制范围——输入服从某一种数据分布\nintrinsic rank最初的定义是解空间的余维度？？？(只是知乎的说法，还要根据原文判断) 若从superposition的角度考虑：neuron的多语义是因为预训练数据范围较广，神经网络是用来拟合\"世界模型\"，故对于表征整个世界来说，m维是不够的； 但对于具体任务，涉及到的特征有限，所以在微调过程中m维是冗余的；具体体现在更新量$\\Delta W$中； 参考\nLORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS ADALORA: ADAPTIVE BUDGET ALLOCATION FOR PARAMETER-EFFICIENT FINE-TUNING PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning Sparse Low-rank Adaptation of Pre-trained Language Models Theorem II（特征选择 feature selection）: By performing singular value decomposition $\\Delta W = U\\Sigma V = \\sum_{i=1}^{r}u_{i}\\sigma_{i}v_{i}$ on weight matrix $\\Delta W$, feature selection is feasible with the optimization of the orthogonal basis in $U,V$ and the mask of singular value $\\sigma_{i}$.\nTheorem III（目标等价 objective equivalence）: Selecting singular values based on importance scores in AdaLoRA is equivalent to selecting nodes based on indirect effect in circuit discovery. (除了AdaLoRA，是否还有其他的指标)\n证明 目标：基于SVD的AdaLora通过重要性分数来选择奇异值 等价于 circuit discovery通过indirect effect来选取components 重要性分数 参考 Importance estimation for neural network pruning Super tickets in pre-trained language models: From model compression to improving generalization Platon: Pruning large transformer models with upper confidence bound of weight importance indirect effect 因果推断里的定义： 在activation patching中的具体形式： 参考 INTRINSIC DIMENSIONALITY EXPLAINS THE EFFECTIVENESS OF LANGUAGE MODEL FINE-TUNING MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES 讨论\n不严格等价：与circuit中components的细粒度，特征选取的指标(I, IE)，更新策略都有关系 其他理论分析\nlora A Kernel-Based View of Language Model Fine-Tuning The Impact of LoRA on the Emergence of Clusters in Transformers LoRA Training in the NTK Regime has No Spurious Local Minima Asymmetry in low rank adapters of foundation models The expressive power of low-rank adaptation feature learning Neural Networks can Learn Representations with Gradient Descent 算法\n算法设计 一般 简单任务 sample from dataset(bootstrap) circuit discovery circuit tuning 动态 复杂任务 aggregation method sample -\u003e circuit-\u003e circuit-tuning sample -\u003e new circuit -\u003e circuit-tuning 细节 MOE routing? 参考 (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability (attribution patching) Attribution Patching Outperforms Automated Circuit Discovery Attribution patching: Activation patching at industrial scale AtP*: An efficient and scalable method for localizing llm behaviour to components How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model Linear Representations of sentiment in large language models 微调的效果 objective: predict next word (weights -\u003e logits -\u003e probs) catastrophic forgetting 理论上解释如何缓解 参考 Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks Understanding Catastrophic Forgetting in Language Models via Implicit Inference Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking *(O-LoRA) Orthogonal Subspace Learning for Language Model Continual Learning *(I-LoRA) Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning 实验设计\ndataset task for small model IOI subject-verb agreement for LLM math, instruction following, code, language transfer, … others safety gender bias(见SAE circuits) model small model v.s. LLM Pythia Mistral Llama circuit-tuning split granularity: nodes(neurons or heads? 详见SAE circuits, AtP*) 哪种patching方式 ablation methods(zero? average? ) $L$选哪个？logit difference / log prob 是下一个token还是什么？(aggregation method) threshld evaluation SAE? analyses LoRA和circuit-tuning相互验证 application model steering 分析circuit-tuning的优点和不足\n优点 参数高效 可解释，更精准 缓解灾难性遗忘 不足 需要先找到circuit 难以scale linguistics 语言习得，二语习得 ","wordCount":"1948","inLanguage":"en","image":"http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-09-27T17:07:06+08:00","dateModified":"2024-09-27T17:07:06+08:00","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/my_research_interests/"},"publisher":{"@type":"Organization","name":"Siriuslala's Blog!","logo":{"@type":"ImageObject","url":"http://localhost:1313/pig.svg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:1313/pig.svg alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:1313/about/ title=About><span>About</span></a></li><li><a href=http://localhost:1313/archives/ title=Archive><span>Archive</span></a></li><li><a href=http://localhost:1313/faq/ title=FAQ><span>FAQ</span></a></li><li><a href=http://localhost:1313/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:1313/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">My research interests
<span class=entry-hint title=Draft><svg height="35" viewBox="0 -960 960 960" fill="currentcolor"><path d="M160-410v-60h3e2v60H160zm0-165v-60h470v60H160zm0-165v-60h470v60H160zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22-4.5 22.5T862.09-380L643-160H520zm3e2-263-37-37 37 37zM580-220h38l121-122-18-19-19-18-122 121v38zm141-141-19-18 37 37-18-19z"/></svg></span></h1><div class=post-meta><span title='2024-09-27 17:07:06 +0800 CST'>September 27, 2024</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;1948 words&nbsp;·&nbsp;Me</div></header><div class=post-content><h2 id=mechanistic-interpretability>mechanistic interpretability<a hidden class=anchor aria-hidden=true href=#mechanistic-interpretability>#</a></h2><h3 id=computational-linguistics>Computational Linguistics<a hidden class=anchor aria-hidden=true href=#computational-linguistics>#</a></h3><ul><li>Circuit-tuning: A Mechanistic Approach for Understanding Instrinsic Rank and Fine-tuning Neural Networks<ul><li><p>motivation</p><ul><li>现有问题<ul><li>现有的微调包括全量微调和PEFT；</li><li>不知道在微调的过程中发生了什么；也正是因为缺乏对机理的探究，导致出现很多问题，如灾难性遗忘等；</li><li>circuit discovey是从结果(activation)出发</li></ul></li><li>方法<ul><li>提供一个理解微调的新的视角：机理可解释性</li></ul></li><li>贡献<ul><li>从mech interp的角度理解intrinsic rank、LoRA、AdaLoRA等；</li><li>提出circuit-tuning，从原因(weights)出发，类似于剪枝(pruning)，作为一种PEFT的新方法，并通过实验证明；</li><li>分析circuit-tuning和LoRA殊途同归</li></ul></li></ul></li><li><p>前置（背景）</p><ul><li>mech interp<ul><li>neuron -> feature</li><li>circuit</li><li>circuit discovery</li></ul></li><li>Finetuning<ul><li>full</li><li>PEFT<ul><li>Lora, intrinsic rank</li></ul></li></ul></li></ul></li><li><p>理论
Provide an understanding of intrinsic rank using mech interp</p><ul><li><p>猜想：Given a weight matrix $W$ and a circuit $C$ with a granularity at the neuron level, the intrinsic rank of $W$ is equivalent to the number of nodes in $C$ which exist in the vector space projected by $W$.</p><ul><li>讨论<ul><li>定性分析：circuit和AdaLoRA都是试图寻找神经网络中任务相关的部分<ul><li>比较共同点<ul><li>表征角度<ul><li>AdaLoRA动态选取rank，说明每一层负责某一task的components数量不一样</li><li>circuit discovery说明只有一部分组件对某一任务有影响；</li><li>人脑中不同区域分管不同功能</li></ul></li><li>计算角度<ul><li>linear addition</li></ul></li></ul></li></ul></li></ul></li></ul></li><li><p>假设</p><ul><li>node -> feature</li><li>Linear feature hypothesis: &ldquo;Let&rsquo;s call a neural network representation linear if features correspond to directions in activation space.&rdquo; &ndash; <a href=https://transformer-circuits.pub/2022/toy_model/index.html>toy model</a><ul><li>两个原则：Composition as Addition & Intensity as Scaling &ndash; <a href=https://transformer-circuits.pub/2024/july-update/index.html#linear-representations>July Updates</a></li></ul></li><li>W特征提取<ul><li>consider the function of $W$ as feature extraction</li></ul></li><li>all features represented in a model is $\mathcal{F}$. The features of a specific task is a subset of $\mathcal{F}$. That is, for a specific task $T$, the related features $\mathcal{F_{t}}\subsetneqq \mathcal{F}$.</li></ul></li><li><p>定理</p><ul><li><p>Theorem I（维度冗余 dimension redundancy）: For any weight matrix $W \in \mathbb{R}^{m\times n}$ in model $M$ and input data $x\in\mathcal{D_{T}}$ that follows a specific distribution corresponding to a specific task $T$, there is redundancy in the dimensions of the latent space $\mathbb{R}^{D}(D=m)$ where the features lie. The dimension redundancy $d_{r} \geq D-rank(W) = m - min(m, n)$.</p><ul><li>思路<ul><li><p>从mech interp角度理解参数矩阵$W$的低秩性</p></li><li><p>假设</p><ul><li>特征是线性的。尽管整个模型是非线性的，但涉及到$W$的操作是线性的；</li><li>假设neuron和feature一一对应</li></ul></li><li><p>理解$W+\Delta W$</p><ul><li>分析$W$的映射 -> m维向量空间，是对输入数据的表征</li><li>LoRA基于SVD，从SVD角度分析$\Delta W$的映射，通过rank说明由W映射得到的m维向量空间的维度存在冗余<ul><li>解释：输入数据的特征有限，即某一task需要用的特征有限，故表征空间维度可降低</li></ul></li><li>特别的，when the dimension $D$ of latent space $\mathbb{R}^{D}$ satisfiies $D \geq |\mathcal{F_{t}}| $, then the dimension redundancy is strictly greater than $m - min(m, n)$(see assumption x).</li></ul></li><li><p>理论的限制范围——输入服从某一种数据分布</p><ul><li>intrinsic rank最初的定义是解空间的余维度？？？(只是知乎的说法，还要根据原文判断)</li><li>若从superposition的角度考虑：neuron的多语义是因为预训练数据范围较广，神经网络是用来拟合"世界模型"，故对于表征整个世界来说，m维是不够的；</li><li>但对于具体任务，涉及到的特征有限，所以在微调过程中m维是冗余的；具体体现在更新量$\Delta W$中；</li></ul></li><li><p>参考</p><ul><li><a href=https://arxiv.org/pdf/2106.09685>LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS</a></li><li><a href=https://arxiv.org/pdf/2303.10512>ADALORA: ADAPTIVE BUDGET ALLOCATION FOR PARAMETER-EFFICIENT FINE-TUNING</a></li><li><a href=https://arxiv.org/pdf/2404.02948>PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2406.09044>MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM Finetuning</a></li><li><a href=https://arxiv.org/pdf/2311.11696>Sparse Low-rank Adaptation of Pre-trained Language Models</a></li></ul></li></ul></li></ul></li><li><p>Theorem II（特征选择 feature selection）: By performing singular value decomposition $\Delta W = U\Sigma V = \sum_{i=1}^{r}u_{i}\sigma_{i}v_{i}$ on weight matrix $\Delta W$, feature selection is feasible with the optimization of the orthogonal basis in $U,V$ and the mask of singular value $\sigma_{i}$.</p></li><li><p>Theorem III（目标等价 objective equivalence）: Selecting singular values based on importance scores in AdaLoRA is equivalent to selecting nodes based on indirect effect in circuit discovery. (除了AdaLoRA，是否还有其他的指标)</p><ul><li>证明<ul><li>目标：基于SVD的AdaLora通过重要性分数来选择奇异值 等价于 circuit discovery通过indirect effect来选取components<ul><li>重要性分数<ul><li>参考<ul><li><a href>Importance estimation for neural network pruning</a></li><li><a href>Super tickets in pre-trained language models: From model compression to improving generalization</a></li><li><a href>Platon: Pruning large transformer models with upper confidence bound of weight importance</a></li></ul></li></ul></li><li>indirect effect<ul><li>因果推断里的定义：</li><li>在activation patching中的具体形式：</li></ul></li></ul></li></ul></li><li>参考<ul><li><a href=https://arxiv.org/pdf/2012.13255>INTRINSIC DIMENSIONALITY EXPLAINS THE EFFECTIVENESS OF LANGUAGE MODEL FINE-TUNING</a></li><li><a href=https://arxiv.org/pdf/1804.08838>MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES</a></li></ul></li></ul></li></ul></li><li><p>讨论</p><ul><li>不严格等价：与circuit中components的细粒度，特征选取的指标(I, IE)，更新策略都有关系</li></ul></li><li><p>其他理论分析</p><ul><li>lora<ul><li><a href=https://proceedings.mlr.press/v202/malladi23a/malladi23a.pdf>A Kernel-Based View of Language Model Fine-Tuning</a></li><li><a href=https://arxiv.org/pdf/2402.15415>The Impact of LoRA on the Emergence of Clusters in Transformers</a></li><li><a href=https://arxiv.org/pdf/2402.11867>LoRA Training in the NTK Regime has No Spurious Local Minima</a></li><li><a href=https://arxiv.org/pdf/2402.16842>Asymmetry in low rank adapters of foundation models</a></li><li><a href=https://arxiv.org/pdf/2310.17513>The expressive power of low-rank adaptation</a></li></ul></li><li>feature learning<ul><li><a href=https://arxiv.org/pdf/2206.15144>Neural Networks can Learn Representations with Gradient Descent</a></li></ul></li></ul></li></ul></li><li><p>算法</p><ul><li>算法设计<ul><li>一般<ul><li>简单任务</li></ul><ol><li>sample from dataset(bootstrap)</li><li>circuit discovery</li><li>circuit tuning</li></ol></li><li>动态<ul><li>复杂任务</li><li>aggregation method</li></ul><ol><li>sample -> circuit-> circuit-tuning</li><li>sample -> new circuit -> circuit-tuning</li></ol></li><li>细节<ul><li>MOE routing?</li></ul></li><li>参考<ul><li><a href=https://arxiv.org/abs/2304.14997>(ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability</a></li><li><a href=https://arxiv.org/abs/2310.10348>(attribution patching) Attribution Patching Outperforms Automated Circuit Discovery</a></li><li><a href=https://www.neelnanda.io/mechanistic-interpretability/attribution-patching>Attribution patching: Activation patching at industrial scale</a></li><li><a href=https://arxiv.org/pdf/2403.00745>AtP*: An efficient and scalable method for localizing llm behaviour to components</a></li><li><a href=https://arxiv.org/pdf/2305.00586>How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model</a></li><li><a href=https://arxiv.org/abs/2310.15154>Linear Representations of sentiment in large language models</a></li></ul></li></ul></li><li>微调的效果<ul><li>objective: predict next word (weights -> logits -> probs)</li><li>catastrophic forgetting<ul><li>理论上解释如何缓解</li></ul></li><li>参考<ul><li><a href=https://arxiv.org/pdf/2311.12786>Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks</a></li><li><a href=https://arxiv.org/pdf/2309.10105>Understanding Catastrophic Forgetting in Language Models via Implicit Inference</a></li><li><a href=https://arxiv.org/pdf/2402.14811>Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking</a></li><li>*<a href=https://arxiv.org/pdf/2310.14152>(O-LoRA) Orthogonal Subspace Learning for Language Model Continual Learning</a></li><li>*<a href=https://arxiv.org/pdf/2402.18865>(I-LoRA) Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning</a></li></ul></li></ul></li></ul></li><li><p>实验设计</p><ul><li>dataset<ul><li>task<ul><li>for small model<ul><li>IOI</li><li>subject-verb agreement</li></ul></li><li>for LLM<ul><li>math, instruction following, code, language transfer, &mldr;</li></ul></li><li>others<ul><li>safety</li><li>gender bias(见SAE circuits)</li></ul></li></ul></li></ul></li><li>model<ul><li>small model v.s. LLM<ul><li>Pythia</li><li>Mistral</li><li>Llama</li></ul></li></ul></li><li>circuit-tuning<ul><li>split granularity: nodes(neurons or heads? 详见SAE circuits, AtP*)</li><li>哪种patching方式</li><li>ablation methods(zero? average? )</li><li>$L$选哪个？logit difference / log prob 是下一个token还是什么？(aggregation method)</li><li>threshld</li><li>evaluation</li><li>SAE?</li></ul></li><li>analyses<ul><li>LoRA和circuit-tuning相互验证</li></ul></li><li>application<ul><li>model steering</li></ul></li></ul></li><li><p>分析circuit-tuning的优点和不足</p><ul><li>优点<ul><li>参数高效</li><li>可解释，更精准</li><li>缓解灾难性遗忘</li></ul></li><li>不足<ul><li>需要先找到circuit</li><li>难以scale</li></ul></li></ul></li></ul></li></ul><h3 id=linguistics>linguistics<a hidden class=anchor aria-hidden=true href=#linguistics>#</a></h3><ul><li>语言习得，二语习得</li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E5%AD%A6%E7%9A%84%E6%A2%97%E5%92%8C%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%9F%A5%E8%AF%86/><span class=title>« Prev</span><br><span>一些语言学的梗和有意思的知识</span>
</a><a class=next href=http://localhost:1313/posts/mech_interp_research/><span class=title>Next »</span><br><span>Possible Research Areas in Mechanistic Interpretability</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share My research interests on x" href="https://x.com/intent/tweet/?text=My%20research%20interests&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f&amp;hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My research interests on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f&amp;title=My%20research%20interests&amp;summary=My%20research%20interests&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My research interests on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f&title=My%20research%20interests"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My research interests on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My research interests on whatsapp" href="https://api.whatsapp.com/send?text=My%20research%20interests%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My research interests on telegram" href="https://telegram.me/share/url?text=My%20research%20interests&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share My research interests on ycombinator" href="https://news.ycombinator.com/submitlink?t=My%20research%20interests&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fmy_research_interests%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><div id=tw-comment></div><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="light"?"light":"dark",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"Siriuslala/siriuslala.github.io","data-repo-id":"R_kgDOMo4X2w","data-category":"Announcements","data-category-id":"DIC_kwDOMo4X284CiI_8","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":getStoredTheme(),"data-lang":"en","data-loading":"lazy",crossorigin:"anonymous"},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#tw-comment").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>Siriuslala's Blog!</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>