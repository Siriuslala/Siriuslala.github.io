<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Siriuslala&#39;s Blog!</title>
    <link>http://localhost:1313/posts/</link>
    <description>Recent content in Posts on Siriuslala&#39;s Blog!</description>
    <image>
      <title>Siriuslala&#39;s Blog!</title>
      <url>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>http://localhost:1313/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.133.1</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Apr 2025 15:02:36 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>My_research_interest2</title>
      <link>http://localhost:1313/posts/my_research_interest2/</link>
      <pubDate>Fri, 18 Apr 2025 15:02:36 +0800</pubDate>
      <guid>http://localhost:1313/posts/my_research_interest2/</guid>
      <description>Traceability tool 难点 如何绕过 attention ? next token 直接由 last token 的 last hidden states 生成，而 last hidden states 是上游所有 attention 和 MLP 的输出的叠加； 溯源的目标是看 next token 与序列中哪些历史 token 有关，换句话说，就</description>
    </item>
    <item>
      <title>My_research_interest1</title>
      <link>http://localhost:1313/posts/my_research_interest1/</link>
      <pubDate>Tue, 01 Apr 2025 15:50:13 +0800</pubDate>
      <guid>http://localhost:1313/posts/my_research_interest1/</guid>
      <description>MM Interp vision-language alignment in VLM vision language 语义对齐 信息交互 位置编码，patch 顺序 现有的 VLM 可解释性研究 dogit lens MATS 的项目 ！VLM 里的 causal tracing VLM 里的信息处理：Towards interpreting visual information</description>
    </item>
    <item>
      <title>MM_Interp</title>
      <link>http://localhost:1313/posts/mm_interp/</link>
      <pubDate>Tue, 25 Feb 2025 15:08:53 +0800</pubDate>
      <guid>http://localhost:1313/posts/mm_interp/</guid>
      <description>Resource dataset (GQA) GQA:ANewDataset for Real-World Visual Reasoning and Compositional Question Answering https://cs.stanford.edu/people/dorarad/gqa/index.html image token compression (multimodal image token compression) *AdaFV: Rethinking of Visual-Language alignment for VLM acceleration (FasterVLM) [CLS] Attention is All You Need for Training-FreeVisual Token Pruning: Make VLM Inference Faster Sparsevlm: Visual token sparsification for efficient vision-languag (FastV) An image is worth 1/2 tokens after layer 2: Plug-and-PLay Acceleration for VLLM Inference LLaVA-Mini: Efficient Image and Video</description>
    </item>
    <item>
      <title>Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks</title>
      <link>http://localhost:1313/posts/circuit_tuning/</link>
      <pubDate>Wed, 05 Feb 2025 16:22:33 +0800</pubDate>
      <guid>http://localhost:1313/posts/circuit_tuning/</guid>
      <description>The paper is here.
ArXiv: https://arxiv.org/pdf/2502.06106</description>
    </item>
    <item>
      <title>面经</title>
      <link>http://localhost:1313/posts/%E9%9D%A2%E7%BB%8F/</link>
      <pubDate>Sun, 13 Oct 2024 12:06:08 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E9%9D%A2%E7%BB%8F/</guid>
      <description>国外 Anthropic (MATS) MATS 项目里 Anthropic 的 mentors 发起的 coding screen，不是Leetcode算法题，而是一个小的工程项目。项目一般是要实现一个系统（一个类）的功能（类的</description>
    </item>
    <item>
      <title>一些语言学的梗和有意思的知识</title>
      <link>http://localhost:1313/posts/%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E5%AD%A6%E7%9A%84%E6%A2%97%E5%92%8C%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%9F%A5%E8%AF%86/</link>
      <pubDate>Fri, 27 Sep 2024 17:15:10 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E5%AD%A6%E7%9A%84%E6%A2%97%E5%92%8C%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%9F%A5%E8%AF%86/</guid>
      <description>This post is written in Chinese. If you don&amp;rsquo;t know Chinese, you can learn it lol. (Sorry for this because simply translating the post into English may not be enough for you to understand). 纯玩梗 语言现象背后蕴含的知识 皮钦语 (pidgin) 大家对那些 1.言语中不时夹杂着英文单</description>
    </item>
    <item>
      <title>My research interests</title>
      <link>http://localhost:1313/posts/my_research_interests/</link>
      <pubDate>Fri, 27 Sep 2024 17:07:06 +0800</pubDate>
      <guid>http://localhost:1313/posts/my_research_interests/</guid>
      <description>mechanistic interpretability Computational Linguistics Circuit-tuning: A Mechanistic Approach for Understanding Instrinsic Dimension and Fine-tuning Neural Networks 袁老师建议 理论 重新审视可解释性，如何给出一个漂亮的解释（思考mech interp局限性，是否可以突破一下） 上</description>
    </item>
    <item>
      <title>Possible Research Areas in Mechanistic Interpretability</title>
      <link>http://localhost:1313/posts/mech_interp_research/</link>
      <pubDate>Fri, 06 Sep 2024 22:52:16 +0800</pubDate>
      <guid>http://localhost:1313/posts/mech_interp_research/</guid>
      <description>The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic. Circuit Discovery Methods basic activation patching (causal mediation/interchange interventions&amp;hellip;) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? &amp;#x1f52d; resources inspirition Analyzing Multi-Head</description>
    </item>
    <item>
      <title>Exploring Emotional Features in GPT2-Small</title>
      <link>http://localhost:1313/posts/happy_feats/</link>
      <pubDate>Thu, 29 Aug 2024 15:51:59 +0800</pubDate>
      <guid>http://localhost:1313/posts/happy_feats/</guid>
      <description>&amp;#x1f3b6;Code in this post can be found at the jupyter notebook in my &amp;ldquo;saeExploration&amp;rdquo; repo.
Find features that reflect positive emotions To find the features related to a specific emotion, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:
1 2 3 4 5 prompt_happy = [&amp;#34;I&amp;#39;ll be on a vacation tomorrow and I&amp;#39;m so happy.&amp;#34;, &amp;#34;My mombrings home a new puppy and I&amp;#39;m so happy.</description>
    </item>
    <item>
      <title>A Brief Introduction to Mechanistic Interpretability Research</title>
      <link>http://localhost:1313/posts/mech_interp_resource/</link>
      <pubDate>Wed, 28 Aug 2024 13:12:25 +0800</pubDate>
      <guid>http://localhost:1313/posts/mech_interp_resource/</guid>
      <description>The purpose I write this blog Mechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challenges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc).</description>
    </item>
    <item>
      <title>My First Post</title>
      <link>http://localhost:1313/posts/my-first-post/</link>
      <pubDate>Tue, 27 Aug 2024 15:52:14 +0800</pubDate>
      <guid>http://localhost:1313/posts/my-first-post/</guid>
      <description></description>
    </item>
  </channel>
</rss>
