<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Siriuslala&#39;s Blog!</title>
    <link>https://Siriuslala.github.io/posts/</link>
    <description>Recent content in Posts on Siriuslala&#39;s Blog!</description>
    <image>
      <title>Siriuslala&#39;s Blog!</title>
      <url>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.147.6</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Apr 2025 15:02:36 +0800</lastBuildDate>
    <atom:link href="https://Siriuslala.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>My_research_interest2</title>
      <link>https://Siriuslala.github.io/posts/my_research_interest2/</link>
      <pubDate>Fri, 18 Apr 2025 15:02:36 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/my_research_interest2/</guid>
      <description>&lt;h2 id=&#34;traceability-tool&#34;&gt;Traceability tool&lt;/h2&gt;
&lt;h3 id=&#34;难点&#34;&gt;难点&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;如何绕过 attention ?
&lt;ul&gt;
&lt;li&gt;next token 直接由 last token 的 last hidden states 生成，而 last hidden states 是上游所有 attention 和 MLP 的输出的叠加；&lt;/li&gt;
&lt;li&gt;溯源的目标是看 next token 与序列中哪些历史 token 有关，换句话说，就是看哪些历史 token 的信息流入了 last token 的 hidden state；&lt;/li&gt;
&lt;li&gt;Transformer 中唯一能实现 token 间信息交互的组件，就是 attention，所以 attention 也需要考虑；
&lt;ul&gt;
&lt;li&gt;QK circuit 告诉模型要关注什么信息；OV circuit 告诉模型要将哪些信息放到输出里&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;不过，attention 的信息已经包含在了 last token 的 hidden states 中，如果能够找到一种方法，从 hidden states 中分解出历史 token 的信息，那就可以实现溯源定位&lt;/li&gt;
&lt;li&gt;可以的方法是，看历史 token 的 hidden states 是否在 last token 的 hidden states 中体现。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;多模态模型里图像信息会集中到 |&amp;lt;vision_end&amp;gt;|
&lt;ul&gt;
&lt;li&gt;MMLM 里图像信息在浅层流动到文本，而在深层基本不会发挥作用 (受到的 attention 很少)&lt;/li&gt;
&lt;li&gt;如何&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;可以参考的&#34;&gt;可以参考的&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;information flow&lt;/li&gt;
&lt;li&gt;attention block 方法&lt;/li&gt;
&lt;li&gt;IOI, sv disagreement&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>My_research_interest1</title>
      <link>https://Siriuslala.github.io/posts/my_research_interest1/</link>
      <pubDate>Tue, 01 Apr 2025 15:50:13 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/my_research_interest1/</guid>
      <description>&lt;h2 id=&#34;mm-interp&#34;&gt;MM Interp&lt;/h2&gt;
&lt;h3 id=&#34;vision-language-alignment-in-vlm&#34;&gt;vision-language alignment in VLM&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;vision language 语义对齐&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;信息交互&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;位置编码，patch 顺序&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;现有的 VLM 可解释性研究&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dogit lens
&lt;ul&gt;
&lt;li&gt;MATS 的项目&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;！VLM 里的 causal tracing&lt;/li&gt;
&lt;li&gt;VLM 里的信息处理：Towards interpreting visual information processing in vision-language models
&lt;ul&gt;
&lt;li&gt;通过 logit lens 解码视觉 token，发现与语言对齐&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;信息流动
&lt;ul&gt;
&lt;li&gt;通过 attention 阻断&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;问题&lt;/p&gt;</description>
    </item>
    <item>
      <title>MM_Interp</title>
      <link>https://Siriuslala.github.io/posts/mm_interp/</link>
      <pubDate>Tue, 25 Feb 2025 15:08:53 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/mm_interp/</guid>
      <description>&lt;h2 id=&#34;resource&#34;&gt;Resource&lt;/h2&gt;
&lt;h3 id=&#34;analyses-on-mllms&#34;&gt;Analyses on MLLMs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;dataset&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1902.09506&#34;&gt;(GQA) GQA:ANewDataset for Real-World Visual Reasoning and Compositional Question Answering&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cs.stanford.edu/people/dorarad/gqa/index.html&#34;&gt;https://cs.stanford.edu/people/dorarad/gqa/index.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;image token compression&lt;/strong&gt;&lt;br&gt;
(multimodal image token compression)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.09532&#34;&gt;*AdaFV: Rethinking of Visual-Language alignment for VLM acceleration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.01818&#34;&gt;(FasterVLM) [CLS] Attention is All You Need for Training-FreeVisual Token Pruning: Make VLM Inference Faster&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2410.04417&#34;&gt;Sparsevlm: Visual token sparsification for efficient vision-languag&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2403.06764&#34;&gt;(FastV) An image is worth 1/2 tokens after layer 2: Plug-and-PLay Acceleration for VLLM Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.03895&#34;&gt;LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.03312&#34;&gt;*Inference Optimal VLMs Need Only One Visual Token but Larger Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2407.02392&#34;&gt;TokenPacker: Efficient Visual Projector for Multimodal LLM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2405.17430&#34;&gt;Matryoshka Multimodal Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2405.19315&#34;&gt;Matryoshka Query Transformer for Large Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.04317&#34;&gt;FlashSloth: Lightning Multimodal Large Language Models via Embedded Visual Compression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.16297&#34;&gt;FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2407.14439&#34;&gt;Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;spatial&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks</title>
      <link>https://Siriuslala.github.io/posts/circuit_tuning/</link>
      <pubDate>Wed, 05 Feb 2025 16:22:33 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/circuit_tuning/</guid>
      <description>&lt;!-- The [paper](/pdfs/circuit_tuning) is here. --&gt;
&lt;p&gt;ArXiv(old version): &lt;a href=&#34;https://arxiv.org/pdf/2502.06106&#34;&gt;https://arxiv.org/pdf/2502.06106&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>一些语言学的梗和有意思的知识</title>
      <link>https://Siriuslala.github.io/posts/%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E5%AD%A6%E7%9A%84%E6%A2%97%E5%92%8C%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%9F%A5%E8%AF%86/</link>
      <pubDate>Fri, 27 Sep 2024 17:15:10 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E5%AD%A6%E7%9A%84%E6%A2%97%E5%92%8C%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%9F%A5%E8%AF%86/</guid>
      <description>&lt;p&gt;This post is written in Chinese. If you don&amp;rsquo;t know Chinese, you can learn it lol. (Sorry for this because simply translating the post into English may not be enough for you to understand).&lt;/p&gt;
&lt;h2 id=&#34;纯玩梗&#34;&gt;纯玩梗&lt;/h2&gt;
&lt;h2 id=&#34;语言现象背后蕴含的知识&#34;&gt;语言现象背后蕴含的知识&lt;/h2&gt;
&lt;h3 id=&#34;皮钦语-pidgin&#34;&gt;皮钦语 (pidgin)&lt;/h3&gt;
&lt;p&gt;大家对那些 1.言语中不时夹杂着英文单词 2.装/凡尔赛 的人表现出一种厌恶。例如，下面是某恋综里的一段名场面：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Possible Research Areas in Mechanistic Interpretability</title>
      <link>https://Siriuslala.github.io/posts/mech_interp_research/</link>
      <pubDate>Fri, 06 Sep 2024 22:52:16 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/mech_interp_research/</guid>
      <description>&lt;h2 id=&#34;the-purpose-i-write-this-blog&#34;&gt;The Purpose I Write This Blog&lt;/h2&gt;
&lt;p&gt;   To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.&lt;/p&gt;
&lt;h2 id=&#34;circuit-discovery&#34;&gt;Circuit Discovery&lt;/h2&gt;
&lt;h3 id=&#34;methods&#34;&gt;Methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;basic&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;activation patching (causal mediation/interchange interventions&amp;hellip;)&lt;/li&gt;
&lt;li&gt;path patching&lt;/li&gt;
&lt;li&gt;scaling techinques: attribution patching&lt;/li&gt;
&lt;li&gt;DAS (distributed alignment search)   directional activation patching?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;telescope-resources&#34;&gt;&amp;#x1f52d; resources&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;inspirition
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1905.09418&#34;&gt;Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2202.05262&#34;&gt;(ROME) Locating and Editing Factual Associations in GPT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.neelnanda.io/mechanistic-interpretability/attribution-patching&#34;&gt;Attribution patching: Activation patching at industrial scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.14997&#34;&gt;(ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2310.10348&#34;&gt;Attribution Patching Outperforms Automated Circuit Discovery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2403.00745&#34;&gt;AtP*: An efficient and scalable method for localizing llm behaviour to components&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing&#34;&gt;Causal Scrubbing: a method for rigorously testing interpretability hypotheses&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;new&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Using SAE
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2403.19647&#34;&gt;Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2405.13868&#34;&gt;Automatically Identifying Local and Global Circuits with Linear Computation Graphs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Contextual Decomposition
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.00886&#34;&gt;Mechanistic Interpretation through Contextual Decomposition in Transformers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Edge Pruning ?
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.16778&#34;&gt;Finding Transformer Circuits with Edge Pruning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.03779&#34;&gt;Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;lack of ground truth&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring Emotional Features in GPT2-Small</title>
      <link>https://Siriuslala.github.io/posts/happy_feats/</link>
      <pubDate>Thu, 29 Aug 2024 15:51:59 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/happy_feats/</guid>
      <description>&lt;p&gt;&amp;#x1f3b6;Code in this post can be found at &lt;a href=&#34;https://github.com/Siriuslala/saeExploration/blob/main/multilingual_study.ipynb&#34;&gt;the jupyter notebook in my &amp;ldquo;saeExploration&amp;rdquo; repo&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;find-features-that-reflect-positive-emotions&#34;&gt;Find features that reflect positive emotions&lt;/h2&gt;
&lt;p&gt;To find the features related to a specific emotion, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-1&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-1&#34;&gt;1&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-2&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-2&#34;&gt;2&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-3&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-3&#34;&gt;3&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-4&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-4&#34;&gt;4&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-5&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-5&#34;&gt;5&lt;/a&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;prompt_happy&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;ll be on a vacation tomorrow and I&amp;#39;m so happy.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;My mombrings home a new puppy and I&amp;#39;m so happy.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;m so glad I got the job I wanted.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I feel so happy when I&amp;#39;m with my friends.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;m so happy I got the promotion I wanted.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;I choose to look for features that reflect happiness and sadness. Apart from that, I also wonder if the feature that reflects excitedness has something to do with the one that reflects happiness (they are alike from the semantic level at least.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Brief Introduction to Mechanistic Interpretability Research</title>
      <link>https://Siriuslala.github.io/posts/mech_interp_resource/</link>
      <pubDate>Wed, 28 Aug 2024 13:12:25 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/mech_interp_resource/</guid>
      <description>&lt;p&gt;&amp;#x26a0;&amp;#xfe0f; &lt;font color=&#34;red&#34;&gt;Warnings !&lt;/font&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;This post was written when I first delved into this area, and it hasn&amp;rsquo;t been updated for a long time. Thus there might be a lot of errors.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Now I&amp;rsquo;ve changed my attitude to this area. The area is not well-defined, and most of the research in this area is of low quality and is not appealing to me. Besides, I think the study of interpretability should be applied to pratical use, though we can also study it for fun.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;I&amp;rsquo;m still interested in interpretability and its applications. I&amp;rsquo;ll write something new and interesting later ~&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;:bumb: This post is accompanied with &lt;a href=&#34;https://Siriuslala.github.io/mech_interp_research.md&#34;&gt;another post&lt;/a&gt;.
:bumb: This post is accompanied with &lt;a href=&#34;https://Siriuslala.github.io/mech_interp_research&#34;&gt;another post&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
