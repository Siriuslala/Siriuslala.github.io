<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>A Brief Introduction to Mechanistic Interpretability Research | Siriuslala's Blog!</title>
<meta name=keywords content="mechanistic interpretability,machine learning"><meta name=description content="The purpose I write this blog Mechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challanges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc)."><meta name=author content="Sirius"><link rel=canonical href=https://Siriuslala.github.io/posts/mech_interp_resource/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.187da127e3564f8ca7db95950f88b9ebc2d4f26e4c83ebf6b5baafee59ff4ebd.css integrity="sha256-GH2hJ+NWT4yn25WVD4i568LU8m5Mg+v2tbqv7ln/Tr0=" rel="preload stylesheet" as=style><link rel=icon href=https://Siriuslala.github.io/pig.svg><link rel=icon type=image/png sizes=16x16 href=https://Siriuslala.github.io/pig.svg><link rel=icon type=image/png sizes=32x32 href=https://Siriuslala.github.io/pig.svg><link rel=apple-touch-icon href=https://Siriuslala.github.io/pig.svg><link rel=mask-icon href=https://Siriuslala.github.io/pig.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Siriuslala.github.io/posts/mech_interp_resource/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}.formula{width:100%;overflow-x:auto}</style><meta property="og:title" content="A Brief Introduction to Mechanistic Interpretability Research"><meta property="og:description" content="The purpose I write this blog Mechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challanges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc)."><meta property="og:type" content="article"><meta property="og:url" content="https://Siriuslala.github.io/posts/mech_interp_resource/"><meta property="og:image" content="https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-08-28T13:12:25+08:00"><meta property="article:modified_time" content="2024-08-28T13:12:25+08:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="A Brief Introduction to Mechanistic Interpretability Research"><meta name=twitter:description content="The purpose I write this blog Mechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challanges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Siriuslala.github.io/posts/"},{"@type":"ListItem","position":2,"name":"A Brief Introduction to Mechanistic Interpretability Research","item":"https://Siriuslala.github.io/posts/mech_interp_resource/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"A Brief Introduction to Mechanistic Interpretability Research","name":"A Brief Introduction to Mechanistic Interpretability Research","description":"The purpose I write this blog Mechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challanges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc).","keywords":["mechanistic interpretability","machine learning"],"articleBody":"The purpose I write this blog Mechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challanges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc). Thus it‚Äôs a little bit difficult for people new to this area to figure out what researchers are really doing.\nTherefore I write this blog to give a brief introduction to mechanistic interpretability without so much of horrible concepts. The blog aims to help you understand the basic ideas, main directions and latest achievements of this field, providing a list of resources to help you get started at the same time!\nIf you really want to do some cool research as a beginner, I highly recommend the guide by Neel Nanda.\nWhat is Mechanistic interpretability? Speaking of AI research, neural network is the tool that is used most widely nowadays for its excellent representation and generalization ability. What does a neural network do? It receives an input and gives an output after some calculations. Specifically speaking, it usually gets the representations of an input and maps it to an expected output under a predefined computation graph. From my perspective, neural networks mainly care about two things: create representations to extract features and establishe the relationship between the representations and the output. Why is neural network so popular? An important reason is that the neural network can save a lot of time for researchers to manually design features. For example, for natural language processing (NLP) people often designed features like ‚Äúthe frequency of a word that appears‚Äù or ‚Äúthe co-occurence probabilities‚Äù in the past. Manually designing features caused too much labor, so people choose to use neural networks to find features automatically. As for optimizing, they set a goal of minimizing the loss function and using backforward propagation (BP) to update the parameters of the model. Thus neural networks free our hands and improve performance at the same time.\nAll is well, so why do we concern about interpretability? Though neural networks can extract a lot of features with a high efficiency, we cannot have a clear understanding of what the features really are. For example, we know that a filter with Laplacian operator can extract the edge of an image, but we don‚Äôt know what the features extracted by a convolution layer mean because the parameters of the filters inside are often randomly initialized and optimized using BP algorithm. As a result, features in neural networks are often ambiguous.\nWhy is interpretability important? Actually this statement is controversial because some people say interpretability is bullshitüí©. I‚Äôm not angry about this. Anyway, people‚Äôs taste varies, just like many people enjoy Picasso‚Äôs abstract paintings while I don‚Äôt. Interpretability still lacks exploring so it‚Äôs now far from application, and that‚Äôs why some people look down on it. While it is this lack of exploration that excites me most because there are a lot of unknown things waiting for me to discover! Actually, Interpretability is a key component in the AI alignment cycle (see Figure 0). The goal of alignment is to ‚Äúmake AI systems behave in line with human intentions and values‚Äù and interpretability plays an important role in ensuring AI safety. For example, unwanted things like malicious text generated by a language model may be avoided using model steering (a trick played on the activations during the forward propagation). Besides, having a clear understanding of neural networks enables us to focus on the relevant part of a model to a specific task, thus reducing unnecessary computation waste and is beneficial for environmental protection.\nFigure 0: The position of interpretability in the AI alignmnet cycle (from Arena)\rLast question: what is mechanistic interpretability? Let‚Äôs call it mech interp first because I‚Äôm really tired of typing the full nameüí¶. There seems not to be a rigorous definition, but I here I want to quote the explanation by Chris Olah:\nMechanistic interpretability seeks to reverse engineer neural networks, similar to how one might reverse engineer a compiled binary computer program.\nAnother thing: there are various of categories of interpretability, such as studies from the geometry perspective or from the game theory and symbol system perspective, which can be found at ICML, ICLR, NeurlPS, etc. When we say mech interp, we often refer to the studies on Transformer-based generative language models now (though the research started before 2017) which will be introduced briefly in the next section. So before we start, let‚Äôs briefly go over the structure of Transformer first!\nFigure 1: The structure of Transformer (from Arena)\rFigure 2: The structure of the self-attention block in a Transformer block (from Arena)\rFigure 3: The structure of the MLP layer in a Transformer block (from Arena)\rFigure 4: The structure of the layer normalization in a Transformer block (from Arena)\rBasic ideas and research topics In this section, I‚Äôm gonna explain some terms for mech interp and help you understand the basic ideas of doing mech interp research. I‚Äôll try to make it easy!\nNote that I‚Äôll only introduce something that I think is important. If you wanna learn more about the concepts in mech interp, please refer to: A Comprehensive Mechanistic Interpretability Explainer \u0026 Glossary which is a very comprehensive guide for beginners that I strongly recommend!\nImportant concepts Features\nThere are a lot of definitions for features. Unfortunately none of the definitions above can be widely recognized, so it‚Äôs open for anyone who wants to seek for the essence of the features. Generally speaking, a feature is a property of an input which is interpretable or cannot be understand by humans. Practically speaking, a feature could be an activation value of a hidden state in a model (at least lots of work is focusing on this). How to find a feature? Or how to know that the thing you find is likely to be a feature? Here I want to quote the concept of ‚Äúthe signal of structure‚Äù proposed by Chris Olah in the post of his thoughts on qualititive research:\nThe signal of structure is any structure in one‚Äôs qualitative observations which cannot be an artifact of measurement or have come from another source, but instead must reflect some kind of structure in the object of inquiry, even if we don‚Äôt understand it.\nJust like the discovery of cells under a microscope. The shape of the cells cannot be random noise but strong evidence for the structure of them.\nCircuits\nIf we view a language model as a directed acyclic graph (DAG) $M$ where nodes are terms in its forward pass (neurons, attention heads, embeddings, etc.) and edges are the interactions between those terms (residual connections, attention, projections, etc.), a circuit $C$ is a subgraph of $M$ responsible for some behavior. That means the components inside the circuit have a big influence on the output of the task, while the components outside the subgraph have almost no influence.\nFrom my perspective, a circuit is a path from an input to an output, just like the way between two hosts in the routing networks.\nFigure 5: The compuatational graph of a model (from Arena)\rSuperposition\nSuperposition is a hypothesis that models can represent more features than the dimensions they have.\nIdeally we expect that each neuron only corresponds to one feature, so we can investigate or even control the feature using the neuron reserved for it. But in practice we find that a neuron fires for more than one features, which is called the phenomenon of polysemanticity in neurons. We believe that we have more features than model dimensions, so we can also say that more than one neurons fire when a feature appears. That is to say, there is not a one-to-one correspondence between neurons and features.\nPrivileged basis\nIt‚Äôs a weird idea that I have some doubt on it (maybe I haven‚Äôt grasp the core idea of it‚Ä¶).\nMy understanding: There are many vector spaces in a model, for example, the residual stream in a layer, the output of the ReLU in a MLP layer, etc. Each vector space can be seen as a representation. Given an input, we can get the hidden states in different vector spaces during the forward propagation of the model. If we could view neurons as directions which may correspond to features in a vector space, then we say there is a privileged basis in this vector space. That is to say, each value at a specific dimension is aligned with a neuron, and that value may be a interpretable feature (maybe not).\nNot all vector spaces in a model have privileged basis. The most accepted view is that privileged bases exist in attention patterns and MLP activations, but not in residual streams. A general law is that a privilege basis often appears with a elementwise nonlinear operation, for instance, ReLu, Softmax, etc. If the operations around a representation are all linear, then we say the basis in the representation is non-privileged. For example, the operations around a residual stream are often non-linear (e.g. $W_{in}$ and $W_{out}$ of a MLP layer which correspond to the ‚Äúread‚Äù and ‚Äúwrite‚Äù operation on the residual stream). If we apply a rotation matrix to the original operations to change the basis, then the result will be unchanged because In other words, something is a privileged basis if it is not rotation-independent, i.e. the nature of computation done on it means that the basis directions have some special significance. A privileged basis is a meaningful basis for a vector space. That is, the coordinates in that basis have some meaning, that coordinates in an arbitrary basis do not have. It does not, necessarily, mean that this is an interpretable basis.\na space can have an interpretable basis without having a privileged basis. In order to be privileged, a basis needs to be interpretable a priori - ie we can predict it solely from the structure of the network architecture.\nResearch techniques Circuits Discovery\nFinding the circuit for a specific task attracts the attention of lots of researchers. The thing we wanna do is to get the relevant components for a specific task. A naive idea is to test the components one by one using causal intervention: change the value of one component while keeping others unchanged, and check if it influences the output. This technique is also called ablation or knockout.\nTo achieve this, we have two possible ways: denoising (find useful components) and noising (delete unuseful components). We usually prepare a clean prompt which is relevant to the task (results in a correct answer) and a corrupted prompt which has nothing to do with the task. Before finding circuits, the two prompts are fed into the model separately to get a clean run and a corrupted run.\nIf we use denoising, at each step we replace (patch) the value of a component in the corrupted run with that in the clean run. If the output is closer to the correct answer under a specific metric (e.g. KL divergence or logit difference), then we add the component into the circuit. If we use noising, then we should replace a component in the clean run with that in the corrupted run. If the output is almost unchanged under a threshod, then we regard the component as useless and delete it. Generally speaking, denoising is better than noising. To understand this, I want to quote a line in Arena: noising tells you what is necessary, denoising tells you what is sufficient.\nSeveral techniques in this area:\nactivation patching (aka causal mediation/interchange interventions‚Ä¶) A method for circuits discovery that take nodes into consideration. path patching A variant of activation patching that also take edges into consideration to study which connections between components matter. For a pair of components A and B, we patch in the clean output of A, but only along paths that affect the input of component B. While in activation patching, all the subsequent components after A are affected. attribution patching An approximation of activation patching using a first-order Taylor expansion on the metric. This method is used to speed up circuits finding. Figure 6: Comparsion of activation patching and path patching (from Arena)\rThe difference between activation patching and path pathcing are shown in Figure 2. In activation patching, we simply patch the node $D$ with $D‚Äô$, so the nodes after $D$ ($H, G$ and $F$) are affected. While in path patching, we patch edges rather than nodes. For example, we only want to patch the edge $D \\to G$, which means the only change is the information from node $D$ to node $G$. As a result, only $G$ and $F$ are affected while $H$ isn‚Äôt.\nüî≠ Recommended papers: (ROME) Locating and Editing Factual Associations in GPT (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability (attribution patching) Attribution Patching Outperforms Automated Circuit Discovery (IOI) INTERPRETABILITY IN THE WILD: A CIRCUIT FOR INDIRECT OBJECT IDENTIFICATION IN GPT-2 SMALL Dictionary Learning\nDictionary Learning aims to deal with the problem of superposition. The idea is like compression sensing in the field of signal processing and is discussed in this article. The implementation of dictionary learning is to train a sparse autoencoder (SAE).\nAn autoencoder consists of an encoder and a decoder. The encoder receives an input and compresses it to a lower dimension, and the decoder maps the hidden representation to the original input. The goal of the autoencoder is to get the representation of the input while compressing it. The autoencoder is optimized using the reconstruction loss.\nCompared with the autoencoedr, the dimension of the hidden representation in SAE is always higher than that of the input, which means the SAE does something completely opposite to the autoencoder. The idea behind is that the model dimension is smaller than the number of features to represent. The model may use superposition to make full use of limited neurons to represent more features. To get one-to-one correspondence between neurons and features, we map the representation to a higher dimensional vector space with SAE encoder. Once we get the representation in SAE (let‚Äôs call it sparse features), we maps it back to the original input with SAE decoder.\nIn practice, any hidden state in a model can be studied using SAE. For example, when we want to get the sparse features of the activations $h$ in a MLP layer. We can do as follows:\n$$ z = ReLU(W_{enc}h + b_{enc}) $$ $$ h^{\\prime} = W_{dec}z + b_{dec} $$\n$$ loss = \\mathbb{E}_{h}\\left[||h-h^{\\prime}||_{2}^{2} + \\lambda||z||\\right] $$\nNote that $ h = [h_{1}, h_{2},‚Ä¶,h_{n}]^{T} \\in \\mathbb{R}^{n\\times1} $ is a hidden state with $n$ dimensions, and each $h_{i} \\in H$ is the value of a specific dimension $i$. $W_{enc} \\in \\mathbb{R}^{m\\times n}$ maps the hidden state to a new vector space with dimension $m\u003en$, $W_{dec} \\in \\mathbb{R}^{n\\times m}$ maps the sparse features back to the original shape, $ b_{enc} \\in \\mathbb{R}^{n} $ and $ b_{dec} \\in \\mathbb{R}^{n} $ are learned bias. The loss function consists of two parts: the MSE loss as the reconstruction loss and L1 norm with a coefficient $\\lambda$ to encourage the sparsity of feature activations. It is the regularization term that separates SAE from ordinary autoencoders, so as to discourage superposition and encourage monosemanticity.\nTo better understanding the encoder and decoder in SAE, we can write a sparse feature $ f_{i} $ as an element of $ z $ :\n$$ f_{i}(h) = z_{i} = ReLU(W^{enc}_{i,.}\\cdot h + b^{enc}_{i}) $$\nEach sparse feature $ f_{i} $ is calculated using row $i$ of the encoder weight matrix. As for decoder, we can write $ h^{\\prime} $ as:\n$$ h^{\\prime} = \\sum_{i=1}^{m}f_{i}(h) \\cdot W^{dec}_{.,i} + b_{dec} $$\nThe reconstructed activation $h^\\prime$ can been seen as the addition of all the features. Each column of the decoder matrix corresponds to a feature, so we call it a ‚Äúfeature direction‚Äù. Note that sometimes the L1 norm term in the loss function can be replaced by $ \\lambda\\sum_{i=1}^{m}f_{i}(h)||W^{dec}_{.,i}||_{2} $ which places a constraint to the decoder weights to reduce ambiguity in the addition operation (we want only one or a few features to be large).\nFor simplicity, The details of the model structure, training method and evaluation will not be shown here.\nüî≠ Recommended papers: Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Model Steering\nA useful technique for eliciting certain model behaviors in a mechanistic way.\nüî≠ Recommended papers: Activation Addition: Steering Language Models Without Optimization (lesswrong) Steering GPT-2-XL by adding an activation vector Steering Llama 2 via Contrastive Activation Addition Mechanistically Eliciting Latent Behaviors in Language Models Research Areas Theory\nUnderstand model components Understand model behaviors Application\ninterpretable model structure AI alignment\nAvoid bias and harmful behaviors Some Useful Resources Here I list some resources that would be helpful for you to get started quickly in the field.\nTutorials Arena‚ÄÇA tutorial created and maintained by Callum McDougall et al, providing a guided path for anyone who finds themselves overwhelmed by the amount of technical AI safety content out there. Neel Nanda‚Äôs Tutorial‚ÄÇNeel‚Äôs tutorial for mech interp. Neel Nanda‚Äôs Quickstart Guide‚ÄÇA quick start for mech interp. Neel Nanda‚Äôs remommended papers‚ÄÇSome classic and important papers for mech interp. Neel Nanda‚Äôs problems v1‚ÄÇNeel‚Äôs old questions for mech interp. Neel Nanda‚Äôs problems v2‚ÄÇNeel‚Äôs 200 new questions for mech interp. Alignment Research Field Guide (by the MIRI team)‚ÄÇFrameworks and Libraries TransformerLens‚ÄÇA library maintained by Bryce Meyer and created by Neel Nanda. SAELens‚ÄÇOriginates from TransformerLens, and is separated from it because of the popularity and importance of SAE. CircuitsVis‚ÄÇA good tool for visualizing LLMs. Plotly‚ÄÇA good tool for plotting. Forums and Communities Transformer Circuits Thread‚ÄÇThe research posts of Anthropic alignment group. Lesswrong‚ÄÇAI Alignment Forum‚ÄÇCompanies, Institutes, Labs and Programs Anthropic‚ÄÇDeepMind‚ÄÇFAR‚ÄÇApollo‚ÄÇRedWood‚ÄÇCHAI (UC Berkeley)‚ÄÇMIRI (UC Berkeley) Alignment Research Center (ARC)‚ÄÇMATS‚ÄÇAn independent research and educational seminar program that connects talented scholars with top mentors in the fields of AI alignment, interpretability, and governance. Blogs Chris Olah‚ÄÇNeel Nanda‚ÄÇArthur Conmy‚ÄÇTrenton Bricken‚ÄÇCallum Mcdougall‚ÄÇAlex Turner(TurnTrout)‚ÄÇ‚Ä¶\n","wordCount":"3052","inLanguage":"en","image":"https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-08-28T13:12:25+08:00","dateModified":"2024-08-28T13:12:25+08:00","author":{"@type":"Person","name":"Sirius"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Siriuslala.github.io/posts/mech_interp_resource/"},"publisher":{"@type":"Organization","name":"Siriuslala's Blog!","logo":{"@type":"ImageObject","url":"https://Siriuslala.github.io/pig.svg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Siriuslala.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://Siriuslala.github.io/pig.svg alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Siriuslala.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://Siriuslala.github.io/about/ title=About><span>About</span></a></li><li><a href=https://Siriuslala.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://Siriuslala.github.io/faq/ title=FAQ><span>FAQ</span></a></li><li><a href=https://Siriuslala.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://Siriuslala.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Siriuslala.github.io/>Home</a>&nbsp;¬ª&nbsp;<a href=https://Siriuslala.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">A Brief Introduction to Mechanistic Interpretability Research</h1><div class=post-meta><span title='2024-08-28 13:12:25 +0800 CST'>August 28, 2024</span>&nbsp;¬∑&nbsp;15 min&nbsp;¬∑&nbsp;3052 words&nbsp;¬∑&nbsp;Sirius</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#the-purpose-i-write-this-blog>The purpose I write this blog</a></li><li><a href=#what-is-mechanistic-interpretability>What is Mechanistic interpretability?</a></li><li><a href=#basic-ideas-and-research-topics>Basic ideas and research topics</a><ul><li><a href=#important-concepts>Important concepts</a></li><li><a href=#research-techniques>Research techniques</a></li><li><a href=#research-areas>Research Areas</a></li></ul></li><li><a href=#some-useful-resources>Some Useful Resources</a><ul><li><a href=#tutorials>Tutorials</a></li><li><a href=#frameworks-and-libraries>Frameworks and Libraries</a></li><li><a href=#forums-and-communities>Forums and Communities</a></li><li><a href=#companies-institutes-labs-and-programs>Companies, Institutes, Labs and Programs</a></li><li><a href=#blogs>Blogs</a></li></ul></li></ul></nav></div></details></div><div class=post-content><h2 id=the-purpose-i-write-this-blog>The purpose I write this blog<a hidden class=anchor aria-hidden=true href=#the-purpose-i-write-this-blog>#</a></h2><p>‚ÄÇ ‚ÄÇMechanistic Interpretability is a new field in machine learning that aims to reverse engineering complicated model structures to something clear, understandable and hopefully controllable for our humans. The study of this field is still at a young age and facing mountains of challanges. While for beginners (like me), there are lots of terms or ideas which are not so familiar (e.g. superposition, circuits, activation patching, etc). Thus it&rsquo;s a little bit difficult for people new to this area to figure out what researchers are really doing.<br>‚ÄÇ ‚ÄÇTherefore I write this blog to give a brief introduction to mechanistic interpretability without so much of horrible concepts. The blog aims to help you understand the <strong>basic ideas, main directions and latest achievements</strong> of this field, providing <strong>a list of resources</strong> to help you get started at the same time!<br>‚ÄÇ ‚ÄÇIf you really want to do some cool research as a beginner, I highly recommend <a href=https://www.neelnanda.io/mechanistic-interpretability/getting-started>the guide by Neel Nanda</a>.</p><h2 id=what-is-mechanistic-interpretability>What is Mechanistic interpretability?<a hidden class=anchor aria-hidden=true href=#what-is-mechanistic-interpretability>#</a></h2><p>‚ÄÇ‚ÄÇSpeaking of AI research, neural network is the tool that is used most widely nowadays for its excellent representation and generalization ability. What does a neural network do? It receives an input and gives an output after some calculations. Specifically speaking, it usually gets the representations of an input and maps it to an expected output under a predefined computation graph. From my perspective, neural networks mainly care about two things: create representations to extract features and establishe the relationship between the representations and the output.<br>‚ÄÇ‚ÄÇWhy is neural network so popular? An important reason is that the neural network can save a lot of time for researchers to <strong>manually</strong> design features. For example, for natural language processing (NLP) people often designed features like &ldquo;the frequency of a word that appears&rdquo; or &ldquo;the co-occurence probabilities&rdquo; in the past. Manually designing features caused too much labor, so people choose to use neural networks to find features automatically. As for optimizing, they set a goal of minimizing the loss function and using backforward propagation (BP) to update the parameters of the model. Thus neural networks free our hands and improve performance at the same time.<br>‚ÄÇ‚ÄÇ All is well, so why do we concern about interpretability? Though neural networks can extract a lot of features with a high efficiency, we cannot have a clear understanding of what the features really are. For example, we know that a filter with Laplacian operator can extract the edge of an image, but we don&rsquo;t know what the features extracted by a convolution layer mean because the parameters of the filters inside are often randomly initialized and optimized using BP algorithm. As a result, features in neural networks are often ambiguous.<br>‚ÄÇ‚ÄÇ Why is interpretability important? Actually this statement is controversial because some people say interpretability is bullshit&#x1f4a9;. I&rsquo;m not angry about this. Anyway, people&rsquo;s taste varies, just like many people enjoy Picasso&rsquo;s abstract paintings while I don&rsquo;t. Interpretability still lacks exploring so it&rsquo;s now far from application, and that&rsquo;s why some people look down on it. While it is this lack of exploration that excites me most because there are a lot of unknown things waiting for me to discover! Actually, Interpretability is a key component in the <a href=https://arxiv.org/pdf/2310.19852><strong>AI alignment</strong></a> cycle (see Figure 0). The goal of alignment is to &ldquo;make AI systems behave in line with human intentions and values&rdquo; and interpretability plays an important role in ensuring <strong>AI safety</strong>. For example, unwanted things like malicious text generated by a language model may be avoided using model steering (a trick played on the activations during the forward propagation). Besides, having a clear understanding of neural networks enables us to focus on the relevant part of a model to a specific task, thus reducing unnecessary computation waste and is beneficial for <strong>environmental protection</strong>.</p><center><img src=/mech_interp_resource/alignment.png width=3000>
<font size=3>Figure 0: The position of interpretability in the AI alignmnet cycle (from <a href=https://arxiv.org/pdf/2310.19852>Arena</a>)</font></center><p>‚ÄÇ‚ÄÇLast question: what is mechanistic interpretability? Let&rsquo;s call it mech interp first because I&rsquo;m really tired of typing the full name&#x1f4a6;. There seems not to be a rigorous definition, but I here I want to quote the explanation by <a href=https://transformer-circuits.pub/2022/mech-interp-essay/index.html>Chris Olah</a>:</p><p>‚ÄÇ‚ÄÇ<em>Mechanistic interpretability seeks to reverse engineer neural networks, similar to how one might reverse engineer a compiled binary computer program.</em></p><p>‚ÄÇ‚ÄÇAnother thing: there are various of categories of interpretability, such as studies from the geometry perspective or from the game theory and symbol system perspective, which can be found at ICML, ICLR, NeurlPS, etc. When we say mech interp, we often refer to the studies on Transformer-based generative language models now (though the research started before 2017) which will be introduced briefly in the next section. So before we start, let&rsquo;s briefly go over the structure of Transformer first!</p><center><img src=/mech_interp_resource/transformer-gpt2.png width=3000>
<font size=3>Figure 1: The structure of Transformer (from <a href=https://arena3-chapter1-transformer-interp.streamlit.app/[1.3]_Indirect_Object_Identification>Arena</a>)</font></center><center><img src=/mech_interp_resource/transformer-attn.png width=3000>
<font size=3>Figure 2: The structure of the self-attention block in a Transformer block (from <a href=https://arena3-chapter1-transformer-interp.streamlit.app/[1.3]_Indirect_Object_Identification>Arena</a>)</font></center><center><img src=/mech_interp_resource/transformer-mlp.png width=3000>
<font size=3>Figure 3: The structure of the MLP layer in a Transformer block (from <a href=https://arena3-chapter1-transformer-interp.streamlit.app/[1.3]_Indirect_Object_Identification>Arena</a>)</font></center><center><img src=/mech_interp_resource/transformer-ln.png width=3000>
<font size=3>Figure 4: The structure of the layer normalization in a Transformer block (from <a href=https://arena3-chapter1-transformer-interp.streamlit.app/[1.3]_Indirect_Object_Identification>Arena</a>)</font></center><h2 id=basic-ideas-and-research-topics>Basic ideas and research topics<a hidden class=anchor aria-hidden=true href=#basic-ideas-and-research-topics>#</a></h2><p>‚ÄÇ‚ÄÇIn this section, I&rsquo;m gonna explain some terms for mech interp and help you understand the basic ideas of doing mech interp research. I&rsquo;ll try to make it easy!<br>‚ÄÇ‚ÄÇNote that I&rsquo;ll only introduce something that I think is important. If you wanna learn more about the concepts in mech interp, please refer to: <a href=https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J>A Comprehensive Mechanistic Interpretability Explainer & Glossary</a> which is a very comprehensive guide for beginners that I strongly recommend!</p><h3 id=important-concepts>Important concepts<a hidden class=anchor aria-hidden=true href=#important-concepts>#</a></h3><p><strong>Features</strong><br>‚ÄÇ‚ÄÇThere are a lot of definitions for features. Unfortunately none of the definitions above can be widely recognized, so it&rsquo;s open for anyone who wants to seek for the essence of the features. Generally speaking, a feature is a property of an input which is interpretable or cannot be understand by humans. Practically speaking, a feature could be an activation value of a hidden state in a model (at least lots of work is focusing on this).<br>‚ÄÇ‚ÄÇHow to find a feature? Or how to know that the thing you find is likely to be a feature? Here I want to quote the concept of &ldquo;the signal of structure&rdquo; proposed by Chris Olah in the post of <a href=https://transformer-circuits.pub/2024/qualitative-essay/index.html>his thoughts on qualititive research</a>:</p><p>‚ÄÇ‚ÄÇ<em>The signal of structure is any structure in one&rsquo;s qualitative observations which cannot be an artifact of measurement or have come from another source, but instead must reflect some kind of structure in the object of inquiry, even if we don&rsquo;t understand it.</em></p><p>‚ÄÇ‚ÄÇJust like the discovery of cells under a microscope. The shape of the cells cannot be random noise but strong evidence for the structure of them.</p><p><strong>Circuits</strong><br>‚ÄÇ‚ÄÇIf we view a language model as a directed acyclic graph (DAG) $M$ where nodes are terms in its forward pass (neurons, attention heads, embeddings, etc.) and edges are the interactions between those terms (residual connections, attention, projections, etc.), a circuit $C$ is a subgraph of $M$ responsible for some behavior. That means the components inside the circuit have a big influence on the output of the task, while the components outside the subgraph have almost no influence.<br>‚ÄÇ‚ÄÇFrom my perspective, a circuit is a path from an input to an output, just like the way between two hosts in the routing networks.</p><center><img src=/mech_interp_resource/forward.png width=400>
<font size=3>Figure 5: The compuatational graph of a model (from <a href=https://arena3-chapter1-transformer-interp.streamlit.app/[1.3]_Indirect_Object_Identification>Arena</a>)</font></center><p><strong>Superposition</strong><br>‚ÄÇ‚ÄÇSuperposition is a hypothesis that models can represent more features than the dimensions they have.<br>‚ÄÇ‚ÄÇIdeally we expect that each neuron only corresponds to one feature, so we can investigate or even control the feature using the neuron reserved for it. But in practice we find that a neuron fires for more than one features, which is called the phenomenon of <strong>polysemanticity</strong> in neurons. We believe that we have more features than model dimensions, so we can also say that more than one neurons fire when a feature appears. That is to say, there is not a one-to-one correspondence between neurons and features.</p><p><strong>Privileged basis</strong><br>‚ÄÇ‚ÄÇIt&rsquo;s a weird idea that I have some doubt on it (maybe I haven&rsquo;t grasp the core idea of it&mldr;).<br>‚ÄÇ‚ÄÇ<strong>My understanding</strong>: There are many <strong>vector</strong> spaces in a model, for example, the residual stream in a layer, the output of the ReLU in a MLP layer, etc. Each vector space can be seen as a <strong>representation</strong>. Given an input, we can get the hidden states in different vector spaces during the forward propagation of the model. If we could view neurons as directions which <strong>may</strong> correspond to features in a vector space, then we say there is a privileged basis in this vector space. That is to say, each value at a specific dimension is aligned with a neuron, and that value may be a interpretable feature (maybe not).<br>‚ÄÇ‚ÄÇNot all vector spaces in a model have privileged basis. The most accepted view is that privileged bases exist in attention patterns and MLP activations, but not in residual streams. A general law is that a privilege basis often appears with a <strong>elementwise nonlinear</strong> operation, for instance, ReLu, Softmax, etc. If the operations around a representation are all linear, then we say the basis in the representation is non-privileged. For example, the operations around a residual stream are often non-linear (e.g. $W_{in}$ and $W_{out}$ of a MLP layer which correspond to the &ldquo;read&rdquo; and &ldquo;write&rdquo; operation on the residual stream). If we apply a rotation matrix to the original operations to change the basis, then the result will be unchanged because In other words, something is a privileged basis if it is not rotation-independent, i.e. the nature of computation done on it means that the basis directions have some special significance.
A privileged basis is a meaningful basis for a vector space. That is, the coordinates in that basis have some meaning, that coordinates in an arbitrary basis do not have. It does not, necessarily, mean that this is an interpretable basis.<br>a space can have an interpretable basis without having a privileged basis. In order to be privileged, a basis needs to be interpretable a priori - ie we can predict it solely from the structure of the network architecture.</p><h3 id=research-techniques>Research techniques<a hidden class=anchor aria-hidden=true href=#research-techniques>#</a></h3><p><strong>Circuits Discovery</strong><br>‚ÄÇ‚ÄÇFinding the circuit for a specific task attracts the attention of lots of researchers. The thing we wanna do is to get the relevant components for a specific task. A naive idea is to test the components one by one using <strong>causal intervention</strong>: change the value of one component while keeping others unchanged, and check if it influences the output. This technique is also called <strong>ablation</strong> or <strong>knockout</strong>.<br>‚ÄÇ‚ÄÇTo achieve this, we have two possible ways: denoising (find useful components) and noising (delete unuseful components). We usually prepare a <strong>clean prompt</strong> which is relevant to the task (results in a correct answer) and a <strong>corrupted prompt</strong> which has nothing to do with the task. Before finding circuits, the two prompts are fed into the model separately to get a clean run and a corrupted run.<br>‚ÄÇ‚ÄÇIf we use denoising, at each step we <strong>replace (patch)</strong> the value of a component in the corrupted run with that in the clean run. If the output is closer to the correct answer under a specific metric (e.g. KL divergence or logit difference), then we add the component into the circuit. If we use noising, then we should replace a component in the clean run with that in the corrupted run. If the output is almost unchanged under a threshod, then we regard the component as useless and delete it. Generally speaking, denoising is better than noising. To understand this, I want to quote a line in <a href>Arena</a>: <em>noising tells you what is necessary, denoising tells you what is sufficient.</em><br>Several techniques in this area:</p><ul><li><strong>activation patching</strong> (aka causal mediation/interchange interventions&mldr;) A method for circuits discovery that take nodes into consideration.</li><li><strong>path patching</strong> A variant of activation patching that also take edges into consideration to study which connections between components matter. For a pair of components A and B, we patch in the clean output of A, but only along paths that affect the input of component B. While in activation patching, all the subsequent components after A are affected.</li><li><strong>attribution patching</strong> An approximation of activation patching using a first-order Taylor expansion on the metric. This method is used to speed up circuits finding.</li></ul><center><img src=/mech_interp_resource/patching.png>
<font size=3>Figure 6: Comparsion of activation patching and path patching (from <a href=https://arena3-chapter1-transformer-interp.streamlit.app/[1.3]_Indirect_Object_Identification>Arena</a>)</font></center><p>‚ÄÇ‚ÄÇThe difference between activation patching and path pathcing are shown in Figure 2. In activation patching, we simply patch the node $D$ with $D&rsquo;$, so the nodes after $D$ ($H, G$ and $F$) are affected. While in path patching, we patch edges rather than nodes. For example, we only want to patch the edge $D \to G$, which means the only change is the information from node $D$ to node $G$. As a result, only $G$ and $F$ are affected while $H$ isn&rsquo;t.</p><h4 id=telescope-recommended-papers>&#x1f52d; Recommended papers:<a hidden class=anchor aria-hidden=true href=#telescope-recommended-papers>#</a></h4><ul><li><a href=https://arxiv.org/abs/2202.05262>(ROME) Locating and Editing Factual Associations in GPT</a></li><li><a href=https://arxiv.org/abs/2304.14997>(ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability</a></li><li><a href=https://arxiv.org/abs/2310.10348>(attribution patching) Attribution Patching Outperforms Automated Circuit Discovery</a></li><li><a href=https://arxiv.org/abs/2211.00593>(IOI) INTERPRETABILITY IN THE WILD: A CIRCUIT FOR INDIRECT OBJECT IDENTIFICATION IN GPT-2 SMALL</a></li></ul><hr><p><strong>Dictionary Learning</strong><br>‚ÄÇ‚ÄÇDictionary Learning aims to deal with the problem of superposition. The idea is like compression sensing in the field of signal processing and is discussed in <a href=https://transformer-circuits.pub/2023/superposition-composition/index.html>this article</a>. The implementation of dictionary learning is to train a sparse autoencoder (SAE).<br>‚ÄÇ‚ÄÇAn autoencoder consists of an encoder and a decoder. The encoder receives an input and compresses it to a lower dimension, and the decoder maps the hidden representation to the original input. The goal of the autoencoder is to get the representation of the input while <strong>compressing</strong> it. The autoencoder is optimized using the reconstruction loss.<br>‚ÄÇ‚ÄÇCompared with the autoencoedr, the dimension of the hidden representation in SAE is always <strong>higher</strong> than that of the input, which means the SAE does something completely opposite to the autoencoder. The idea behind is that the model dimension is smaller than the number of features to represent. The model may use superposition to make full use of limited neurons to represent more features. To get one-to-one correspondence between neurons and features, we map the representation to a higher dimensional vector space with SAE encoder. Once we get the representation in SAE (let&rsquo;s call it sparse features), we maps it back to the original input with SAE decoder.<br>‚ÄÇ‚ÄÇIn practice, any hidden state in a model can be studied using SAE. For example, when we want to get the sparse features of the activations $h$ in a MLP layer. We can do as follows:</p><p>$$ z = ReLU(W_{enc}h + b_{enc}) $$
$$ h^{\prime} = W_{dec}z + b_{dec} $$</p><p>$$ loss = \mathbb{E}_{h}\left[||h-h^{\prime}||_{2}^{2} + \lambda||z||\right] $$</p><p>‚ÄÇ‚ÄÇNote that $ h = [h_{1}, h_{2},&mldr;,h_{n}]^{T} \in \mathbb{R}^{n\times1} $ is a hidden state with $n$ dimensions, and each $h_{i} \in H$ is the value of a specific dimension $i$. $W_{enc} \in \mathbb{R}^{m\times n}$ maps the hidden state to a new vector space with dimension $m>n$, $W_{dec} \in \mathbb{R}^{n\times m}$ maps the sparse features back to the original shape, $ b_{enc} \in \mathbb{R}^{n} $ and $ b_{dec} \in \mathbb{R}^{n} $ are learned bias. The loss function consists of two parts: the MSE loss as the reconstruction loss and L1 norm with a coefficient $\lambda$ to encourage the sparsity of feature activations. It is the regularization term that separates SAE from ordinary autoencoders, so as to discourage superposition and encourage <strong>monosemanticity</strong>.<br>‚ÄÇ‚ÄÇTo better understanding the encoder and decoder in SAE, we can write a sparse feature $ f_{i} $ as an element of $ z $ :</p><p>$$ f_{i}(h) = z_{i} = ReLU(W^{enc}_{i,.}\cdot h + b^{enc}_{i}) $$</p><p>‚ÄÇ‚ÄÇEach sparse feature $ f_{i} $ is calculated using row $i$ of the encoder weight matrix. As for decoder, we can write $ h^{\prime} $ as:</p><p>$$ h^{\prime} = \sum_{i=1}^{m}f_{i}(h) \cdot W^{dec}_{.,i} + b_{dec} $$</p><p>‚ÄÇ‚ÄÇThe reconstructed activation $h^\prime$ can been seen as the <strong>addition</strong> of all the features. Each column of the decoder matrix corresponds to a feature, so we call it a &ldquo;feature direction&rdquo;. Note that sometimes the L1 norm term in the loss function can be replaced by $ \lambda\sum_{i=1}^{m}f_{i}(h)||W^{dec}_{.,i}||_{2} $ which places a constraint to the decoder weights to reduce ambiguity in the addition operation (we want only one or a few features to be large).<br>‚ÄÇ‚ÄÇFor simplicity, The details of the model structure, training method and evaluation will not be shown here.</p><h4 id=telescope-recommended-papers-1>&#x1f52d; Recommended papers:<a hidden class=anchor aria-hidden=true href=#telescope-recommended-papers-1>#</a></h4><ul><li><a href=https://transformer-circuits.pub/2023/monosemantic-features/index.html>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</a></li><li><a href=https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html>Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a></li></ul><hr><p><strong>Model Steering</strong><br>‚ÄÇ‚ÄÇA useful technique for eliciting certain model behaviors in a mechanistic way.</p><h4 id=telescope-recommended-papers-2>&#x1f52d; Recommended papers:<a hidden class=anchor aria-hidden=true href=#telescope-recommended-papers-2>#</a></h4><ul><li><a href=https://arxiv.org/abs/2308.10248>Activation Addition: Steering Language Models Without Optimization</a><ul><li><a href=https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector>(lesswrong) Steering GPT-2-XL by adding an activation vector</a></li></ul></li><li><a href=https://arxiv.org/abs/2312.06681>Steering Llama 2 via Contrastive Activation Addition</a></li><li><a href=https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1>Mechanistically Eliciting Latent Behaviors in Language Models</a></li></ul><h3 id=research-areas>Research Areas<a hidden class=anchor aria-hidden=true href=#research-areas>#</a></h3><p><strong>Theory</strong></p><ul><li><strong>Understand model components</strong></li><li><strong>Understand model behaviors</strong></li></ul><p><strong>Application</strong></p><ul><li><strong>interpretable model structure</strong></li><li><strong>AI alignment</strong><br>Avoid bias and harmful behaviors</li></ul><h2 id=some-useful-resources>Some Useful Resources<a hidden class=anchor aria-hidden=true href=#some-useful-resources>#</a></h2><p>Here I list some resources that would be helpful for you to get started quickly in the field.</p><h3 id=tutorials>Tutorials<a hidden class=anchor aria-hidden=true href=#tutorials>#</a></h3><ul><li><strong><a href=https://arena3-chapter1-transformer-interp.streamlit.app/#about-this-page>Arena</a></strong>‚ÄÇ‚ÄÇA tutorial created and maintained by Callum McDougall et al, providing a guided path for anyone who finds themselves overwhelmed by the amount of technical AI safety content out there.</li><li><strong><a href=https://www.neelnanda.io/mechanistic-interpretability/getting-started>Neel Nanda&rsquo;s Tutorial</a></strong>‚ÄÇ‚ÄÇNeel&rsquo;s tutorial for mech interp.</li><li><strong><a href=https://www.alignmentforum.org/posts/jLAvJt8wuSFySN975/mechanistic-interpretability-quickstart-guide>Neel Nanda&rsquo;s Quickstart Guide</a></strong>‚ÄÇ‚ÄÇA quick start for mech interp.</li><li><strong><a href=https://www.alignmentforum.org/posts/NfFST5Mio7BCAQHPA/an-extremely-opinionated-annotated-list-of-my-favourite-1>Neel Nanda&rsquo;s remommended papers</a></strong>‚ÄÇ‚ÄÇSome classic and important papers for mech interp.</li><li><strong><a href=https://www.alignmentforum.org/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability>Neel Nanda&rsquo;s problems v1</a></strong>‚ÄÇ‚ÄÇNeel&rsquo;s old questions for mech interp.</li><li><strong><a href="https://www.google.com/url?q=https%3A%2F%2Fdocs.google.com%2Fdocument%2Fd%2F1lIIzMjenXh-U0j5jkuqSDTawCoMNW4TqUlxk7mmbmRg%2Fedit">Neel Nanda&rsquo;s problems v2</a></strong>‚ÄÇ‚ÄÇNeel&rsquo;s 200 new questions for mech interp.</li><li><strong><a href=https://www.lesswrong.com/posts/PqMT9zGrNsGJNfiFR/alignment-research-field-guide>Alignment Research Field Guide (by the MIRI team)</a></strong>‚ÄÇ‚ÄÇ</li><li><strong><a href></a></strong>‚ÄÇ‚ÄÇ</li></ul><h3 id=frameworks-and-libraries>Frameworks and Libraries<a hidden class=anchor aria-hidden=true href=#frameworks-and-libraries>#</a></h3><ul><li><strong><a href=https://transformerlensorg.github.io/TransformerLens/>TransformerLens</a></strong>‚ÄÇ‚ÄÇA library maintained by Bryce Meyer and created by Neel Nanda.</li><li><strong><a href=https://jbloomaus.github.io/SAELens/>SAELens</a></strong>‚ÄÇ‚ÄÇOriginates from TransformerLens, and is separated from it because of the popularity and importance of SAE.</li><li><strong><a href=https://github.com/TransformerLensOrg/CircuitsVis>CircuitsVis</a></strong>‚ÄÇ‚ÄÇA good tool for visualizing LLMs.</li><li><strong><a href=https://www.perfectlynormal.co.uk/blog-plotly-widgets>Plotly</a></strong>‚ÄÇ‚ÄÇA good tool for plotting.</li></ul><h3 id=forums-and-communities>Forums and Communities<a hidden class=anchor aria-hidden=true href=#forums-and-communities>#</a></h3><ul><li><strong><a href=https://transformer-circuits.pub/>Transformer Circuits Thread</a></strong>‚ÄÇ‚ÄÇThe research posts of Anthropic alignment group.</li><li><strong><a href=https://www.lesswrong.com/>Lesswrong</a></strong>‚ÄÇ‚ÄÇ</li><li><strong><a href=https://www.alignmentforum.org/>AI Alignment Forum</a></strong>‚ÄÇ‚ÄÇ<br><strong><a href></a></strong>‚ÄÇ‚ÄÇ</li></ul><h3 id=companies-institutes-labs-and-programs>Companies, Institutes, Labs and Programs<a hidden class=anchor aria-hidden=true href=#companies-institutes-labs-and-programs>#</a></h3><ul><li><strong><a href=https://www.anthropic.com/>Anthropic</a></strong>‚ÄÇ‚ÄÇ</li><li><strong><a href=https://deepmind.google/>DeepMind</a></strong>‚ÄÇ‚ÄÇ</li><li><strong><a href=https://far.ai/>FAR</a></strong>‚ÄÇ‚ÄÇ</li><li><strong><a href=https://www.apolloresearch.ai/>Apollo</a></strong>‚ÄÇ‚ÄÇ</li><li><strong><a href=https://www.redwoodresearch.org/>RedWood</a></strong>‚ÄÇ‚ÄÇ</li><li><strong><a href=https://humancompatible.ai/>CHAI (UC Berkeley)</a></strong>‚ÄÇ‚ÄÇ</li><li><strong><a href=https://intelligence.org/>MIRI (UC Berkeley)</a></strong></li><li><strong><a href=https://www.alignment.org/>Alignment Research Center (ARC)</a></strong>‚ÄÇ‚ÄÇ</li><li><strong><a href=https://www.matsprogram.org/>MATS</a></strong>‚ÄÇ‚ÄÇAn independent research and educational seminar program that connects talented scholars with top mentors in the fields of AI alignment, interpretability, and governance.
<strong><a href></a></strong>‚ÄÇ‚ÄÇ</li></ul><h3 id=blogs>Blogs<a hidden class=anchor aria-hidden=true href=#blogs>#</a></h3><p><strong><a href=https://colah.github.io/>Chris Olah</a></strong>‚ÄÇ‚ÄÇ<br><strong><a href=https://www.neelnanda.io/>Neel Nanda</a></strong>‚ÄÇ‚ÄÇ<br><strong><a href=https://arthurconmy.github.io/>Arthur Conmy</a></strong>‚ÄÇ‚ÄÇ<br><strong><a href=https://www.trentonbricken.com/>Trenton Bricken</a></strong>‚ÄÇ‚ÄÇ<br><strong><a href=https://www.perfectlynormal.co.uk/>Callum Mcdougall</a></strong>‚ÄÇ‚ÄÇ<br><strong><a href=https://turntrout.com/about>Alex Turner(TurnTrout)</a></strong>‚ÄÇ‚ÄÇ<br><strong><a href></a></strong>‚ÄÇ‚ÄÇ
<strong><a href></a></strong>‚ÄÇ‚ÄÇ</p><p>&mldr;</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://Siriuslala.github.io/tags/mechanistic-interpretability/>Mechanistic Interpretability</a></li><li><a href=https://Siriuslala.github.io/tags/machine-learning/>Machine Learning</a></li></ul><nav class=paginav><a class=prev href=https://Siriuslala.github.io/posts/happy_feats/><span class=title>¬´ Prev</span><br><span>Exploring Emotional Features in GPT2-Small</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share A Brief Introduction to Mechanistic Interpretability Research on x" href="https://x.com/intent/tweet/?text=A%20Brief%20Introduction%20to%20Mechanistic%20Interpretability%20Research&amp;url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_resource%2f&amp;hashtags=mechanisticinterpretability%2cmachinelearning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Brief Introduction to Mechanistic Interpretability Research on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_resource%2f&amp;title=A%20Brief%20Introduction%20to%20Mechanistic%20Interpretability%20Research&amp;summary=A%20Brief%20Introduction%20to%20Mechanistic%20Interpretability%20Research&amp;source=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_resource%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Brief Introduction to Mechanistic Interpretability Research on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_resource%2f&title=A%20Brief%20Introduction%20to%20Mechanistic%20Interpretability%20Research"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Brief Introduction to Mechanistic Interpretability Research on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_resource%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Brief Introduction to Mechanistic Interpretability Research on whatsapp" href="https://api.whatsapp.com/send?text=A%20Brief%20Introduction%20to%20Mechanistic%20Interpretability%20Research%20-%20https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_resource%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Brief Introduction to Mechanistic Interpretability Research on telegram" href="https://telegram.me/share/url?text=A%20Brief%20Introduction%20to%20Mechanistic%20Interpretability%20Research&amp;url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_resource%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share A Brief Introduction to Mechanistic Interpretability Research on ycombinator" href="https://news.ycombinator.com/submitlink?t=A%20Brief%20Introduction%20to%20Mechanistic%20Interpretability%20Research&u=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_resource%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><div id=tw-comment></div><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="light"?"light":"dark",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"Siriuslala/siriuslala.github.io","data-repo-id":"R_kgDOMo4X2w","data-category":"Announcements","data-category-id":"DIC_kwDOMo4X284CiI_8","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":getStoredTheme(),"data-lang":"en","data-loading":"lazy",crossorigin:"anonymous"},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#tw-comment").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span>&copy; 2024 <a href=https://Siriuslala.github.io/>Siriuslala's Blog!</a></span> ¬∑
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>