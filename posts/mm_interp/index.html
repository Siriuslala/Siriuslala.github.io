<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Interpretability for Multimodal Models | Siriuslala's Blog!</title><meta name=keywords content="mechanistic interpretability,machine learning,multimodal"><meta name=description content="&#x1f4a1;
This post isÂ initially focused on interpretability for multimodal models, while later a lot of papers in other fields are included, just for convenience.
Resource
Interpretability for MLLMs

survey

A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models
Sparks of Explainability Recent Advancements in Explaining Large Vision Models
Awesome LMMs Mechanistic Interpretability


probing

Probing Multimodal Large Language Models for Global and Local Semantic Representations


representation

Zoom in: An introduction to circuits
Multimodal Neurons in Artificial Neural Networks
Interpreting CLIP&rsquo;s Image Representation via Text-Based Decomposition
Interpreting the Second-Order Effects of Neurons in CLIP
CLIPä¸åŒå±‚
Multimodal Neurons in Pretrained Text-Only Transformers


circuit

**(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models
Automatic Discovery of Visual Circuits
Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP


SAE

Case study: Interpreting, manipulating, and controlling clip with sparse autoencoders
Towards multimodal interpretability: Learning sparse interpretable features in vision transformers
Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery


visualization

Visualizerï¼ç®€åŒ–ä½ çš„Vision Transformerå¯è§†åŒ–ï¼
(DVT) Denoising Vision Transformers
Token Activation Map to Visually Explain Multimodal LLMs
LVLM-Intrepret: An Interpretability Tool for Large Vision Language Models
Transformer Interpretability Beyond Attention Visualization


others

**Towards interpreting visual information processing in vision-language models

demo


(dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability & Open Problems
Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space


information flow

**Cross-modal Information Flow in Multimodal Large Language Models
*From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks
*What&rsquo;s in the Image? A Deep-Dive into the Vision of Vision Language Models
The Narrow Gate: Localized Image-Text Communication in Vision-Language Models
Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models
Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference


analyses on MLLMs

Words or Vision: Do Vision-Language Models Have Blind Faith in Text?
Forgotten Polygons: Multimodal Large Language Models are Shape-Blind
Vision Transformers Need Registers
On the rankability of visual embeddings



Other fields of MLLMs


visual pretraining"><meta name=author content="Sirius"><link rel=canonical href=https://Siriuslala.github.io/posts/mm_interp/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.4413c2fe10c3ffde2ccee0ba56ebbe7434d016f0dab7817008f33227c9f3f0fa.css integrity="sha256-RBPC/hDD/94szuC6Vuu+dDTQFvDat4FwCPMyJ8nz8Po=" rel="preload stylesheet" as=style><link rel=icon href=https://Siriuslala.github.io/pig.svg><link rel=icon type=image/png sizes=16x16 href=https://Siriuslala.github.io/pig.svg><link rel=icon type=image/png sizes=32x32 href=https://Siriuslala.github.io/pig.svg><link rel=apple-touch-icon href=https://Siriuslala.github.io/pig.svg><link rel=mask-icon href=https://Siriuslala.github.io/pig.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Siriuslala.github.io/posts/mm_interp/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}.formula{width:100%;overflow-x:auto}</style><meta property="og:title" content="Interpretability for Multimodal Models"><meta property="og:description" content="&#x1f4a1;
This post isÂ initially focused on interpretability for multimodal models, while later a lot of papers in other fields are included, just for convenience.
Resource
Interpretability for MLLMs

survey

A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models
Sparks of Explainability Recent Advancements in Explaining Large Vision Models
Awesome LMMs Mechanistic Interpretability


probing

Probing Multimodal Large Language Models for Global and Local Semantic Representations


representation

Zoom in: An introduction to circuits
Multimodal Neurons in Artificial Neural Networks
Interpreting CLIP&rsquo;s Image Representation via Text-Based Decomposition
Interpreting the Second-Order Effects of Neurons in CLIP
CLIPä¸åŒå±‚
Multimodal Neurons in Pretrained Text-Only Transformers


circuit

**(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models
Automatic Discovery of Visual Circuits
Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP


SAE

Case study: Interpreting, manipulating, and controlling clip with sparse autoencoders
Towards multimodal interpretability: Learning sparse interpretable features in vision transformers
Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery


visualization

Visualizerï¼ç®€åŒ–ä½ çš„Vision Transformerå¯è§†åŒ–ï¼
(DVT) Denoising Vision Transformers
Token Activation Map to Visually Explain Multimodal LLMs
LVLM-Intrepret: An Interpretability Tool for Large Vision Language Models
Transformer Interpretability Beyond Attention Visualization


others

**Towards interpreting visual information processing in vision-language models

demo


(dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability & Open Problems
Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space


information flow

**Cross-modal Information Flow in Multimodal Large Language Models
*From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks
*What&rsquo;s in the Image? A Deep-Dive into the Vision of Vision Language Models
The Narrow Gate: Localized Image-Text Communication in Vision-Language Models
Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models
Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference


analyses on MLLMs

Words or Vision: Do Vision-Language Models Have Blind Faith in Text?
Forgotten Polygons: Multimodal Large Language Models are Shape-Blind
Vision Transformers Need Registers
On the rankability of visual embeddings



Other fields of MLLMs


visual pretraining"><meta property="og:type" content="article"><meta property="og:url" content="https://Siriuslala.github.io/posts/mm_interp/"><meta property="og:image" content="https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-02-25T15:08:53+08:00"><meta property="article:modified_time" content="2025-02-25T15:08:53+08:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Interpretability for Multimodal Models"><meta name=twitter:description content="&#x1f4a1;
This post isÂ initially focused on interpretability for multimodal models, while later a lot of papers in other fields are included, just for convenience.
Resource
Interpretability for MLLMs

survey

A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models
Sparks of Explainability Recent Advancements in Explaining Large Vision Models
Awesome LMMs Mechanistic Interpretability


probing

Probing Multimodal Large Language Models for Global and Local Semantic Representations


representation

Zoom in: An introduction to circuits
Multimodal Neurons in Artificial Neural Networks
Interpreting CLIP&rsquo;s Image Representation via Text-Based Decomposition
Interpreting the Second-Order Effects of Neurons in CLIP
CLIPä¸åŒå±‚
Multimodal Neurons in Pretrained Text-Only Transformers


circuit

**(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models
Automatic Discovery of Visual Circuits
Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP


SAE

Case study: Interpreting, manipulating, and controlling clip with sparse autoencoders
Towards multimodal interpretability: Learning sparse interpretable features in vision transformers
Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery


visualization

Visualizerï¼ç®€åŒ–ä½ çš„Vision Transformerå¯è§†åŒ–ï¼
(DVT) Denoising Vision Transformers
Token Activation Map to Visually Explain Multimodal LLMs
LVLM-Intrepret: An Interpretability Tool for Large Vision Language Models
Transformer Interpretability Beyond Attention Visualization


others

**Towards interpreting visual information processing in vision-language models

demo


(dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability & Open Problems
Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space


information flow

**Cross-modal Information Flow in Multimodal Large Language Models
*From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks
*What&rsquo;s in the Image? A Deep-Dive into the Vision of Vision Language Models
The Narrow Gate: Localized Image-Text Communication in Vision-Language Models
Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models
Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference


analyses on MLLMs

Words or Vision: Do Vision-Language Models Have Blind Faith in Text?
Forgotten Polygons: Multimodal Large Language Models are Shape-Blind
Vision Transformers Need Registers
On the rankability of visual embeddings



Other fields of MLLMs


visual pretraining"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Siriuslala.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Interpretability for Multimodal Models","item":"https://Siriuslala.github.io/posts/mm_interp/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Interpretability for Multimodal Models","name":"Interpretability for Multimodal Models","description":"\u0026#x1f4a1; This post isÂ initially focused on interpretability for multimodal models, while later a lot of papers in other fields are included, just for convenience.\nResource Interpretability for MLLMs survey A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models Sparks of Explainability Recent Advancements in Explaining Large Vision Models Awesome LMMs Mechanistic Interpretability probing Probing Multimodal Large Language Models for Global and Local Semantic Representations representation Zoom in: An introduction to circuits Multimodal Neurons in Artificial Neural Networks Interpreting CLIP\u0026rsquo;s Image Representation via Text-Based Decomposition Interpreting the Second-Order Effects of Neurons in CLIP CLIPä¸åŒå±‚ Multimodal Neurons in Pretrained Text-Only Transformers circuit **(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models Automatic Discovery of Visual Circuits Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP SAE Case study: Interpreting, manipulating, and controlling clip with sparse autoencoders Towards multimodal interpretability: Learning sparse interpretable features in vision transformers Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery visualization Visualizerï¼ç®€åŒ–ä½ çš„Vision Transformerå¯è§†åŒ–ï¼ (DVT) Denoising Vision Transformers Token Activation Map to Visually Explain Multimodal LLMs LVLM-Intrepret: An Interpretability Tool for Large Vision Language Models Transformer Interpretability Beyond Attention Visualization others **Towards interpreting visual information processing in vision-language models demo (dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability \u0026amp; Open Problems Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space information flow **Cross-modal Information Flow in Multimodal Large Language Models *From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks *What\u0026rsquo;s in the Image? A Deep-Dive into the Vision of Vision Language Models The Narrow Gate: Localized Image-Text Communication in Vision-Language Models Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference analyses on MLLMs Words or Vision: Do Vision-Language Models Have Blind Faith in Text? Forgotten Polygons: Multimodal Large Language Models are Shape-Blind Vision Transformers Need Registers On the rankability of visual embeddings Other fields of MLLMs visual pretraining\n","keywords":["mechanistic interpretability","machine learning","multimodal"],"articleBody":"ğŸ’¡ This post isÂ initially focused on interpretability for multimodal models, while later a lot of papers in other fields are included, just for convenience.\nResource Interpretability for MLLMs survey A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models Sparks of Explainability Recent Advancements in Explaining Large Vision Models Awesome LMMs Mechanistic Interpretability probing Probing Multimodal Large Language Models for Global and Local Semantic Representations representation Zoom in: An introduction to circuits Multimodal Neurons in Artificial Neural Networks Interpreting CLIPâ€™s Image Representation via Text-Based Decomposition Interpreting the Second-Order Effects of Neurons in CLIP CLIPä¸åŒå±‚ Multimodal Neurons in Pretrained Text-Only Transformers circuit **(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models Automatic Discovery of Visual Circuits Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP SAE Case study: Interpreting, manipulating, and controlling clip with sparse autoencoders Towards multimodal interpretability: Learning sparse interpretable features in vision transformers Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery visualization Visualizerï¼ç®€åŒ–ä½ çš„Vision Transformerå¯è§†åŒ–ï¼ (DVT) Denoising Vision Transformers Token Activation Map to Visually Explain Multimodal LLMs LVLM-Intrepret: An Interpretability Tool for Large Vision Language Models Transformer Interpretability Beyond Attention Visualization others **Towards interpreting visual information processing in vision-language models demo (dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability \u0026 Open Problems Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space information flow **Cross-modal Information Flow in Multimodal Large Language Models *From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks *Whatâ€™s in the Image? A Deep-Dive into the Vision of Vision Language Models The Narrow Gate: Localized Image-Text Communication in Vision-Language Models Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference analyses on MLLMs Words or Vision: Do Vision-Language Models Have Blind Faith in Text? Forgotten Polygons: Multimodal Large Language Models are Shape-Blind Vision Transformers Need Registers On the rankability of visual embeddings Other fields of MLLMs visual pretraining\nScaling Language-Free Visual Representation Learning spatial\ngood\nBeyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas (ViT+LLM \u003e ViT) Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision Encoder ! Learning Visual Composition through Improved Semantic Guidance (prompt-based) Things not Written in Text: Exploring Spatial Commonsense from Visual Signals (prompt-based) Does CLIP Bind Concepts? Probing Compositionality in Large Image Models Probing the Role of Positional Information in Vision-Language Models evaluation\nCan Multimodal Large Language Models Understand Spatial Relations? SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data SPHERE: Unveiling Spatial Blind Spots in Vision-Language Models Through Hierarchical Evaluation REC\n(çœ‹ related work) Exploring Spatial Language Grounding Through Referring Expressions (æ–‡æœ¬å¼•å¯¼) ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension attention\nLooking Beyond Text: Reducing Language bias in Large Vision-Language Models via Multimodal Dual-Attention and Soft-Image Guidance MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs Lost in the middle: How language models use long contexts Efficient streaming language models with attention sinks (delimiters) Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and Joint Low-Rank Projection MagicPIG: LSH Sampling for Efficient LLM Generation Label words are anchors: An information flow perspective for understanding in-context learning ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features POS â€œé—­é—¨é€ è½¦â€ä¹‹å¤šæ¨¡æ€æ€è·¯æµ…è°ˆï¼ˆä¸‰ï¼‰ï¼šä½ç½®ç¼–ç  ç¤¾åŒºä¾›ç¨¿ | å›¾è§£RoPEæ—‹è½¬ä½ç½®ç¼–ç åŠå…¶ç‰¹æ€§ Base of RoPE Bounds Context Length Sensitivity Meets Sparsity: The Impact of Extremely Sparse Parameter Patterns on Theory-of-Mind of Large Language Models Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding NoPE Transformer Language Models without Positional Encodings Still Learn Positional Information Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings The Impact of Positional Encoding on Length Generalization in Transformers (NoPE limits) Length Generalization of Causal Transformers without Position Encoding Long context Extending Context Window of Large Language Models via Positional Interpolation YaRN: Efficient Context Window Extension of Large Language Models hallucination\nsurvey Sirenâ€™s Song in the AI Ocean: A Survey on Hallucination in Large Language Models Hallucination of Multimodal Large Language Models: A Survey *Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs Interpreting and editing vision-language representations to mitigate hallucinations Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference Debiasing Multimodal Large Language Models token compression\nsurvey When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios methods (CDPruner) Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs *AdaFV: Rethinking of Visual-Language alignment for VLM acceleration (FasterVLM) [CLS] Attention is All You Need for Training-FreeVisual Token Pruning: Make VLM Inference Faster Sparsevlm: Visual token sparsification for efficient vision-language Model Inference (FastV) An image is worth 1/2 tokens after layer 2: Plug-and-PLay Acceleration for VLLM Inference LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token *Inference Optimal VLMs Need Only One Visual Token but Larger Models TokenPacker: Efficient Visual Projector for Multimodal LLM Matryoshka Multimodal Models Matryoshka Query Transformer for Large Vision-Language Models FlashSloth: Lightning Multimodal Large Language Models via Embedded Visual Compression FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding active perception Please refer to this post.\nDataset general VQA: Visual Question Answering repo download [(VQA v2.0) Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering] (GQA) GQA:ANewDataset for Real-World Visual Reasoning and Compositional Question Answering [website]https://cs.stanford.edu/people/dorarad/gqa/index.html (TextVQA) Towards VQA Models That Can Read website repo MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models repo MMBench: Is Your Multi-modal Model an All-around Player? spatial (ARO) When and why vision-language models behave like bags-of-words, and what to do about it? (Whatsup) Whatâ€™s â€œupâ€ with vision-language models? Investigating their struggle with spatial reasoning [repo]https://github.com/amitakamath/whatsup_vlms (VSR) Visual Spatial Reasoning [repo]https://github.com/cambridgeltl/visual-spatial-reasoning SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data (COMFORT) Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities hallucination (POPE) Evaluating object hallucination in large vision-language models repo (CHAIR) Object hallucination in image captioning repo (OpenCHAIR) Mitigating Open-Vocabulary Caption Hallucinations website Models llm\ngpt-oss Introducing gpt-oss | OpenAI OpenAIå¼€æºæ¨¡å‹gpt-oss-120bçš„å¦™å¦™å°è§‚å¯Ÿ å…³äºgpt-ossé‚£äº›å€¼å¾—å…³æ³¨çš„ç‚¹ vlm\nbasic (Transformer) Attention is all you need (VIT) An Image is Worth 16x16 Words: Transformers for Image Recognition fat Scale (CLIP) Learning Transferable Visual Models From Natural Language Supervision (SigLIP) Sigmoid Loss for Language Image Pre-Training (PACL) Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning (BLIP-2) BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond Qwen2-VL: Enhancing Vision-Language Modelâ€™s Perception of the World at Any Resolution Qwen2.5-VL Technical Report ä¸‡å­—é•¿æ–‡å›¾è§£Qwen2.5-VLå®ç°ç»†èŠ‚ (DFN) Data Filtering Networks LLaVAç³»åˆ— (LLaVA) Visual Instruction Tuning (LLaVA-1.5) Improved Baselines with Visual Instruction Tuning InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks (InternVL 1.5) How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites (InternVL 2.5) Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling visual contrastive learning (SimCLR) A Simple Framework for Contrastive Learning of Visual Representations (MoCo) Momentum Contrast for Unsupervised Visual Representation Learning BeiT (MAE) Masked Autoencoders Are Scalable Vision Learners (iBOT) Image BERT: A New Vision-Language Pre-training Framework (BYOL) Bootstrap Your Own Latent A New Approach to Self-Supervised Learning (DINO) Emerging Properties in Self-Supervised Vision Transformers DINOv2: Learning Robust Visual Features without Supervision DINOv3 Self-Supervised Learning è¶…è¯¦ç»†è§£è¯» (ç›®å½•) ä¸‡å­—é•¿æ–‡è¶…è¯¦è§£è¯»ä¹‹DINOå…¨ç³»åˆ—â€”è§†è§‰è¡¨å¾å¯¹æ¯”å­¦ä¹ çš„é«˜å³° ä¸‡å­—é•¿æ–‡è¶…è¯¦è§£ä¹‹DINO-V3ï¼ˆDINOå…¨ç³»åˆ—ä¹‹è¡¥å……ç¯‡ï¼‰ DINO\u0026DINO v2ï¼šé¢ è¦†è‡ªç›‘ç£è§†è§‰ç‰¹å¾è¡¨ç¤ºå­¦ä¹  resolution Patch nâ€™ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution Swin Transformer: Hierarchical Vision Transformer using Shifted Windows DualFocus: A Unified Framework for Integrating Positive and Negative Descriptors in Text-based Person Retrieval generative models\nè§†è§‰ç”Ÿæˆè¶…è¯¦ç»†è§£è¯» (ç›®å½•)\nimage\nbasic (T2I)\n(VAE) Auto-Encoding Variational Bayes vaeå’Œé‡å‚æ•°åŒ–æŠ€å·§ (VQGAN) Taming transformers for high-resolution image synthesis VQGAN è®ºæ–‡ä¸æºç è§£è¯»ï¼šå‰Diffusionæ—¶ä»£çš„é«˜æ¸…å›¾åƒç”Ÿæˆæ¨¡å‹ (LlamaGen) Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation (VAR) Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction (DDPM) Denoising Diffusion Probabilistic Models (DDIM) Denoising Diffusion Implicit Models (Classifier-guided) Diffusion Models Beat GANs on Image Synthesis (Classifier-free) Classifier-free diffusion guidance (DIT) Scalable Diffusion Models with Transformers (LDM) High-resolution image synthesis with latent diffusion models (GLIDE) GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models (Imagen) (DALL-E) Zero-Shot Text-to-Image Generation (DALL-E-2) Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E-3) Improving Image Generation with Better Captions (ControlNet) Adding Conditional Control to Text-to-Image Diffusion Models Consistency Models (LCM) Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference (SDXL Turbo) Adversarial Diffusion Distillation (EDM) Elucidating the Design Space of Diffusion-Based Generative Models Flow Matching for Generative Modeling (Stable Diffusion 3) Scaling Rectified Flow Transformers for High-Resolution Image Synthesis Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction Stable DiffusionæŠ€æœ¯è·¯çº¿å‘å±•å†ç¨‹å›é¡¾ Difussion Modelã€Flow Matching ä¸ Rectified Flow æµ…æ image editing\nInstructPix2Pix: Learning to Follow Image Editing Instructions Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry Diffusion Model-Based Image Editing: A Survey video\nsurvey\nThe Dawn of Video Generation: Preliminary Explorations with SORA-like Models generation\nVideo Diffusion Models Latte: Latent Diffusion Transformer for Video Generation Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning I2V\nvideo editing\nAnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks TokenFlow: Consistent Diffusion Features for Consistent Video Editing STABLEV2V: Stablizing Shape Consistency in Video-to-Video Editing VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing unified generation and comprehension\nsurvey Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities papers unified Chameleon: Mixed-Modal Early-Fusion Foundation Models VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding separate Janus-Pro Janus+Janus-Pro è®ºæ–‡è§£æ JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation (MetaQuery) Transfer between Modalities with MetaQueries (BAGEL) Emerging Properties in Unified Multimodal Pretraining Emu: Generative Pretraining in Multimodality (Emu2) Generative Multimodal Models are In-Context Learners Emu3: Next-Token Prediction is All You Need NExT-GPTï¼š Any-to-Any Multimodal LLM blogs 2025 å¹´ï¼Œç”Ÿæˆå’Œç†è§£å¤šæ¨¡æ€å¤§æ¨¡å‹ä¸€äº›æ€è€ƒ evaluation\nå›¾åƒç”Ÿæˆå¸¸ç”¨æŒ‡æ ‡ï¼šIS scoreå’ŒFID score AGI\nnew mllm archs Fuyu-8B: A Multimodal Architecture for AI Agents (SAIL) The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer å­—èŠ‚ SAIL è®ºæ–‡è§£è¯» world models blogs\n3D/4D World Modelï¼ˆWMï¼‰è¿‘æœŸå‘å±•çš„æ€»ç»“å’Œæ€è€ƒ VLA\nå¯¹VLAçš„RLæœ€æ–°è¿›å±•æ¢³ç† VLAæ‰©å……æ•°æ®æ¥æºæ–¹æ³•è¿›å±•æ¢³ç† JEPA\nJEPAï¼šè‡ªä¸»æœºå™¨æ™ºèƒ½çš„â€œè›‹ç³•èƒšâ€â€”â€”JEPAæ¨¡å‹å‘å±•è„‰ç»œæ¢³ç†ï¼ˆç¬¬ä¸€å¼¹ï¼‰ (I-JEPA) Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning world simulator\nA Summary â€œEmbodied AIâ€\nA Survey on Vision-Language-Action Models: An Action Tokenization Perspective navigation\nNavigation World Models physical reasoning\nDenoising Hamiltonian Network for Physical Reasoning ","wordCount":"2309","inLanguage":"en","image":"https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2025-02-25T15:08:53+08:00","dateModified":"2025-02-25T15:08:53+08:00","author":{"@type":"Person","name":"Sirius"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Siriuslala.github.io/posts/mm_interp/"},"publisher":{"@type":"Organization","name":"Siriuslala's Blog!","logo":{"@type":"ImageObject","url":"https://Siriuslala.github.io/pig.svg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Siriuslala.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://Siriuslala.github.io/pig.svg alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Siriuslala.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://Siriuslala.github.io/about/ title=About><span>About</span></a></li><li><a href=https://Siriuslala.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://Siriuslala.github.io/faq/ title=FAQ><span>FAQ</span></a></li><li><a href=https://Siriuslala.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://Siriuslala.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Siriuslala.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://Siriuslala.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Interpretability for Multimodal Models</h1><div class=post-meta><span title='2025-02-25 15:08:53 +0800 CST'>February 25, 2025</span>&nbsp;Â·&nbsp;5 min&nbsp;Â·&nbsp;2309 words&nbsp;Â·&nbsp;Sirius</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#resource>Resource</a><ul><li><a href=#interpretability-for-mllms>Interpretability for MLLMs</a></li><li><a href=#other-fields-of-mllms>Other fields of MLLMs</a></li></ul></li><li><a href=#dataset>Dataset</a></li><li><a href=#models>Models</a></li></ul></nav></div></details></div><div class=post-content><p>&#x1f4a1;
This post isÂ initially focused on interpretability for multimodal models, while later a lot of papers in other fields are included, just for convenience.</p><h2 id=resource>Resource<a hidden class=anchor aria-hidden=true href=#resource>#</a></h2><h3 id=interpretability-for-mllms>Interpretability for MLLMs<a hidden class=anchor aria-hidden=true href=#interpretability-for-mllms>#</a></h3><ul><li><strong>survey</strong><ul><li><a href=https://arxiv.org/pdf/2502.17516>A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models</a></li><li><a href=https://arxiv.org/pdf/2502.01048v1>Sparks of Explainability Recent Advancements in Explaining Large Vision Models</a></li><li><a href="https://github.com/itsqyh/Awesome-LMMs-Mechanistic-Interpretability?tab=readme-ov-file#-blog">Awesome LMMs Mechanistic Interpretability</a></li></ul></li><li><strong>probing</strong><ul><li><a href=https://arxiv.org/pdf/2402.17304>Probing Multimodal Large Language Models for Global and Local Semantic Representations</a></li></ul></li><li><strong>representation</strong><ul><li><a href="https://distill.pub/2020/circuits/zoom-in/?ref=cold-takes">Zoom in: An introduction to circuits</a></li><li><a href=https://distill.pub/2021/multimodal-neurons/>Multimodal Neurons in Artificial Neural Networks</a></li><li><a href=https://arxiv.org/pdf/2310.05916>Interpreting CLIP&rsquo;s Image Representation via Text-Based Decomposition</a></li><li><a href=https://arxiv.org/pdf/2406.04341>Interpreting the Second-Order Effects of Neurons in CLIP</a></li><li><a href=https://www.cnblogs.com/LittleHenry/p/18688886>CLIPä¸åŒå±‚</a></li><li><a href=https://openaccess.thecvf.com/content/ICCV2023W/CLVL/papers/Schwettmann_Multimodal_Neurons_in_Pretrained_Text-Only_Transformers_ICCVW_2023_paper.pdf>Multimodal Neurons in Pretrained Text-Only Transformers</a></li></ul></li><li><strong>circuit</strong><ul><li><a href=https://arxiv.org/pdf/2406.04236>**(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2404.14349>Automatic Discovery of Visual Circuits</a></li><li><a href=https://openaccess.thecvf.com/content/ICCV2023W/CLVL/papers/Palit_Towards_Vision-Language_Mechanistic_Interpretability_A_Causal_Tracing_Tool_for_BLIP_ICCVW_2023_paper.pdf>Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP</a></li></ul></li><li><strong>SAE</strong><ul><li><a href=https://www.lesswrong.com/posts/iYFuZo9BMvr6GgMs5/case-study-interpreting-manipulating-and-controlling-clip>Case study: Interpreting, manipulating, and controlling clip with sparse autoencoders</a></li><li><a href=https://www.lesswrong.com/posts/bCtbuWraqYTDtuARg/towards-multimodal-interpretability-learning-sparse-2>Towards multimodal interpretability: Learning sparse interpretable features in vision transformers</a></li><li><a href=https://arxiv.org/pdf/2407.14499>Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery</a></li></ul></li><li><strong>visualization</strong><ul><li><a href=https://zhuanlan.zhihu.com/p/398408338>Visualizerï¼ç®€åŒ–ä½ çš„Vision Transformerå¯è§†åŒ–ï¼</a></li><li><a href=https://arxiv.org/pdf/2401.02957>(DVT) Denoising Vision Transformers</a></li><li><a href=https://arxiv.org/pdf/2506.23270>Token Activation Map to Visually Explain Multimodal LLMs</a></li><li><a href=https://openaccess.thecvf.com/content/CVPR2024W/XAI4CV/papers/Stan_LVLM-Intrepret_An_Interpretability_Tool_for_Large_Vision-Language_Models_CVPRW_2024_paper.pdf>LVLM-Intrepret: An Interpretability Tool for Large Vision Language Models</a></li><li><a href=https://arxiv.org/pdf/2012.09838>Transformer Interpretability Beyond Attention Visualization</a></li></ul></li><li><strong>others</strong><ul><li><a href=https://arxiv.org/pdf/2410.07149>**Towards interpreting visual information processing in vision-language models</a><ul><li><a href=https://clementneo.com/llava_logit_lens/>demo</a></li></ul></li><li><a href=https://www.alignmentforum.org/posts/kobJymvvcvhbjWFKe/laying-the-foundations-for-vision-and-multimodal-mechanistic>(dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability & Open Problems</a></li><li><a href=https://arxiv.org/pdf/2203.14680>Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space</a></li></ul></li><li><strong>information flow</strong><ul><li><a href=https://arxiv.org/pdf/2411.18620>**Cross-modal Information Flow in Multimodal Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2406.06579>*From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks</a></li><li><a href=https://arxiv.org/pdf/2411.17491>*What&rsquo;s in the Image? A Deep-Dive into the Vision of Vision Language Models</a></li><li><a href=https://arxiv.org/pdf/2412.06646>The Narrow Gate: Localized Image-Text Communication in Vision-Language Models</a></li><li><a href>Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models</a></li><li><a href=https://arxiv.org/pdf/2503.13108>Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference</a></li></ul></li><li><strong>analyses on MLLMs</strong><ul><li><a href=https://arxiv.org/pdf/2503.02199>Words or Vision: Do Vision-Language Models Have Blind Faith in Text?</a></li><li><a href=https://arxiv.org/pdf/2502.15969>Forgotten Polygons: Multimodal Large Language Models are Shape-Blind</a></li><li><a href=https://arxiv.org/pdf/2309.16588>Vision Transformers Need Registers</a></li><li><a href=https://arxiv.org/pdf/2507.03683>On the rankability of visual embeddings</a></li></ul></li></ul><h3 id=other-fields-of-mllms>Other fields of MLLMs<a hidden class=anchor aria-hidden=true href=#other-fields-of-mllms>#</a></h3><ul><li><p><strong>visual pretraining</strong></p><ul><li><a href=https://arxiv.org/pdf/2504.01017?>Scaling Language-Free Visual Representation Learning</a></li></ul></li><li><p><strong>spatial</strong></p><ul><li><p>good</p><ul><li><a href=https://arxiv.org/pdf/2503.17349>Beyond Semantics: Rediscovering Spatial Awareness in Vision-Language Models</a></li><li><a href=https://arxiv.org/pdf/2503.01773>Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas</a></li><li><a href=https://arxiv.org/pdf/2411.05195>(ViT+LLM > ViT) Exploring How Generative MLLMs Perceive More Than CLIP with the Same Vision Encoder</a></li><li><a href=https://arxiv.org/pdf/2412.15396>! Learning Visual Composition through Improved Semantic Guidance</a></li><li><a href=https://arxiv.org/pdf/2203.08075>(prompt-based) Things not Written in Text: Exploring Spatial Commonsense from Visual Signals</a></li><li><a href=https://arxiv.org/pdf/2212.10537>(prompt-based) Does CLIP Bind Concepts? Probing Compositionality in Large Image Models</a></li><li><a href=https://arxiv.org/pdf/2305.10046>Probing the Role of Positional Information in Vision-Language Models</a></li></ul></li><li><p>evaluation</p><ul><li><a href=https://arxiv.org/pdf/2505.19015>Can Multimodal Large Language Models Understand Spatial Relations?</a></li><li><a href="https://openreview.net/pdf?id=MX9wmPhuke">SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data</a></li><li><a href=https://arxiv.org/pdf/2412.12693>SPHERE: Unveiling Spatial Blind Spots in Vision-Language Models Through Hierarchical Evaluation</a></li></ul></li><li><p>REC</p><ul><li><a href=https://arxiv.org/pdf/2502.04359>(çœ‹ related work) Exploring Spatial Language Grounding Through Referring Expressions</a></li><li><a href=https://arxiv.org/pdf/2204.05991>(æ–‡æœ¬å¼•å¯¼) ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension</a></li></ul></li></ul></li><li><p><strong>attention</strong></p><ul><li><a href=https://arxiv.org/abs/2411.14279>Looking Beyond Text: Reducing Language bias in Large Vision-Language Models via Multimodal Dual-Attention and Soft-Image Guidance</a></li><li><a href=https://arxiv.org/pdf/2502.17422>MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs</a></li><li><a href=https://arxiv.org/pdf/2307.03172>Lost in the middle: How language models use long contexts</a></li><li><a href=https://arxiv.org/pdf/2309.17453>Efficient streaming language models with attention sinks</a></li><li><a href=https://arxiv.org/pdf/2306.12929>(delimiters) Quantizable Transformers: Removing Outliers by Helping Attention Heads Do Nothing</a></li><li><a href=https://arxiv.org/abs/2503.01586>EliteKV: Scalable KV Cache Compression via RoPE Frequency Selection and Joint Low-Rank Projection</a></li><li><a href=https://arxiv.org/pdf/2410.16179>MagicPIG: LSH Sampling for Efficient LLM Generation</a></li><li><a href=https://arxiv.org/pdf/2305.14160>Label words are anchors: An information flow perspective for understanding in-context learning</a></li><li><a href=https://arxiv.org/pdf/2502.04320>ConceptAttention: Diffusion Transformers Learn Highly Interpretable Features</a></li><li><strong>POS</strong><ul><li><a href=https://kexue.fm/archives/10352>â€œé—­é—¨é€ è½¦â€ä¹‹å¤šæ¨¡æ€æ€è·¯æµ…è°ˆï¼ˆä¸‰ï¼‰ï¼šä½ç½®ç¼–ç </a></li><li><a href=https://hub.baai.ac.cn/view/32862>ç¤¾åŒºä¾›ç¨¿ | å›¾è§£RoPEæ—‹è½¬ä½ç½®ç¼–ç åŠå…¶ç‰¹æ€§</a></li><li><a href=https://arxiv.org/pdf/2405.14591>Base of RoPE Bounds Context Length</a></li><li><a href=https://arxiv.org/pdf/2504.04238>Sensitivity Meets Sparsity: The Impact of Extremely Sparse Parameter Patterns on Theory-of-Mind of Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2502.01563>Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding</a></li><li>NoPE<ul><li><a href=https://aclanthology.org/2022.findings-emnlp.99.pdf>Transformer Language Models without Positional Encodings Still Learn Positional Information</a></li><li><a href=https://aclanthology.org/2023.acl-short.102.pdf>Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings</a></li><li><a href=https://proceedings.neurips.cc/paper_files/paper/2023/file/4e85362c02172c0c6567ce593122d31c-Paper-Conference.pdf>The Impact of Positional Encoding on Length Generalization in Transformers</a></li><li><a href=https://arxiv.org/pdf/2404.12224>(NoPE limits) Length Generalization of Causal Transformers without Position Encoding</a></li></ul></li></ul></li><li><strong>Long context</strong><ul><li><a href>Extending Context Window of Large Language Models via Positional Interpolation</a></li><li><a href>YaRN: Efficient Context Window Extension of Large Language Models</a></li></ul></li></ul></li><li><p><strong>hallucination</strong></p><ul><li>survey<ul><li><a href=https://arxiv.org/pdf/2309.01219>Sirenâ€™s Song in the AI Ocean: A Survey on Hallucination in Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2404.18930>Hallucination of Multimodal Large Language Models: A Survey</a></li></ul></li><li><a href=https://openaccess.thecvf.com/content/CVPR2024/papers/Tong_Eyes_Wide_Shut_Exploring_the_Visual_Shortcomings_of_Multimodal_LLMs_CVPR_2024_paper.pdf>*Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs</a></li><li><a href=https://arxiv.org/pdf/2410.02762?>Interpreting and editing vision-language representations to mitigate hallucinations</a></li><li><a href=https://arxiv.org/pdf/2407.21771>Paying More Attention to Image: A Training-Free Method for Alleviating Hallucination in LVLMs</a></li><li><a href=https://arxiv.org/pdf/2502.20750>Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference</a></li><li><a href=https://arxiv.org/pdf/2403.05262>Debiasing Multimodal Large Language Models</a></li></ul></li><li><p><strong>token compression</strong></p><ul><li>survey<ul><li><a href=https://arxiv.org/pdf/2507.20198>When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token Compression across Images, Videos, and Audios</a></li></ul></li><li>methods<ul><li><a href=https://arxiv.org/pdf/2506.10967v1>(CDPruner) Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs</a></li><li><a href=https://arxiv.org/pdf/2501.09532>*AdaFV: Rethinking of Visual-Language alignment for VLM acceleration</a></li><li><a href=https://arxiv.org/pdf/2412.01818>(FasterVLM) [CLS] Attention is All You Need for Training-FreeVisual Token Pruning: Make VLM Inference Faster</a></li><li><a href=https://arxiv.org/pdf/2410.04417>Sparsevlm: Visual token sparsification for efficient vision-language Model Inference</a></li><li><a href=https://arxiv.org/pdf/2403.06764>(FastV) An image is worth 1/2 tokens after layer 2: Plug-and-PLay Acceleration for VLLM Inference</a></li><li><a href=https://arxiv.org/pdf/2501.03895>LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token</a></li><li><a href=https://arxiv.org/pdf/2411.03312>*Inference Optimal VLMs Need Only One Visual Token but Larger Models</a></li><li><a href=https://arxiv.org/pdf/2407.02392>TokenPacker: Efficient Visual Projector for Multimodal LLM</a></li><li><a href=https://arxiv.org/pdf/2405.17430>Matryoshka Multimodal Models</a></li><li><a href=https://arxiv.org/pdf/2405.19315>Matryoshka Query Transformer for Large Vision-Language Models</a></li><li><a href=https://arxiv.org/pdf/2412.04317>FlashSloth: Lightning Multimodal Large Language Models via Embedded Visual Compression</a></li><li><a href=https://arxiv.org/pdf/2501.16297>FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers</a></li><li><a href=https://arxiv.org/pdf/2407.14439>Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding</a></li></ul></li></ul></li><li><p><strong>active perception</strong>
Please refer to <a href=https://Siriuslala.github.io/posts/thinking_and_reasoning/>this post</a>.</p></li></ul><h2 id=dataset>Dataset<a hidden class=anchor aria-hidden=true href=#dataset>#</a></h2><ul><li><strong>general</strong><ul><li><a href=https://openaccess.thecvf.com/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf>VQA: Visual Question Answering</a><ul><li><a href=https://github.com/GT-Vision-Lab/VQA>repo</a></li><li><a href=https://visualqa.org/download.html>download</a></li></ul></li><li>[(VQA v2.0) Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering]</li><li><a href=https://arxiv.org/pdf/1902.09506>(GQA) GQA:ANewDataset for Real-World Visual Reasoning and Compositional Question Answering</a><ul><li>[website]https://cs.stanford.edu/people/dorarad/gqa/index.html</li></ul></li><li><a href=https://arxiv.org/pdf/1904.08920>(TextVQA) Towards VQA Models That Can Read</a><ul><li><a href=https://textvqa.org/>website</a></li><li><a href=https://github.com/facebookresearch/mmf>repo</a></li></ul></li><li><a href=https://arxiv.org/pdf/2306.13394>MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models</a><ul><li><a href=https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Evaluation>repo</a></li></ul></li><li><a href=https://arxiv.org/pdf/2307.06281>MMBench: Is Your Multi-modal Model an All-around Player?</a></li></ul></li><li><strong>spatial</strong><ul><li><a href=https://arxiv.org/pdf/2210.01936>(ARO) When and why vision-language models behave like bags-of-words, and what to do about it?</a></li><li><a href=https://aclanthology.org/2023.emnlp-main.568.pdf>(Whatsup) Whatâ€™s â€œupâ€ with vision-language models? Investigating their struggle with spatial reasoning</a><ul><li>[repo]https://github.com/amitakamath/whatsup_vlms</li></ul></li><li><a href=https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00566/116470>(VSR) Visual Spatial Reasoning</a><ul><li>[repo]https://github.com/cambridgeltl/visual-spatial-reasoning</li></ul></li><li><a href=https://arxiv.org/pdf/2504.20648>SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data</a></li><li><a href=https://arxiv.org/pdf/2410.17385>(COMFORT) Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities</a></li></ul></li><li><strong>hallucination</strong><ul><li><a href=https://arxiv.org/pdf/2305.10355>(POPE) Evaluating object hallucination in large vision-language models</a><ul><li><a href=https://github.com/AoiDragon/POPE>repo</a></li></ul></li><li><a href=https://arxiv.org/pdf/1809.02156>(CHAIR) Object hallucination in image captioning</a><ul><li><a href=https://github.com/LisaAnne/Hallucination/tree/master>repo</a></li></ul></li><li><a href=https://assafbk.github.io/mocha/paper.pdf>(OpenCHAIR) Mitigating Open-Vocabulary Caption Hallucinations</a><ul><li><a href=https://assafbk.github.io/mocha/>website</a></li></ul></li></ul></li></ul><h2 id=models>Models<a hidden class=anchor aria-hidden=true href=#models>#</a></h2><ul><li><p><strong>llm</strong></p><ul><li>gpt-oss<ul><li><a href=https://openai.com/zh-Hans-CN/index/introducing-gpt-oss/>Introducing gpt-oss | OpenAI</a></li><li><a href=https://zhuanlan.zhihu.com/p/1934722616544954132>OpenAIå¼€æºæ¨¡å‹gpt-oss-120bçš„å¦™å¦™å°è§‚å¯Ÿ</a></li><li><a href=https://yam.gift/2025/08/06/NLP/2025-08-06-gpt-oss/>å…³äºgpt-ossé‚£äº›å€¼å¾—å…³æ³¨çš„ç‚¹</a></li></ul></li></ul></li><li><p><strong>vlm</strong></p><ul><li>basic<ul><li><a href=https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>(Transformer) Attention is all you need</a></li><li><a href=https://arxiv.org/pdf/2010.11929>(VIT) An Image is Worth 16x16 Words: Transformers for Image Recognition fat Scale</a></li><li><a href=https://arxiv.org/pdf/2103.00020>(CLIP) Learning Transferable Visual Models From Natural Language Supervision</a></li><li><a href=https://arxiv.org/abs/2303.15343>(SigLIP) Sigmoid Loss for Language Image Pre-Training</a></li><li><a href=https://openaccess.thecvf.com/content/CVPR2023/papers/Mukhoti_Open_Vocabulary_Semantic_Segmentation_With_Patch_Aligned_Contrastive_Learning_CVPR_2023_paper.pdf>(PACL) Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning</a></li><li><a href=https://arxiv.org/pdf/2301.12597>(BLIP-2) BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2308.12966>Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</a></li><li><a href=https://arxiv.org/pdf/2409.12191>Qwen2-VL: Enhancing Vision-Language Model&rsquo;s Perception of the World at Any Resolution</a></li><li><a href=https://arxiv.org/abs/2502.13923>Qwen2.5-VL Technical Report</a><ul><li><a href=https://zhuanlan.zhihu.com/p/1921289925552210138>ä¸‡å­—é•¿æ–‡å›¾è§£Qwen2.5-VLå®ç°ç»†èŠ‚</a></li></ul></li><li><a href=https://arxiv.org/abs/2309.17425>(DFN) Data Filtering Networks</a></li><li><a href=https://zhuanlan.zhihu.com/p/692398098>LLaVAç³»åˆ—</a></li><li><a href=https://arxiv.org/pdf/2304.08485>(LLaVA) Visual Instruction Tuning</a></li><li><a href=https://static.hliu.cc/files/llava/improved_llava.pdf>(LLaVA-1.5) Improved Baselines with Visual Instruction Tuning</a></li><li><a href=https://arxiv.org/pdf/2312.14238>InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks</a></li><li><a href=https://arxiv.org/pdf/2404.16821>(InternVL 1.5) How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites</a></li><li><a href=https://arxiv.org/pdf/2412.05271>(InternVL 2.5) Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling</a></li></ul></li><li>visual contrastive learning<ul><li><a href=https://arxiv.org/pdf/2002.05709>(SimCLR) A Simple Framework for Contrastive Learning of Visual Representations</a></li><li><a href=https://arxiv.org/pdf/1911.05722>(MoCo) Momentum Contrast for Unsupervised Visual Representation Learning</a></li><li><a href>BeiT</a></li><li><a href=https://arxiv.org/pdf/2111.06377>(MAE) Masked Autoencoders Are Scalable Vision Learners</a></li><li><a href>(iBOT) Image BERT: A New Vision-Language Pre-training Framework</a></li><li><a href=https://arxiv.org/pdf/2006.07733>(BYOL) Bootstrap Your Own Latent A New Approach to Self-Supervised Learning</a></li><li><a href=https://arxiv.org/pdf/2104.14294>(DINO) Emerging Properties in Self-Supervised Vision Transformers</a></li><li><a href=https://arxiv.org/pdf/2304.07193>DINOv2: Learning Robust Visual Features without Supervision</a></li><li><a href=https://arxiv.org/pdf/2508.10104>DINOv3</a></li><li><a href=https://zhuanlan.zhihu.com/p/381354026>Self-Supervised Learning è¶…è¯¦ç»†è§£è¯» (ç›®å½•)</a></li><li><a href=https://zhuanlan.zhihu.com/p/1933583851923439816>ä¸‡å­—é•¿æ–‡è¶…è¯¦è§£è¯»ä¹‹DINOå…¨ç³»åˆ—â€”è§†è§‰è¡¨å¾å¯¹æ¯”å­¦ä¹ çš„é«˜å³°</a></li><li><a href=https://zhuanlan.zhihu.com/p/1940400858836742367>ä¸‡å­—é•¿æ–‡è¶…è¯¦è§£ä¹‹DINO-V3ï¼ˆDINOå…¨ç³»åˆ—ä¹‹è¡¥å……ç¯‡ï¼‰</a></li><li><a href=https://blog.csdn.net/weixin_45429089/article/details/142931708>DINO&amp;DINO v2ï¼šé¢ è¦†è‡ªç›‘ç£è§†è§‰ç‰¹å¾è¡¨ç¤ºå­¦ä¹ </a></li></ul></li><li>resolution<ul><li><a href=https://proceedings.neurips.cc/paper_files/paper/2023/file/06ea400b9b7cfce6428ec27a371632eb-Paper-Conference.pdf>Patch nâ€™ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution</a></li><li><a href=https://arxiv.org/pdf/2103.14030>Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></li><li><a href=https://arxiv.org/pdf/2405.07459v1>DualFocus: A Unified Framework for Integrating Positive and Negative Descriptors in Text-based Person Retrieval</a></li></ul></li></ul></li><li><p><strong>generative models</strong></p><ul><li><p><a href=https://zhuanlan.zhihu.com/p/687092760>è§†è§‰ç”Ÿæˆè¶…è¯¦ç»†è§£è¯» (ç›®å½•)</a></p></li><li><p><strong>image</strong></p><ul><li><p>basic (T2I)</p><ul><li><a href=https://arxiv.org/pdf/1312.6114>(VAE) Auto-Encoding Variational Bayes</a><ul><li><a href=https://zhuanlan.zhihu.com/p/570269617>vaeå’Œé‡å‚æ•°åŒ–æŠ€å·§</a></li></ul></li><li><a href=https://arxiv.org/pdf/2012.09841>(VQGAN) Taming transformers for high-resolution image synthesis</a><ul><li><a href=https://zhuanlan.zhihu.com/p/637705399>VQGAN è®ºæ–‡ä¸æºç è§£è¯»ï¼šå‰Diffusionæ—¶ä»£çš„é«˜æ¸…å›¾åƒç”Ÿæˆæ¨¡å‹</a></li></ul></li><li><a href=https://arxiv.org/pdf/2406.06525>(LlamaGen) Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation</a></li><li><a href=https://arxiv.org/pdf/2404.02905>(VAR) Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</a></li><li><a href=https://arxiv.org/pdf/2006.11239>(DDPM) Denoising Diffusion Probabilistic Models</a></li><li><a href=https://arxiv.org/pdf/2010.02502>(DDIM) Denoising Diffusion Implicit Models</a></li><li><a href=https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf>(Classifier-guided) Diffusion Models Beat GANs on Image Synthesis</a></li><li><a href=https://arxiv.org/pdf/2207.12598>(Classifier-free) Classifier-free diffusion guidance</a></li><li><a href=https://arxiv.org/pdf/2212.09748>(DIT) Scalable Diffusion Models with Transformers</a></li><li><a href=https://arxiv.org/pdf/2112.10752>(LDM) High-resolution image synthesis with latent diffusion models</a></li><li><a href=https://arxiv.org/pdf/2112.10741.pdf>(GLIDE) GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</a></li><li><a href>(Imagen)</a></li><li><a href=https://arxiv.org/pdf/2102.12092>(DALL-E) Zero-Shot Text-to-Image Generation</a></li><li><a href=https://arxiv.org/pdf/2204.06125>(DALL-E-2) Hierarchical Text-Conditional Image Generation with CLIP Latents</a></li><li><a href=https://cdn.openai.com/papers/dall-e-3.pdf>(DALL-E-3) Improving Image Generation with Better Captions</a></li><li><a href=https://arxiv.org/pdf/2302.05543>(ControlNet) Adding Conditional Control to Text-to-Image Diffusion Models</a></li><li><a href=https://arxiv.org/pdf/2303.01469>Consistency Models</a></li><li><a href=https://arxiv.org/pdf/2310.04378>(LCM) Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference</a></li><li><a href=https://static1.squarespace.com/static/6213c340453c3f502425776e/t/65663480a92fba51d0e1023f/1701197769659/adversarial_diffusion_distillation.pdf>(SDXL Turbo) Adversarial Diffusion Distillation</a></li><li><a href=https://arxiv.org/pdf/2206.00364>(EDM) Elucidating the Design Space of Diffusion-Based Generative Models</a></li><li><a href=https://arxiv.org/pdf/2210.02747>Flow Matching for Generative Modeling</a></li><li><a href=https://arxiv.org/pdf/2403.03206>(Stable Diffusion 3) Scaling Rectified Flow Transformers for High-Resolution Image Synthesis</a></li><li><a href=https://proceedings.neurips.cc/paper_files/paper/2024/file/9a24e284b187f662681440ba15c416fb-Paper-Conference.pdf>Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction</a></li><li><a href=https://www.cnblogs.com/tgltt/p/18517850>Stable DiffusionæŠ€æœ¯è·¯çº¿å‘å±•å†ç¨‹å›é¡¾</a></li><li><a href=https://blog.csdn.net/qq_52511048/article/details/149639505>Difussion Modelã€Flow Matching ä¸ Rectified Flow æµ…æ</a></li></ul></li><li><p>image editing</p><ul><li><a href=https://arxiv.org/pdf/2211.09800>InstructPix2Pix: Learning to Follow Image Editing Instructions</a></li><li><a href=https://arxiv.org/pdf/2211.12572>Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation</a></li><li><a href=https://arxiv.org/pdf/2311.12092>Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models</a></li><li><a href=https://arxiv.org/pdf/2307.12868>Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry</a></li><li><a href=https://arxiv.org/pdf/2402.17525>Diffusion Model-Based Image Editing: A Survey</a></li></ul></li></ul></li><li><p><strong>video</strong></p><ul><li><p>survey</p><ul><li><a href=https://arxiv.org/pdf/2410.05227>The Dawn of Video Generation: Preliminary Explorations with SORA-like Models</a></li></ul></li><li><p>generation</p><ul><li><a href>Video Diffusion Models</a></li><li><a href>Latte: Latent Diffusion Transformer for Video Generation</a></li><li><a href=https://arxiv.org/pdf/2304.08818>Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</a></li><li><a href=https://arxiv.org/pdf/2405.04233>Vidu: a Highly Consistent, Dynamic and Skilled Text-to-Video Generator with Diffusion Models</a></li><li><a href=https://arxiv.org/pdf/2212.11565>Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation</a></li><li><a href>Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets</a></li><li><a href=https://arxiv.org/pdf/2307.04725>AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning</a></li></ul></li><li><p>I2V</p><ul><li><a href></a></li></ul></li><li><p>video editing</p><ul><li><a href=https://arxiv.org/pdf/2403.14468>AnyV2V: A Tuning-Free Framework For Any Video-to-Video Editing Tasks</a></li><li><a href=https://arxiv.org/pdf/2307.10373>TokenFlow: Consistent Diffusion Features for Consistent Video Editing</a></li><li><a href=https://arxiv.org/pdf/2411.11045>STABLEV2V: Stablizing Shape Consistency in Video-to-Video Editing</a></li><li><a href=https://arxiv.org/pdf/2502.17258>VideoGrain: Modulating Space-Time Attention for Multi-grained Video Editing</a></li></ul></li></ul></li><li><p><strong>unified generation and comprehension</strong></p><ul><li>survey<ul><li><a href=https://arxiv.org/pdf/2505.02567v1>Unified Multimodal Understanding and Generation Models: Advances, Challenges, and Opportunities</a></li></ul></li><li>papers<ul><li>unified<ul><li><a href=https://arxiv.org/pdf/2405.09818>Chameleon: Mixed-Modal Early-Fusion Foundation Models</a></li><li><a href=https://arxiv.org/pdf/2409.04429>VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation</a></li><li><a href=https://openaccess.thecvf.com/content/CVPR2025/papers/Qu_TokenFlow_Unified_Image_Tokenizer_for_Multimodal_Understanding_and_Generation_CVPR_2025_paper.pdf>TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation</a></li><li><a href=https://arxiv.org/pdf/2411.17762v3>MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding</a></li></ul></li><li>separate<ul><li><a href=https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf>Janus-Pro</a><ul><li><a href=https://www.cnblogs.com/syy001/articles/18705255>Janus+Janus-Pro è®ºæ–‡è§£æ</a></li></ul></li><li><a href=https://arxiv.org/pdf/2411.07975>JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation</a></li><li><a href=https://arxiv.org/pdf/2506.10395>Pisces: An Auto-regressive Foundation Model for Image Understanding and Generation</a></li><li><a href=https://arxiv.org/pdf/2404.14396v2>SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation</a></li><li><a href=https://arxiv.org/pdf/2504.06256>(MetaQuery) Transfer between Modalities with MetaQueries</a></li><li><a href=https://arxiv.org/pdf/2505.14683>(BAGEL) Emerging Properties in Unified Multimodal Pretraining</a></li><li><a href=https://arxiv.org/pdf/2307.05222>Emu: Generative Pretraining in Multimodality</a></li><li><a href=https://arxiv.org/pdf/2312.13286>(Emu2) Generative Multimodal Models are In-Context Learners</a></li><li><a href=https://arxiv.org/pdf/2409.18869>Emu3: Next-Token Prediction is All You Need</a></li><li><a href>NExT-GPTï¼š Any-to-Any Multimodal LLM</a></li></ul></li></ul></li><li>blogs<ul><li><a href="https://www.xiaohongshu.com/explore/687243970000000012030620?app_platform=android&amp;ignoreEngage=true&amp;app_version=8.91.0&amp;share_from_user_hidden=true&amp;xsec_source=app_share&amp;type=normal&amp;xsec_token=CBiUx-K5xJLRbxS_5aCm9DiyTG7qBdBQsu_enUzpgIN-U=&amp;author_share=1&amp;xhsshare=WeixinSession&amp;shareRedId=N0wyNUk4SUA2NzUyOTgwNjY0OTc3SDhP&amp;apptime=1752466211&amp;share_id=ec19b5395b18402dbff92e06e4c07d0a&amp;share_channel=wechat">2025 å¹´ï¼Œç”Ÿæˆå’Œç†è§£å¤šæ¨¡æ€å¤§æ¨¡å‹ä¸€äº›æ€è€ƒ</a></li></ul></li></ul></li><li><p><strong>evaluation</strong></p><ul><li><a href=https://juejin.cn/post/7156610588137750564>å›¾åƒç”Ÿæˆå¸¸ç”¨æŒ‡æ ‡ï¼šIS scoreå’ŒFID score</a></li></ul></li></ul></li><li><p><strong>AGI</strong></p><ul><li><strong>new mllm archs</strong><ul><li><a href=https://www.adept.ai/blog/fuyu-8b>Fuyu-8B: A Multimodal Architecture for AI Agents</a></li><li><a href=https://arxiv.org/pdf/2504.10462>(SAIL) The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer</a><ul><li><a href=https://blog.csdn.net/qq_61786525/article/details/147384370>å­—èŠ‚ SAIL è®ºæ–‡è§£è¯»</a></li></ul></li></ul></li><li><strong>world models</strong><ul><li><p>blogs</p><ul><li><a href=https://mp.weixin.qq.com/s/I8oZQ3QeAXAKik3oPSCoOg>3D/4D World Modelï¼ˆWMï¼‰è¿‘æœŸå‘å±•çš„æ€»ç»“å’Œæ€è€ƒ</a></li></ul></li><li><p>VLA</p><ul><li><a href=https://zhuanlan.zhihu.com/p/1916810989434807458>å¯¹VLAçš„RLæœ€æ–°è¿›å±•æ¢³ç†</a></li><li><a href=https://zhuanlan.zhihu.com/p/1918618318333519462>VLAæ‰©å……æ•°æ®æ¥æºæ–¹æ³•è¿›å±•æ¢³ç†</a></li></ul></li><li><p>JEPA</p><ul><li><a href=https://zhuanlan.zhihu.com/p/1921300496779568394>JEPAï¼šè‡ªä¸»æœºå™¨æ™ºèƒ½çš„â€œè›‹ç³•èƒšâ€â€”â€”JEPAæ¨¡å‹å‘å±•è„‰ç»œæ¢³ç†ï¼ˆç¬¬ä¸€å¼¹ï¼‰</a></li><li><a href=https://arxiv.org/pdf/2301.08243>(I-JEPA) Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture</a></li><li><a href=https://arxiv.org/pdf/2506.09985>V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning</a></li></ul></li><li><p>world simulator</p><ul><li><a href=https://github.com/ALEEEHU/World-Simulator>A Summary</a></li></ul></li><li><p>&ldquo;Embodied AI&rdquo;</p><ul><li><a href=https://arxiv.org/pdf/2507.01925>A Survey on Vision-Language-Action Models: An Action Tokenization Perspective</a></li></ul></li><li><p>navigation</p><ul><li><a href=https://arxiv.org/pdf/2412.03572v1>Navigation World Models</a></li><li><a href></a></li></ul></li><li><p>physical reasoning</p><ul><li><a href=https://arxiv.org/pdf/2503.07596>Denoising Hamiltonian Network for Physical Reasoning</a></li></ul></li></ul></li></ul></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://Siriuslala.github.io/tags/mechanistic-interpretability/>Mechanistic Interpretability</a></li><li><a href=https://Siriuslala.github.io/tags/machine-learning/>Machine Learning</a></li><li><a href=https://Siriuslala.github.io/tags/multimodal/>Multimodal</a></li></ul><nav class=paginav><a class=prev href=https://Siriuslala.github.io/posts/llm-agent/><span class=title>Â« Prev</span><br><span>LLM Agents</span>
</a><a class=next href=https://Siriuslala.github.io/posts/circuit_tuning/><span class=title>Next Â»</span><br><span>Circuit-tuning: A Mechanistic Approach for Identifying Parameter Redundancy and Fine-tuning Neural Networks</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Interpretability for Multimodal Models on x" href="https://x.com/intent/tweet/?text=Interpretability%20for%20Multimodal%20Models&amp;url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmm_interp%2f&amp;hashtags=mechanisticinterpretability%2cmachinelearning%2cmultimodal"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Interpretability for Multimodal Models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmm_interp%2f&amp;title=Interpretability%20for%20Multimodal%20Models&amp;summary=Interpretability%20for%20Multimodal%20Models&amp;source=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmm_interp%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Interpretability for Multimodal Models on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmm_interp%2f&title=Interpretability%20for%20Multimodal%20Models"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Interpretability for Multimodal Models on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmm_interp%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Interpretability for Multimodal Models on whatsapp" href="https://api.whatsapp.com/send?text=Interpretability%20for%20Multimodal%20Models%20-%20https%3a%2f%2fSiriuslala.github.io%2fposts%2fmm_interp%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Interpretability for Multimodal Models on telegram" href="https://telegram.me/share/url?text=Interpretability%20for%20Multimodal%20Models&amp;url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmm_interp%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Interpretability for Multimodal Models on ycombinator" href="https://news.ycombinator.com/submitlink?t=Interpretability%20for%20Multimodal%20Models&u=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmm_interp%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><div id=tw-comment></div><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="light"?"light":"dark",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"Siriuslala/siriuslala.github.io","data-repo-id":"R_kgDOMo4X2w","data-category":"Announcements","data-category-id":"DIC_kwDOMo4X284CiI_8","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":getStoredTheme(),"data-lang":"en","data-loading":"lazy",crossorigin:"anonymous"},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#tw-comment").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span>&copy; 2025 <a href=https://Siriuslala.github.io/>Siriuslala's Blog!</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>