<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Possible Research Areas in Mechanistic Interpretability | Siriuslala's Blog!</title>
<meta name=keywords content="mechanistic interpretability,machine learning"><meta name=description content="The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic. Circuit Discovery Methods basic activation patching (causal mediation/interchange interventions&mldr;) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? &#x1f52d; resources inspirition Analyzing Multi-Head"><meta name=author content="Sirius"><link rel=canonical href=https://Siriuslala.github.io/posts/mech_interp_research/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.187da127e3564f8ca7db95950f88b9ebc2d4f26e4c83ebf6b5baafee59ff4ebd.css integrity="sha256-GH2hJ+NWT4yn25WVD4i568LU8m5Mg+v2tbqv7ln/Tr0=" rel="preload stylesheet" as=style><link rel=icon href=https://Siriuslala.github.io/pig.svg><link rel=icon type=image/png sizes=16x16 href=https://Siriuslala.github.io/pig.svg><link rel=icon type=image/png sizes=32x32 href=https://Siriuslala.github.io/pig.svg><link rel=apple-touch-icon href=https://Siriuslala.github.io/pig.svg><link rel=mask-icon href=https://Siriuslala.github.io/pig.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Siriuslala.github.io/posts/mech_interp_research/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}.formula{width:100%;overflow-x:auto}</style><meta property="og:title" content="Possible Research Areas in Mechanistic Interpretability"><meta property="og:description" content="The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic. Circuit Discovery Methods basic activation patching (causal mediation/interchange interventions&mldr;) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? &#x1f52d; resources inspirition Analyzing Multi-Head"><meta property="og:type" content="article"><meta property="og:url" content="https://Siriuslala.github.io/posts/mech_interp_research/"><meta property="og:image" content="https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-06T22:52:16+08:00"><meta property="article:modified_time" content="2024-09-06T22:52:16+08:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Possible Research Areas in Mechanistic Interpretability"><meta name=twitter:description content="The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic. Circuit Discovery Methods basic activation patching (causal mediation/interchange interventions&mldr;) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? &#x1f52d; resources inspirition Analyzing Multi-Head"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Siriuslala.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Possible Research Areas in Mechanistic Interpretability","item":"https://Siriuslala.github.io/posts/mech_interp_research/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Possible Research Areas in Mechanistic Interpretability","name":"Possible Research Areas in Mechanistic Interpretability","description":"The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic. Circuit Discovery Methods basic activation patching (causal mediation/interchange interventions\u0026hellip;) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? \u0026#x1f52d; resources inspirition Analyzing Multi-Head","keywords":["mechanistic interpretability","machine learning"],"articleBody":"The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.\nCircuit Discovery Methods basic activation patching (causal mediation/interchange interventions…) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? 🔭 resources inspirition Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned (ROME) Locating and Editing Factual Associations in GPT Attribution patching: Activation patching at industrial scale (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability Attribution Patching Outperforms Automated Circuit Discovery AtP*: An efficient and scalable method for localizing llm behaviour to components Causal Scrubbing: a method for rigorously testing interpretability hypotheses new Using SAE Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models Automatically Identifying Local and Global Circuits with Linear Computation Graphs Contextual Decomposition Mechanistic Interpretation through Contextual Decomposition in Transformers Edge Pruning ? Finding Transformer Circuits with Edge Pruning Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning Evaluation lack of ground truth\nhuman interpretability automatic faithfulness (see this)\nhow much of the full model’s performance can a circuit account for.\ncompleteness\ncomputational efficiency Hypothesis Testing the Circuit Hypothesis in LLMs Issues ablation methods: dropout out is also an ablation, so does zero ablation work? superposition, need the help of SAE? Dictionary Learning SAE\nTraining and optimization proper SAE width dead neurons 🔭 resources A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models Circuit Tracing: Revealing Computational Graphs in Language Models Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders Sparse Crosscoders for Cross-Layer Features and Model Diffing Open Source Replication of Anthropic’s Crosscoder paper for model-diffing Transcoders Find Interpretable LLM Feature Circuits Scaling and evaluating sparse autoencoders Improving Dictionary Learning with Gated Sparse Autoencoders Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Evaluation human auto 🔭 resources SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability website (Anthropic, 2024-8, contrastive eval \u0026 sort eval) Interpretability Evals for Dictionary Learning (RAVEL) RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations Language models can explain neurons in language models Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control Analysis A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders Are Sparse Autoencoders Useful? A Case Study in Sparse Probing Applications SAE + feature discovery 🔭 resources Sparse Autoencoders Find Highly Interpretable Features in Language Models Anthropic research Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet SAE + circuit discovery 🔭 resources Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models SAE + explain model components Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors Sparse Autoencoders Work on Attention Layer Outputs Interpreting Attention Layer Outputs with Sparse Autoencoders SAE + explain model behaviors The Geometry of Concepts: Sparse Autoencoder Feature Structure SAE + model steering SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders Steering vectors activation steering Steering Language Models With Activation Engineering (lesswrong) Steering GPT-2-XL by adding an activation vector Steering Llama 2 via Contrastive Activation Addition Inference-Time Intervention: Eliciting Truthful Answers from a Language Model Function vectors in large language models feature steering Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Evaluating feature steering: A case study in mitigating social biases representation engineering Representation Engineering: A Top-Down Approach to AI Transparency Others LUNAR: LLM Unlearning via Neural Activation Redirection Mechanistically Eliciting Latent Behaviors in Language Models Model Diffing Stage-wise fine-tuning fine-tuning interp fine-tuning\nscaling Stage-Wise Model Diffing Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks Understanding Catastrophic Forgetting in Language Models via Implicit Inference Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To! intrinsic dimension Measuring the Intrinsic Dimension of Objective Landscapes Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning Dataset-wise Modeldiff: A framework for comparing learning algorithms Algorithm-wise Adversarial robustness as a prior for learned representations Representation equivariance meta-SNE Visualizing Representations: Deep Learning and Human Beings model stitching Understanding image representations by measuring their equivariance and equivalence Revisiting model stitching to compare neural representations SVCCA and similar methods SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability others analyses The Platonic Representation Hypothesis theory When Representations Align: Universality in Representation Learning Dynamics Explain Model Components explain neurons, attention heads and circuits\nExplain neurons 🔭 resources Finding Neurons In A Haystack LatentQA: Teaching LLMs to Decode Activations Into Natural Language Language models can explain neurons in language models Multimodal Neurons in Artificial Neural Networks Finding Safety Neurons in Large Language Models Explain attention heads different heads in one layer/heads in different layer -\u003e grammar/semantic feats\nattention pattern\nAnalyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention 为什么Transformer 需要进行 Multi-head Attention？ special heads\nCopy Suppression: Comprehensively Understanding An Attention Head Explain circuits understand specific circuits on the subspace level\n(IOI) Interpretability in The Wild: A Circuit For Indirect Object Identification in GPT-2 Small What Do the Circuits Mean? A Knowledge Edit View (DAS) Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations LLM Circuit Analyses Are Consistent Across Training and Scale Explain layernorm On the Nonlinearity of Layer Normalization Others The Quantization Model of Neural Scaling Explain Model Behaviors Feature representations linear representations theory Linear Explanations for Individual Neurons multilingual representations Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs Emerging Cross-lingual Structure in Pretrained Language Models Probing the Emergence of Cross-lingual Alignment during LLM Training Exploring Alignment in Shared Cross-lingual Spaces mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models? Probing LLMs for Joint Encoding of Linguistic Categories Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure multimodal representations Interpreting the Second-Order Effects of Neurons in CLIP Interpreting CLIP’s Image Representation via Text-Based Decomposition safety reprs 🔭 resources Linear Representations of sentiment in large language models nonlinear representations Not All Language Model Features Are Linear Model capabilities training (learning) dynamics\nin-context learning\nbasic In-context Learning and Induction Heads What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation bad in-context learning (learn wrong things) Overthinking The Truth: Understanding How language Models Process False Demonstrations chain of thought (COT)\nhow and why step by step?\nzero-shot COT ???\nanalyses Iteration Head: A Mechanistic Study of Chain-of-Thought How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation Do Large Language Models Latently Perform Multi-Hop Reasoning? unfaithful COT Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting Does the model already know the answer while reasoning, or the model really has a goal? reasoning\nUnveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization How Do LLMs Perform Two-Hop Reasoning in Context? planning\nEvaluating Cognitive Maps and Planning in Large Language Models with CogEval instruction following\nhow does reinforcement learning change the inside of a model? understand RL at mechanistic level high efficient RLxF SAIF: A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models knowledge\n*Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models *(Entity Recognition and Hallucinations) On the Biology of a Large Language Model *Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level (Post 1) Dissecting Recall of Factual Associations in Auto-Regressive Language Models Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts Locating and Editing Factual Associations in GPT website Characterizing Mechanisms for Factual Recall in Language Models Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans The Geometry of Concepts: Sparse Autoencoder Feature Structure memorization \u0026 generalization phase transition\nWhat Do Learning Dynamics Reveal About Generalization in LLM Reasoning? In-context Learning and Induction Heads grokking Progress Measures For Grokking Via Mechanistic Interpretability Towards Understanding Grokking: An Effective Theory of Representation Learning learning dynamics\nLoss Landscape Degeneracy Drives Stagewise Development in Transformers Loss landscape geometry reveals stagewise development of transformers Multi-Component Learning and S-Curves Deep double descent duplication\nself-repair\nThe Hydra Effect: Emergent Self-repair in Language Model Computations Explorations of Self-Repair in Language Models massive activations\nMassive Activations in Large Language Models Systematic Outliers in Large Language Models Narrow tasks counting greater-than How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model Indirect Object Indentification (IOI) Interpretability in The Wild: A Circuit For Indirect Object Identification in GPT-2 Small gender Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias Interpretable model structure also called intrinsic interpretability\nModifying Model Components (SoLU) Softmax Linear Units Reengineering Model Architecture (Except interpretable model architectures, I also list some fresh-new architectures.)\nCBM (Concept Bottleneck Models)\nConcept Bottleneck Models Concept Bottleneck Large Language Models Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization Backpack Language Models\nBackpack Language Models others\nSeeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability SEER: Self-Explainability Enhancement of Large Language Models’ Representations Modular Training of Neural Networks aids Interpretability Compositional attention networks for machine reasoning new architetures\nA Path Towards Autonomous Machine Intelligence Large Concept Models: Language Modeling in a Sentence Representation Space Large Language Diffusion Models Fractal Generative Models (VLM single transformer) The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer JEPA Brain Inspired Findings Creations Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration Other Methods \u0026 Analysis decomposing a model Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition representation SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability (CKA) Similarity of Neural Network Representations Revisited 神经网络表征度量（一） Application AI alignment 3H (helpful, honest, harmless) Avoid bias and harmful behaviors\nconcept-based interpretability representation-based interpretability red-teaming perturbations\nbackdoor detection, red-teaming, capability discovery\nanomaly detection\nbackdoor detection\nSleeper Agents: Training Deceptive LLMs that Persist Through Safety Training SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks Mechanistic anomaly detection and ELK A gentle introduction to mechanistic anomaly detection Concrete empirical research projects in mechanistic anomaly detection refuse to request \u0026 jailbreak circuit; SAE; steering vector (anti-refusal)\nmeasures (repr engineering) Improving Alignment and Robustness with Circuit Breakers (steering) Refusal in Language Models Is Mediated by a Single Direction (steering) Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models (training) Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications analyses Universal and Transferable Adversarial Attacks on Aligned Language Models Many-shot jailbreaking Jailbroken: How Does LLM Safety Training Fail? Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs (vlm) Visual Adversarial Examples Jailbreak Aligned Large Language Models (vlm) Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models (vlm) Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything evaluation \u0026 benchmark Jailbreak prompts finding on Twitter JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models power-seeking\nParametrically Retargetable Decision-Makers Tend To Seek Power social injustice prejudice, gender bias: doctor \u0026 nurse, discrimination\ntraining dynamics; dataset; gradient descent; SAE circuits\nEvaluating feature steering: A case study in mitigating social biases Prejudice and Volatility: A Statistical Framework for Measuring Social Discrimination in Large Language Models Unveiling Gender Bias in Large Language Models: Using Teacher’s Evaluation in Higher Education As an Example deception\ndishonesty\nAlignment faking in large language models Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training Language Models Learn To Mislead Humans Via RLHF How do Large Language Models Navigate Conflicts between Honesty and Helpfulness? reward hacking\nReward Hacking in Reinforcement Learning measurement tampering\nThe AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome.\nBenchmarks for Detecting Measurement Tampering persona drift\nMeasuring and Controlling Persona Drift in Language Model Dialogs other human values\nSycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models Reducing sycophancy and improving honesty via activation steering Modulating sycophancy in an RLHF model via activation steering Agency\nUnderstanding and Controlling a Maze-Solving Policy Network Parametrically Retargetable Decision-Makers Tend To Seek Power Avoiding Side Effects in Complex Environments Alignmnet theory\nBoth alignment and interpretability are related to AI safety, so the mech interp tools are widely used in alignment research. I’ll put some good resources of alignment work here.\nqualitative work (findings, analyses, concepts, …) * alignment representation * instrumental convergence * shard theory\nProposed by Alex Turner (TurnTrout) and Quintin Pope The Shard Theory of Human Values (lesswrong) The Shard Theory of Human Values 🔭 resources AI Alignment: A Comprehensive Survey Representation Engineering: A Top-Down Approach to AI Transparency Mechanistic Interpretability for AI Safety A Review Improved Algorithms ReFT: Representation Finetuning for Language Models Harmonic Loss Trains Interpretable AI Models Research Limitations Current work mainly focuses on Transformer-based models. Is transformer a inevitable model structure for generative language models? How can we use post-hoc methods as a guide for training a more interpretable and controllable model? Other Interpretability Fields Neural network interpretability Not necessarily a transformer-based model, maybe an lstm or simply a toy model\nTheories for DL foocker/deeplearningtheory Feature Learning Huang wei’s repo game (chess) Evidence of Learned Look-Ahead in a Chess-Playing Neural Network (Sokoban, planning) Planning behavior in a recurrent neural network that plays Sokoban geometry Reasoning in Large Language Models: A Geometric Perspective Bertology A Primer in BERTology: What we know about how BERT works What do you mean, BERT? Assessing BERT as a Distributional Semantics Model Other Surveys Open Problems in Mechanistic Interpretability A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models Mechanistic Interpretability for AI Safety: A Review A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models Towards Uncovering How Large Language Model Works: An Explainability Perspective ","wordCount":"2463","inLanguage":"en","image":"https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-09-06T22:52:16+08:00","dateModified":"2024-09-06T22:52:16+08:00","author":{"@type":"Person","name":"Sirius"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Siriuslala.github.io/posts/mech_interp_research/"},"publisher":{"@type":"Organization","name":"Siriuslala's Blog!","logo":{"@type":"ImageObject","url":"https://Siriuslala.github.io/pig.svg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Siriuslala.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://Siriuslala.github.io/pig.svg alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Siriuslala.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://Siriuslala.github.io/about/ title=About><span>About</span></a></li><li><a href=https://Siriuslala.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://Siriuslala.github.io/faq/ title=FAQ><span>FAQ</span></a></li><li><a href=https://Siriuslala.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://Siriuslala.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Siriuslala.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://Siriuslala.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Possible Research Areas in Mechanistic Interpretability</h1><div class=post-meta><span title='2024-09-06 22:52:16 +0800 CST'>September 6, 2024</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;2463 words&nbsp;·&nbsp;Sirius</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#the-purpose-i-write-this-blog>The Purpose I Write This Blog</a></li><li><a href=#circuit-discovery>Circuit Discovery</a><ul><li><a href=#methods>Methods</a></li><li><a href=#evaluation>Evaluation</a></li><li><a href=#issues>Issues</a></li></ul></li><li><a href=#dictionary-learning>Dictionary Learning</a><ul><li><a href=#training-and-optimization>Training and optimization</a></li><li><a href=#evaluation-1>Evaluation</a></li><li><a href=#analysis>Analysis</a></li><li><a href=#applications>Applications</a></li></ul></li><li><a href=#steering-vectors>Steering vectors</a><ul><li><a href=#activation-steering>activation steering</a></li><li><a href=#feature-steering>feature steering</a></li><li><a href=#representation-engineering>representation engineering</a></li><li><a href=#others>Others</a></li></ul></li><li><a href=#model-diffing>Model Diffing</a><ul><li><a href=#stage-wise>Stage-wise</a></li><li><a href=#dataset-wise>Dataset-wise</a></li><li><a href=#algorithm-wise>Algorithm-wise</a></li><li><a href=#representation-equivariance>Representation equivariance</a></li></ul></li><li><a href=#explain-model-components>Explain Model Components</a><ul><li><a href=#explain-neurons>Explain neurons</a></li><li><a href=#explain-attention-heads>Explain attention heads</a></li><li><a href=#explain-circuits>Explain circuits</a></li><li><a href=#explain-layernorm>Explain layernorm</a></li><li><a href=#others-1>Others</a></li></ul></li><li><a href=#explain-model-behaviors>Explain Model Behaviors</a><ul><li><a href=#feature-representations>Feature representations</a></li><li><a href=#model-capabilities>Model capabilities</a></li><li><a href=#narrow-tasks>Narrow tasks</a></li></ul></li><li><a href=#interpretable-model-structure>Interpretable model structure</a><ul><li><a href=#modifying-model-components>Modifying Model Components</a></li><li><a href=#reengineering-model-architecture>Reengineering Model Architecture</a></li><li><a href=#brain-inspired>Brain Inspired</a></li></ul></li><li><a href=#other-methods--analysis>Other Methods & Analysis</a><ul><li><a href=#decomposing-a-model>decomposing a model</a></li><li><a href=#representation>representation</a></li></ul></li><li><a href=#application>Application</a><ul><li><a href=#ai-alignment>AI alignment</a></li><li><a href=#improved-algorithms>Improved Algorithms</a></li></ul></li><li><a href=#research-limitations>Research Limitations</a></li><li><a href=#other-interpretability-fields>Other Interpretability Fields</a><ul><li><a href=#neural-network-interpretability>Neural network interpretability</a></li></ul></li><li><a href=#other-surveys>Other Surveys</a></li></ul></nav></div></details></div><div class=post-content><h2 id=the-purpose-i-write-this-blog>The Purpose I Write This Blog<a hidden class=anchor aria-hidden=true href=#the-purpose-i-write-this-blog>#</a></h2><p>   To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.</p><h2 id=circuit-discovery>Circuit Discovery<a hidden class=anchor aria-hidden=true href=#circuit-discovery>#</a></h2><h3 id=methods>Methods<a hidden class=anchor aria-hidden=true href=#methods>#</a></h3><ul><li><strong>basic</strong><ul><li>activation patching (causal mediation/interchange interventions&mldr;)</li><li>path patching</li><li>scaling techinques: attribution patching</li><li>DAS (distributed alignment search) directional activation patching?</li></ul><h4 id=telescope-resources>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources>#</a></h4><ul><li>inspirition<ul><li><a href=https://arxiv.org/pdf/1905.09418>Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</a></li></ul></li><li><a href=https://arxiv.org/abs/2202.05262>(ROME) Locating and Editing Factual Associations in GPT</a></li><li><a href=https://www.neelnanda.io/mechanistic-interpretability/attribution-patching>Attribution patching: Activation patching at industrial scale</a></li><li><a href=https://arxiv.org/abs/2304.14997>(ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability</a></li><li><a href=https://arxiv.org/abs/2310.10348>Attribution Patching Outperforms Automated Circuit Discovery</a></li><li><a href=https://arxiv.org/pdf/2403.00745>AtP*: An efficient and scalable method for localizing llm behaviour to components</a></li><li><a href=https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing>Causal Scrubbing: a method for rigorously testing interpretability hypotheses</a></li></ul></li><li><strong>new</strong><ul><li>Using SAE<ul><li><a href=https://arxiv.org/abs/2403.19647>Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models</a></li><li><a href=https://arxiv.org/pdf/2405.13868>Automatically Identifying Local and Global Circuits with Linear Computation Graphs</a></li></ul></li><li>Contextual Decomposition<ul><li><a href=https://arxiv.org/abs/2407.00886>Mechanistic Interpretation through Contextual Decomposition in Transformers</a></li></ul></li><li>Edge Pruning ?<ul><li><a href=https://arxiv.org/pdf/2406.16778>Finding Transformer Circuits with Edge Pruning</a></li></ul></li><li><a href=https://arxiv.org/abs/2407.03779>Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning</a></li><li><a href></a></li></ul></li></ul><h3 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h3><p>lack of ground truth</p><ul><li><strong>human</strong><ul><li>interpretability</li></ul></li><li><strong>automatic</strong>
faithfulness (see <a href=https://arxiv.org/pdf/2403.19647>this</a>)<br>how much of the full model’s performance can a circuit account for.<br>completeness<br>computational efficiency<ul><li><a href=https://arxiv.org/pdf/2410.13032>Hypothesis Testing the Circuit Hypothesis in LLMs</a></li></ul></li></ul><h3 id=issues>Issues<a hidden class=anchor aria-hidden=true href=#issues>#</a></h3><ul><li>ablation methods: dropout out is also an ablation, so does zero ablation work?</li><li>superposition, need the help of SAE?</li></ul><h2 id=dictionary-learning>Dictionary Learning<a hidden class=anchor aria-hidden=true href=#dictionary-learning>#</a></h2><p>SAE</p><h3 id=training-and-optimization>Training and optimization<a hidden class=anchor aria-hidden=true href=#training-and-optimization>#</a></h3><ul><li>proper SAE width</li><li>dead neurons</li></ul><h4 id=telescope-resources-1>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources-1>#</a></h4><ul><li><a href=https://arxiv.org/pdf/2503.05613>A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models</a></li><li><a href=https://transformer-circuits.pub/2025/attribution-graphs/methods.html>Circuit Tracing: Revealing Computational Graphs in Language Models</a></li><li><a href=https://arxiv.org/pdf/2407.14435>Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders</a></li><li><a href=https://transformer-circuits.pub/2024/crosscoders/index.html>Sparse Crosscoders for Cross-Layer Features and Model Diffing</a><ul><li><a href=https://www.alignmentforum.org/posts/srt6JXsRMtmqAJavD/open-source-replication-of-anthropic-s-crosscoder-paper-for>Open Source Replication of Anthropic’s Crosscoder paper for model-diffing</a></li></ul></li><li><a href=https://arxiv.org/abs/2406.11944>Transcoders Find Interpretable LLM Feature Circuits</a></li><li><a href=https://arxiv.org/pdf/2406.04093>Scaling and evaluating sparse autoencoders</a></li><li><a href=https://arxiv.org/abs/2404.16014>Improving Dictionary Learning with Gated Sparse Autoencoders</a></li><li><a href=https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html>Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a></li><li><a href=https://transformer-circuits.pub/2023/monosemantic-features/index.html>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</a></li><li><a href></a></li></ul><h3 id=evaluation-1>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation-1>#</a></h3><ul><li><strong>human</strong></li><li><strong>auto</strong></li></ul><h4 id=telescope-resources-2>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources-2>#</a></h4><ul><li><a href=https://arxiv.org/pdf/2503.09532>SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability</a><ul><li><a href=https://www.neuronpedia.org/sae-bench/info>website</a></li></ul></li><li><a href=https://transformer-circuits.pub/2024/august-update/index.html>(Anthropic, 2024-8, contrastive eval & sort eval) Interpretability Evals for Dictionary Learning</a></li><li><a href>(RAVEL) RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations</a></li><li><a href=https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html>Language models can explain neurons in language models</a></li><li><a href=https://arxiv.org/abs/2405.08366>Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control</a></li><li><a href></a></li></ul><h3 id=analysis>Analysis<a hidden class=anchor aria-hidden=true href=#analysis>#</a></h3><ul><li><a href=https://arxiv.org/pdf/2409.14507>A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders</a></li><li><a href=https://arxiv.org/pdf/2502.16681>Are Sparse Autoencoders Useful? A Case Study in Sparse Probing</a></li></ul><h3 id=applications>Applications<a hidden class=anchor aria-hidden=true href=#applications>#</a></h3><ul><li><strong>SAE + feature discovery</strong><h4 id=telescope-resources-3>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources-3>#</a></h4><ul><li><a href>Sparse Autoencoders Find Highly Interpretable Features in Language Models</a></li><li>Anthropic research<ul><li><a href=https://transformer-circuits.pub/2023/monosemantic-features/index.html>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</a></li><li><a href=https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html>Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a></li><li><a href></a></li></ul></li></ul></li><li><strong>SAE + circuit discovery</strong><h4 id=telescope-resources-4>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources-4>#</a></h4><ul><li><a href>Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models</a></li><li><a href></a></li></ul></li><li><strong>SAE + explain model components</strong><ul><li><a href=https://arxiv.org/pdf/2103.15949>Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors</a></li><li><a href=https://www.lesswrong.com/posts/DtdzGwFh9dCfsekZZ/sparse-autoencoders-work-on-attention-layer-outputs>Sparse Autoencoders Work on Attention Layer Outputs</a></li><li><a href=https://arxiv.org/abs/2406.17759>Interpreting Attention Layer Outputs with Sparse Autoencoders</a></li></ul></li><li><strong>SAE + explain model behaviors</strong><ul><li><a href=https://arxiv.org/pdf/2410.19750>The Geometry of Concepts: Sparse Autoencoder Feature Structure</a></li></ul></li><li><strong>SAE + model steering</strong><ul><li><a href=https://arxiv.org/pdf/2501.18052>SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders</a></li></ul></li></ul><h2 id=steering-vectors>Steering vectors<a hidden class=anchor aria-hidden=true href=#steering-vectors>#</a></h2><h3 id=activation-steering>activation steering<a hidden class=anchor aria-hidden=true href=#activation-steering>#</a></h3><ul><li><a href=https://arxiv.org/abs/2308.10248>Steering Language Models With Activation Engineering</a><ul><li><a href=https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector>(lesswrong) Steering GPT-2-XL by adding an activation vector</a></li></ul></li><li><a href=https://arxiv.org/abs/2312.06681>Steering Llama 2 via Contrastive Activation Addition</a></li><li><a href=https://arxiv.org/abs/2306.03341>Inference-Time Intervention: Eliciting Truthful Answers from a Language Model</a></li><li><a href=https://arxiv.org/pdf/2310.15213>Function vectors in large language models</a></li></ul><h3 id=feature-steering>feature steering<a hidden class=anchor aria-hidden=true href=#feature-steering>#</a></h3><ul><li><a href=https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#assessing-interp>Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a></li><li><a href=https://www.anthropic.com/research/evaluating-feature-steering>Evaluating feature steering: A case study in mitigating social biases</a></li></ul><h3 id=representation-engineering>representation engineering<a hidden class=anchor aria-hidden=true href=#representation-engineering>#</a></h3><ul><li><a href=https://arxiv.org/abs/2310.01405>Representation Engineering: A Top-Down Approach to AI Transparency</a></li></ul><h3 id=others>Others<a hidden class=anchor aria-hidden=true href=#others>#</a></h3><ul><li><a href=https://arxiv.org/pdf/2502.07218>LUNAR: LLM Unlearning via Neural Activation Redirection</a></li><li><a href=https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1>Mechanistically Eliciting Latent Behaviors in Language Models</a></li></ul><h2 id=model-diffing>Model Diffing<a hidden class=anchor aria-hidden=true href=#model-diffing>#</a></h2><h3 id=stage-wise>Stage-wise<a hidden class=anchor aria-hidden=true href=#stage-wise>#</a></h3><ul><li><strong>fine-tuning</strong><ul><li>fine-tuning interp
fine-tuning<br>scaling<ul><li><a href=https://transformer-circuits.pub/2024/model-diffing/index.html>Stage-Wise Model Diffing</a></li><li><a href=https://arxiv.org/pdf/2502.11812>Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis</a></li><li><a href=https://arxiv.org/pdf/2402.14811>Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking</a></li><li><a href=https://arxiv.org/pdf/2401.01967>A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity</a></li><li><a href=https://arxiv.org/pdf/2311.12786>Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks</a></li><li><a href=https://arxiv.org/pdf/2309.10105>Understanding Catastrophic Forgetting in Language Models via Implicit Inference</a></li><li><a href=https://arxiv.org/pdf/2404.04392>Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes</a></li><li><a href="https://openreview.net/pdf?id=hTEGyKf0dZ">Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</a></li></ul></li><li>intrinsic dimension<ul><li><a href=https://arxiv.org/pdf/1804.08838>Measuring the Intrinsic Dimension of Objective Landscapes</a></li><li><a href=https://arxiv.org/pdf/2012.13255>Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</a></li></ul></li></ul></li></ul><h3 id=dataset-wise>Dataset-wise<a hidden class=anchor aria-hidden=true href=#dataset-wise>#</a></h3><ul><li><a href=https://proceedings.mlr.press/v202/shah23a/shah23a.pdf>Modeldiff: A framework for comparing learning algorithms</a></li></ul><h3 id=algorithm-wise>Algorithm-wise<a hidden class=anchor aria-hidden=true href=#algorithm-wise>#</a></h3><ul><li><a href=https://arxiv.org/pdf/1906.00945>Adversarial robustness as a prior for learned representations</a></li></ul><h3 id=representation-equivariance>Representation equivariance<a hidden class=anchor aria-hidden=true href=#representation-equivariance>#</a></h3><ul><li><strong>meta-SNE</strong><ul><li><a href=http://colah.github.io/posts/2015-01-Visualizing-Representations/>Visualizing Representations: Deep Learning and Human Beings</a></li></ul></li><li><strong>model stitching</strong><ul><li><a href=https://arxiv.org/pdf/1411.5908>Understanding image representations by measuring their equivariance and equivalence</a></li><li><a href=https://arxiv.org/pdf/2106.07682>Revisiting model stitching to compare neural representations</a></li></ul></li><li><strong>SVCCA and similar methods</strong><ul><li><a href=https://proceedings.neurips.cc/paper_files/paper/2017/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf>SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability</a></li></ul></li><li><strong>others analyses</strong><ul><li><a href=https://arxiv.org/pdf/2405.07987>The Platonic Representation Hypothesis</a></li></ul></li><li><strong>theory</strong><ul><li><a href=https://arxiv.org/pdf/2402.09142>When Representations Align: Universality in Representation Learning Dynamics</a></li></ul></li></ul><h2 id=explain-model-components>Explain Model Components<a hidden class=anchor aria-hidden=true href=#explain-model-components>#</a></h2><p>explain neurons, attention heads and circuits</p><h3 id=explain-neurons>Explain neurons<a hidden class=anchor aria-hidden=true href=#explain-neurons>#</a></h3><h4 id=telescope-resources-5>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources-5>#</a></h4><ul><li><a href=https://arxiv.org/pdf/2305.01610>Finding Neurons In A Haystack</a></li><li><a href=https://arxiv.org/pdf/2412.08686>LatentQA: Teaching LLMs to Decode Activations Into Natural Language</a></li><li><a href=https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html>Language models can explain neurons in language models</a></li><li><a href=https://distill.pub/2021/multimodal-neurons/>Multimodal Neurons in Artificial Neural Networks</a></li><li><a href>Finding Safety Neurons in Large Language Models</a></li><li><a href></a></li></ul><h3 id=explain-attention-heads>Explain attention heads<a hidden class=anchor aria-hidden=true href=#explain-attention-heads>#</a></h3><p>different heads in one layer/heads in different layer -> grammar/semantic feats</p><ul><li><p><strong>attention pattern</strong></p><ul><li><a href=https://arxiv.org/pdf/1905.09418>Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</a></li><li><a href=https://arxiv.org/pdf/2407.02490>MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</a></li><li><a href=https://www.zhihu.com/question/341222779/answer/3054459222>为什么Transformer 需要进行 Multi-head Attention？</a></li></ul></li><li><p><strong>special heads</strong></p><ul><li><a href=https://arxiv.org/abs/2310.04625>Copy Suppression: Comprehensively Understanding An Attention Head</a></li></ul></li></ul><h3 id=explain-circuits>Explain circuits<a hidden class=anchor aria-hidden=true href=#explain-circuits>#</a></h3><p>understand specific circuits on the subspace level</p><ul><li><a href=https://arxiv.org/abs/2211.00593>(IOI) Interpretability in The Wild: A Circuit For Indirect Object Identification in GPT-2 Small</a></li><li><a href=https://arxiv.org/pdf/2406.17241>What Do the Circuits Mean? A Knowledge Edit View</a></li><li><a href>(DAS) Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations</a></li><li><a href=https://arxiv.org/pdf/2407.10827>LLM Circuit Analyses Are Consistent Across Training and Scale</a></li></ul><h3 id=explain-layernorm>Explain layernorm<a hidden class=anchor aria-hidden=true href=#explain-layernorm>#</a></h3><ul><li><a href="https://openreview.net/pdf?id=18f6iPn0zq">On the Nonlinearity of Layer Normalization</a></li></ul><h3 id=others-1>Others<a hidden class=anchor aria-hidden=true href=#others-1>#</a></h3><ul><li><a href=https://arxiv.org/abs/2303.13506>The Quantization Model of Neural Scaling</a></li></ul><h2 id=explain-model-behaviors>Explain Model Behaviors<a hidden class=anchor aria-hidden=true href=#explain-model-behaviors>#</a></h2><h3 id=feature-representations>Feature representations<a hidden class=anchor aria-hidden=true href=#feature-representations>#</a></h3><ul><li><strong>linear representations</strong><ul><li>theory<ul><li><a href="https://openreview.net/pdf?id=WIbntm28cM">Linear Explanations for Individual Neurons</a></li></ul></li><li><font color=green>multilingual representations</font><ul><li><a href=https://arxiv.org/pdf/2406.09265>Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs</a></li><li><a href=https://arxiv.org/pdf/1911.01464>Emerging Cross-lingual Structure in Pretrained Language Models</a></li><li><a href=https://arxiv.org/pdf/2406.13229>Probing the Emergence of Cross-lingual Alignment during LLM Training</a></li><li><a href=https://arxiv.org/pdf/2405.14535>Exploring Alignment in Shared Cross-lingual Spaces</a></li><li><a href=https://arxiv.org/pdf/2404.12444>mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models?</a></li><li><a href=https://arxiv.org/pdf/2310.18696>Probing LLMs for Joint Encoding of Linguistic Categories</a></li><li><a href=https://arxiv.org/pdf/2203.08430>Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure</a></li><li><a href></a></li><li><a href></a></li></ul></li><li><font color=green>multimodal representations</font><ul><li><a href=https://arxiv.org/pdf/2406.04341>Interpreting the Second-Order Effects of Neurons in CLIP</a></li><li><a href=https://arxiv.org/pdf/2310.05916>Interpreting CLIP&rsquo;s Image Representation via Text-Based Decomposition</a></li></ul></li><li><font color=green>safety reprs</font></li></ul><h4 id=telescope-resources-6>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources-6>#</a></h4><ul><li><a href=https://arxiv.org/abs/2310.15154>Linear Representations of sentiment in large language models</a></li></ul></li><li><strong>nonlinear representations</strong><ul><li><a href>Not All Language Model Features Are Linear</a></li></ul></li></ul><h3 id=model-capabilities>Model capabilities<a hidden class=anchor aria-hidden=true href=#model-capabilities>#</a></h3><p>training (learning) dynamics</p><ul><li><p><strong>in-context learning</strong></p><ul><li>basic<ul><li><a href=https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html>In-context Learning and Induction Heads</a></li><li><a href="https://openreview.net/pdf?id=O8rrXl71D5">What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation</a></li></ul></li><li>bad in-context learning (learn wrong things)<ul><li><a href=https://arxiv.org/pdf/2307.09476>Overthinking The Truth: Understanding How language Models Process False Demonstrations</a></li></ul></li></ul></li><li><p><strong>chain of thought (COT)</strong><br>how and why step by step?<br>zero-shot COT ???</p><ul><li>analyses<ul><li><a href=https://arxiv.org/pdf/2406.02128>Iteration Head: A Mechanistic Study of Chain-of-Thought</a></li><li><a href=https://arxiv.org/pdf/2402.18312>How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning</a></li><li><a href=https://arxiv.org/pdf/2406.12288>An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs</a></li><li><a href=https://arxiv.org/pdf/2402.18344>Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning</a></li><li><a href=https://arxiv.org/pdf/2406.12255>A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning</a></li><li><a href=https://arxiv.org/pdf/2405.15302>Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation</a></li><li><a href=https://arxiv.org/pdf/2402.16837>Do Large Language Models Latently Perform Multi-Hop Reasoning?</a></li></ul></li><li>unfaithful COT<ul><li><a href=https://arxiv.org/abs/2305.04388>Language Models Don’t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</a></li></ul></li><li>Does the model already know the answer while reasoning, or the model really has a goal?</li></ul></li><li><p><strong>reasoning</strong></p><ul><li><a href=https://arxiv.org/pdf/2502.04667>Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization</a></li><li><a href=https://arxiv.org/pdf/2502.13913>How Do LLMs Perform Two-Hop Reasoning in Context?</a></li></ul></li><li><p><strong>planning</strong></p><ul><li><a href=https://arxiv.org/abs/2309.15129>Evaluating Cognitive Maps and Planning in Large Language Models with CogEval</a></li></ul></li><li><p><strong>instruction following</strong></p><ul><li><font color=green>how does reinforcement learning change the inside of a model?</font><ul><li>understand RL at mechanistic level</li><li>high efficient RLxF</li></ul></li><li><a href=https://arxiv.org/abs/2502.11356>SAIF: A Sparse Autoencoder Framework for Interpreting and Steering Instruction Following of Language Models</a></li></ul></li><li><p><strong>knowledge</strong></p><ul><li><a href=https://arxiv.org/pdf/2411.14257>*Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models</a></li><li><a href=https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-hallucinations>*(Entity Recognition and Hallucinations) On the Biology of a Large Language Model</a></li><li><a href="h'https://www.alignmentforum.org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall">*Fact Finding: Attempting to Reverse-Engineer Factual Recall on the Neuron Level (Post 1)</a></li><li><a href=https://arxiv.org/pdf/2304.14767>Dissecting Recall of Factual Associations in Auto-Regressive Language Models</a></li><li><a href=https://arxiv.org/pdf/2412.04614>Extractive Structures Learned in Pretraining Enable Generalization on Finetuned Facts</a></li><li><a href=https://arxiv.org/pdf/2202.05262>Locating and Editing Factual Associations in GPT</a><ul><li><a href=https://rome.baulab.info/>website</a></li></ul></li><li><a href=https://arxiv.org/pdf/2310.15910>Characterizing Mechanisms for Factual Recall in Language Models</a></li><li><a href=https://arxiv.org/pdf/2502.15090>Analyze the Neurons, not the Embeddings: Understanding When and Where LLM Representations Align with Humans</a></li><li><a href=https://arxiv.org/pdf/2410.19750>The Geometry of Concepts: Sparse Autoencoder Feature Structure</a></li></ul></li><li><p><strong>memorization & generalization</strong>
phase transition</p><ul><li><a href=https://arxiv.org/pdf/2411.07681>What Do Learning Dynamics Reveal About Generalization in LLM Reasoning?</a></li><li><a href=https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html>In-context Learning and Induction Heads</a></li><li>grokking<ul><li><a href=https://arxiv.org/abs/2301.05217>Progress Measures For Grokking Via Mechanistic Interpretability</a></li><li><a href>Towards Understanding Grokking: An Effective Theory of Representation Learning</a></li></ul></li></ul></li><li><p><strong>learning dynamics</strong></p><ul><li><a href=https://arxiv.org/pdf/2402.02364>Loss Landscape Degeneracy Drives Stagewise Development in Transformers</a></li><li><a href="https://openreview.net/forum?id=2JabyZjM5H">Loss landscape geometry reveals stagewise development of transformers</a></li><li><a href=https://www.alignmentforum.org/posts/RKDQCB6smLWgs2Mhr/multi-component-learning-and-s-curves>Multi-Component Learning and S-Curves</a></li><li><a href=https://openai.com/index/deep-double-descent/>Deep double descent</a></li></ul></li><li><p><strong>duplication</strong></p></li><li><p><strong>self-repair</strong></p><ul><li><a href=https://arxiv.org/abs/2307.15771>The Hydra Effect: Emergent Self-repair in Language Model Computations</a></li><li><a href=https://arxiv.org/abs/2402.15390v1>Explorations of Self-Repair in Language Models</a></li></ul></li><li><p><strong>massive activations</strong></p><ul><li><a href=https://arxiv.org/pdf/2402.17762>Massive Activations in Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2502.06415>Systematic Outliers in Large Language Models</a></li></ul></li></ul><h3 id=narrow-tasks>Narrow tasks<a hidden class=anchor aria-hidden=true href=#narrow-tasks>#</a></h3><ul><li>counting</li><li>greater-than<ul><li><a href=https://arxiv.org/pdf/2305.00586>How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model</a></li></ul></li><li>Indirect Object Indentification<ul><li><a href=https://arxiv.org/abs/2211.00593>(IOI) Interpretability in The Wild: A Circuit For Indirect Object Identification in GPT-2 Small</a></li></ul></li><li>gender<ul><li><a href=https://arxiv.org/pdf/2004.12265>Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias</a></li></ul></li></ul><h2 id=interpretable-model-structure>Interpretable model structure<a hidden class=anchor aria-hidden=true href=#interpretable-model-structure>#</a></h2><p>also called intrinsic interpretability</p><h3 id=modifying-model-components>Modifying Model Components<a hidden class=anchor aria-hidden=true href=#modifying-model-components>#</a></h3><ul><li><a href=https://transformer-circuits.pub/2022/solu/index.html>(SoLU) Softmax Linear Units</a></li></ul><h3 id=reengineering-model-architecture>Reengineering Model Architecture<a hidden class=anchor aria-hidden=true href=#reengineering-model-architecture>#</a></h3><p>(Except interpretable model architectures, I also list some fresh-new architectures.)</p><ul><li><p><strong>CBM (Concept Bottleneck Models)</strong></p><ul><li><a href=https://arxiv.org/pdf/2007.04612>Concept Bottleneck Models</a></li><li><a href=https://arxiv.org/pdf/2412.07992>Concept Bottleneck Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2502.13632>Concept Layers: Enhancing Interpretability and Intervenability via LLM Conceptualization</a></li></ul></li><li><p><strong>Backpack Language Models</strong></p><ul><li><a href=https://aclanthology.org/2023.acl-long.506.pdf>Backpack Language Models</a></li></ul></li><li><p><strong>others</strong></p><ul><li><a href=https://arxiv.org/pdf/2305.08746>Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability</a></li><li><a href=https://arxiv.org/pdf/2502.05242>SEER: Self-Explainability Enhancement of Large Language Models’ Representations</a></li><li><a href=https://arxiv.org/pdf/2502.02470>Modular Training of Neural Networks aids Interpretability</a></li><li><a href=https://arxiv.org/pdf/1803.03067>Compositional attention networks for machine reasoning</a></li></ul></li><li><p><strong>new architetures</strong></p><ul><li><a href="https://openreview.net/pdf?id=BZ5a1r-kVsf">A Path Towards Autonomous Machine Intelligence</a></li><li><a href=https://arxiv.org/pdf/2412.08821>Large Concept Models: Language Modeling in a Sentence Representation Space</a></li><li><a href=https://arxiv.org/pdf/2502.09992>Large Language Diffusion Models</a></li><li><a href=https://arxiv.org/pdf/2502.17437>Fractal Generative Models</a></li><li><a href=https://arxiv.org/pdf/2504.10462>(VLM single transformer) The Scalability of Simplicity: Empirical Analysis of Vision-Language Learning with a Single Transformer</a></li><li><a href>JEPA</a></li></ul></li></ul><h3 id=brain-inspired>Brain Inspired<a hidden class=anchor aria-hidden=true href=#brain-inspired>#</a></h3><ul><li><strong>Findings</strong></li><li><strong>Creations</strong><ul><li><a href=https://arxiv.org/pdf/2502.10699>Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration</a></li></ul></li></ul><h2 id=other-methods--analysis>Other Methods & Analysis<a hidden class=anchor aria-hidden=true href=#other-methods--analysis>#</a></h2><h3 id=decomposing-a-model>decomposing a model<a hidden class=anchor aria-hidden=true href=#decomposing-a-model>#</a></h3><ul><li><a href=https://arxiv.org/pdf/2501.14926>Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-based Parameter Decomposition</a></li></ul><h3 id=representation>representation<a hidden class=anchor aria-hidden=true href=#representation>#</a></h3><ul><li><a href=https://proceedings.neurips.cc/paper_files/paper/2017/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf>SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability</a></li><li><a href=https://arxiv.org/pdf/1905.00414>(CKA) Similarity of Neural Network Representations Revisited</a></li><li><a href=https://zhuanlan.zhihu.com/p/534165786>神经网络表征度量（一）</a></li></ul><h2 id=application>Application<a hidden class=anchor aria-hidden=true href=#application>#</a></h2><h3 id=ai-alignment>AI alignment<a hidden class=anchor aria-hidden=true href=#ai-alignment>#</a></h3><p>3H (helpful, honest, harmless)
Avoid bias and harmful behaviors<br>concept-based interpretability
representation-based interpretability<br>red-teaming
perturbations<br>backdoor detection, red-teaming, capability discovery</p><ul><li><p><strong>anomaly detection</strong><br>backdoor detection</p><ul><li><a href=https://arxiv.org/abs/2401.05566>Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</a></li><li><a href=https://arxiv.org/pdf/2405.11575>SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks</a></li><li><a href=https://www.alignmentforum.org/posts/vwt3wKXWaCvqZyF74/mechanistic-anomaly-detection-and-elk>Mechanistic anomaly detection and ELK</a></li><li><a href=https://www.lesswrong.com/posts/n7DFwtJvCzkuKmtbG/a-gentle-introduction-to-mechanistic-anomaly-detection>A gentle introduction to mechanistic anomaly detection</a></li><li><a href=https://www.lesswrong.com/s/GiZ6puwmHozLuBrph/p/99gWh9jxeumcmuduw>Concrete empirical research projects in mechanistic anomaly detection</a></li></ul></li><li><p><font color=green><b>refuse to request & jailbreak</b></font><br>circuit; SAE; steering vector (anti-refusal)</p><ul><li>measures<ul><li><a href=https://arxiv.org/pdf/2406.04313>(repr engineering) Improving Alignment and Robustness with Circuit Breakers</a></li><li><a href=https://arxiv.org/abs/2406.11717>(steering) Refusal in Language Models Is Mediated by a Single Direction</a></li><li><a href=https://arxiv.org/pdf/2406.09289>(steering) Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2402.05162>(training) Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications</a></li></ul></li><li>analyses<ul><li><a href=https://arxiv.org/pdf/2307.15043>Universal and Transferable Adversarial Attacks on Aligned Language Models</a></li><li><a href=https://www-cdn.anthropic.com/af5633c94ed2beb282f6a53c595eb437e8e7b630/Many_Shot_Jailbreaking__2024_04_02_0936.pdf>Many-shot jailbreaking</a></li><li><a href=https://arxiv.org/abs/2307.02483>Jailbroken: How Does LLM Safety Training Fail?</a></li><li><a href=https://arxiv.org/pdf/2404.14461>Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs</a></li><li><a href=https://arxiv.org/pdf/2306.13213>(vlm) Visual Adversarial Examples Jailbreak Aligned Large Language Models</a></li><li><a href="https://openreview.net/pdf?id=plmBsXHxgR">(vlm) Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models</a></li><li><a href=https://arxiv.org/pdf/2407.02534>(vlm) Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything</a></li></ul></li><li>evaluation & benchmark<ul><li>Jailbreak prompts finding on Twitter</li><li><a href=https://arxiv.org/pdf/2404.01318>JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models</a></li></ul></li></ul></li><li><p><strong>power-seeking</strong></p><ul><li><a href=https://arxiv.org/abs/2206.13477>Parametrically Retargetable Decision-Makers Tend To Seek Power</a></li></ul></li><li><p><font color=green><b>social injustice</b></font><br>prejudice, gender bias: doctor & nurse, discrimination<br>training dynamics; dataset; gradient descent; SAE circuits</p><ul><li><a href=https://www.anthropic.com/research/evaluating-feature-steering>Evaluating feature steering: A case study in mitigating social biases</a></li><li><a href=https://arxiv.org/pdf/2402.15481>Prejudice and Volatility: A Statistical Framework for Measuring Social Discrimination in Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2409.09652>Unveiling Gender Bias in Large Language Models: Using Teacher’s Evaluation in Higher Education As an Example</a></li></ul></li><li><p><strong>deception</strong><br>dishonesty</p><ul><li><a href=https://arxiv.org/abs/2412.14093>Alignment faking in large language models</a></li><li><a href=https://arxiv.org/abs/2401.05566>Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</a></li><li><a href=https://arxiv.org/abs/2409.12822>Language Models Learn To Mislead Humans Via RLHF</a></li><li><a href="https://openreview.net/pdf?id=685vj0lC9z">How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?</a></li></ul></li><li><p><strong>reward hacking</strong></p><ul><li><a href=https://lilianweng.github.io/posts/2024-11-28-reward-hacking/>Reward Hacking in Reinforcement Learning</a></li></ul></li><li><p><strong>measurement tampering</strong><br>The AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome.</p><ul><li><a href=https://arxiv.org/pdf/2308.15605>Benchmarks for Detecting Measurement Tampering</a></li></ul></li><li><p><strong>persona drift</strong></p><ul><li><a href=https://arxiv.org/pdf/2402.10962v1>Measuring and Controlling Persona Drift in Language Model Dialogs</a></li></ul></li><li><p><strong>other human values</strong></p><ul><li><a href=https://arxiv.org/abs/2406.10162>Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models</a></li><li><a href=https://www.lesswrong.com/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation>Reducing sycophancy and improving honesty via activation steering</a></li><li><a href=https://www.lesswrong.com/posts/raoeNarFYCxxyKAop/modulating-sycophancy-in-an-rlhf-model-via-activation#TruthfulQA_performance>Modulating sycophancy in an RLHF model via activation steering</a></li></ul></li><li><p><strong>Agency</strong></p><ul><li><a href=https://arxiv.org/pdf/2310.08043>Understanding and Controlling a Maze-Solving Policy Network</a></li><li><a href=https://arxiv.org/abs/2206.13477>Parametrically Retargetable Decision-Makers Tend To Seek Power</a></li><li><a href=https://arxiv.org/abs/2006.06547>Avoiding Side Effects in Complex Environments</a></li></ul></li><li><p><strong>Alignmnet theory</strong><br>Both alignment and interpretability are related to AI safety, so the mech interp tools are widely used in alignment research. I&rsquo;ll put some good resources of alignment work here.</p><ul><li><strong>qualitative work (findings, analyses, concepts, &mldr;)</strong>
*</li><li><strong>alignment representation</strong>
*</li><li><strong>instrumental convergence</strong>
*</li><li><strong>shard theory</strong><br>Proposed by Alex Turner (TurnTrout) and Quintin Pope<ul><li><a href=https://turntrout.com/shard-theory>The Shard Theory of Human Values</a><ul><li><a href=https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values>(lesswrong) The Shard Theory of Human Values</a></li></ul></li></ul></li></ul></li></ul><h4 id=telescope-resources-7>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources-7>#</a></h4><ul><li><a href=https://arxiv.org/pdf/2310.19852>AI Alignment: A Comprehensive Survey</a></li><li><a href=https://arxiv.org/abs/2310.01405>Representation Engineering: A Top-Down Approach to AI Transparency</a></li><li><a href=https://arxiv.org/pdf/2404.14082>Mechanistic Interpretability for AI Safety A Review</a></li></ul><h3 id=improved-algorithms>Improved Algorithms<a hidden class=anchor aria-hidden=true href=#improved-algorithms>#</a></h3><ul><li><a href=https://arxiv.org/pdf/2404.03592>ReFT: Representation Finetuning for Language Models</a></li><li><a href=https://arxiv.org/pdf/2502.01628>Harmonic Loss Trains Interpretable AI Models</a></li></ul><h2 id=research-limitations>Research Limitations<a hidden class=anchor aria-hidden=true href=#research-limitations>#</a></h2><ul><li>Current work mainly focuses on Transformer-based models. Is transformer a inevitable model structure for generative language models?</li><li>How can we use post-hoc methods as a guide for training a more interpretable and controllable model?</li></ul><h2 id=other-interpretability-fields>Other Interpretability Fields<a hidden class=anchor aria-hidden=true href=#other-interpretability-fields>#</a></h2><h3 id=neural-network-interpretability>Neural network interpretability<a hidden class=anchor aria-hidden=true href=#neural-network-interpretability>#</a></h3><p>Not necessarily a transformer-based model, maybe an lstm or simply a toy model</p><ul><li><strong>Theories for DL</strong><ul><li><a href=https://github.com/foocker/deeplearningtheory>foocker/deeplearningtheory</a></li></ul></li><li><strong>Feature Learning</strong><ul><li><a href=https://github.com/WeiHuang05/Awesome-Feature-Learning-in-Deep-Learning-Thoery>Huang wei&rsquo;s repo</a></li></ul></li><li><strong>game</strong><ul><li><a href>(chess) Evidence of Learned Look-Ahead in a Chess-Playing Neural Network</a></li><li><a href>(Sokoban, planning) Planning behavior in a recurrent neural network that plays Sokoban</a></li></ul></li><li><strong>geometry</strong><ul><li><a href>Reasoning in Large Language Models: A Geometric Perspective</a></li></ul></li><li><strong>Bertology</strong><ul><li><a href=https://arxiv.org/pdf/2002.12327>A Primer in BERTology: What we know about how BERT works</a></li><li><a href=https://arxiv.org/pdf/1911.05758>What do you mean, BERT? Assessing BERT as a Distributional Semantics Model</a></li></ul></li></ul><h2 id=other-surveys>Other Surveys<a hidden class=anchor aria-hidden=true href=#other-surveys>#</a></h2><ul><li><a href=https://arxiv.org/pdf/2501.16496>Open Problems in Mechanistic Interpretability</a></li><li><a href=https://arxiv.org/pdf/2502.17516>A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models</a></li><li><a href=https://arxiv.org/pdf/2404.14082>Mechanistic Interpretability for AI Safety: A Review</a></li><li><a href=https://arxiv.org/pdf/2407.02646>A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models</a></li><li><a href=https://arxiv.org/pdf/2402.10688>Towards Uncovering How Large Language Model Works: An Explainability Perspective</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://Siriuslala.github.io/tags/mechanistic-interpretability/>Mechanistic Interpretability</a></li><li><a href=https://Siriuslala.github.io/tags/machine-learning/>Machine Learning</a></li></ul><nav class=paginav><a class=prev href=https://Siriuslala.github.io/posts/%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E5%AD%A6%E7%9A%84%E6%A2%97%E5%92%8C%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%9F%A5%E8%AF%86/><span class=title>« Prev</span><br><span>一些语言学的梗和有意思的知识</span>
</a><a class=next href=https://Siriuslala.github.io/posts/happy_feats/><span class=title>Next »</span><br><span>Exploring Emotional Features in GPT2-Small</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Possible Research Areas in Mechanistic Interpretability on x" href="https://x.com/intent/tweet/?text=Possible%20Research%20Areas%20in%20Mechanistic%20Interpretability&amp;url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f&amp;hashtags=mechanisticinterpretability%2cmachinelearning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Possible Research Areas in Mechanistic Interpretability on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f&amp;title=Possible%20Research%20Areas%20in%20Mechanistic%20Interpretability&amp;summary=Possible%20Research%20Areas%20in%20Mechanistic%20Interpretability&amp;source=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Possible Research Areas in Mechanistic Interpretability on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f&title=Possible%20Research%20Areas%20in%20Mechanistic%20Interpretability"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Possible Research Areas in Mechanistic Interpretability on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Possible Research Areas in Mechanistic Interpretability on whatsapp" href="https://api.whatsapp.com/send?text=Possible%20Research%20Areas%20in%20Mechanistic%20Interpretability%20-%20https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Possible Research Areas in Mechanistic Interpretability on telegram" href="https://telegram.me/share/url?text=Possible%20Research%20Areas%20in%20Mechanistic%20Interpretability&amp;url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Possible Research Areas in Mechanistic Interpretability on ycombinator" href="https://news.ycombinator.com/submitlink?t=Possible%20Research%20Areas%20in%20Mechanistic%20Interpretability&u=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><div id=tw-comment></div><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="light"?"light":"dark",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"Siriuslala/siriuslala.github.io","data-repo-id":"R_kgDOMo4X2w","data-category":"Announcements","data-category-id":"DIC_kwDOMo4X284CiI_8","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":getStoredTheme(),"data-lang":"en","data-loading":"lazy",crossorigin:"anonymous"},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#tw-comment").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span>&copy; 2025 <a href=https://Siriuslala.github.io/>Siriuslala's Blog!</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>