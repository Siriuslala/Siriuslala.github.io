<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Possible Research Areas in Mechanistic Interpretability | Siriuslala's Blog!</title>
<meta name=keywords content="mechanistic interpretability,machine learning"><meta name=description content="The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.
Circuits Discovery Methods basic activation patching (causal mediation/interchange interventions&mldr;) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? &#x1f52d; resources (ROME) Locating and Editing Factual Associations in GPT (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability (attribution patching) Attribution Patching Outperforms Automated Circuit Discovery Attribution patching: Activation patching at industrial scale AtP*: An efficient and scalable method for localizing llm behaviour to components new Using SAE Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models Automatically Identifying Local and Global Circuits with Linear Computation Graphs Contextual Decomposition Mechanistic Interpretation through Contextual Decomposition in Transformers Edge Pruning ?"><meta name=author content="Sirius"><link rel=canonical href=https://Siriuslala.github.io/posts/mech_interp_research/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.187da127e3564f8ca7db95950f88b9ebc2d4f26e4c83ebf6b5baafee59ff4ebd.css integrity="sha256-GH2hJ+NWT4yn25WVD4i568LU8m5Mg+v2tbqv7ln/Tr0=" rel="preload stylesheet" as=style><link rel=icon href=https://Siriuslala.github.io/pig.svg><link rel=icon type=image/png sizes=16x16 href=https://Siriuslala.github.io/pig.svg><link rel=icon type=image/png sizes=32x32 href=https://Siriuslala.github.io/pig.svg><link rel=apple-touch-icon href=https://Siriuslala.github.io/pig.svg><link rel=mask-icon href=https://Siriuslala.github.io/pig.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Siriuslala.github.io/posts/mech_interp_research/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel=stylesheet><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><style>code.has-jax{font:inherit;font-size:100%;background:inherit;border:inherit;color:#515151}.formula{width:100%;overflow-x:auto}</style><meta property="og:title" content="Possible Research Areas in Mechanistic Interpretability"><meta property="og:description" content="The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.
Circuits Discovery Methods basic activation patching (causal mediation/interchange interventions&mldr;) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? &#x1f52d; resources (ROME) Locating and Editing Factual Associations in GPT (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability (attribution patching) Attribution Patching Outperforms Automated Circuit Discovery Attribution patching: Activation patching at industrial scale AtP*: An efficient and scalable method for localizing llm behaviour to components new Using SAE Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models Automatically Identifying Local and Global Circuits with Linear Computation Graphs Contextual Decomposition Mechanistic Interpretation through Contextual Decomposition in Transformers Edge Pruning ?"><meta property="og:type" content="article"><meta property="og:url" content="https://Siriuslala.github.io/posts/mech_interp_research/"><meta property="og:image" content="https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-06T22:52:16+08:00"><meta property="article:modified_time" content="2024-09-06T22:52:16+08:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Possible Research Areas in Mechanistic Interpretability"><meta name=twitter:description content="The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.
Circuits Discovery Methods basic activation patching (causal mediation/interchange interventions&mldr;) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? &#x1f52d; resources (ROME) Locating and Editing Factual Associations in GPT (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability (attribution patching) Attribution Patching Outperforms Automated Circuit Discovery Attribution patching: Activation patching at industrial scale AtP*: An efficient and scalable method for localizing llm behaviour to components new Using SAE Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models Automatically Identifying Local and Global Circuits with Linear Computation Graphs Contextual Decomposition Mechanistic Interpretation through Contextual Decomposition in Transformers Edge Pruning ?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Siriuslala.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Possible Research Areas in Mechanistic Interpretability","item":"https://Siriuslala.github.io/posts/mech_interp_research/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Possible Research Areas in Mechanistic Interpretability","name":"Possible Research Areas in Mechanistic Interpretability","description":"The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.\nCircuits Discovery Methods basic activation patching (causal mediation/interchange interventions\u0026hellip;) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? \u0026#x1f52d; resources (ROME) Locating and Editing Factual Associations in GPT (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability (attribution patching) Attribution Patching Outperforms Automated Circuit Discovery Attribution patching: Activation patching at industrial scale AtP*: An efficient and scalable method for localizing llm behaviour to components new Using SAE Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models Automatically Identifying Local and Global Circuits with Linear Computation Graphs Contextual Decomposition Mechanistic Interpretation through Contextual Decomposition in Transformers Edge Pruning ?","keywords":["mechanistic interpretability","machine learning"],"articleBody":"The Purpose I Write This Blog To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.\nCircuits Discovery Methods basic activation patching (causal mediation/interchange interventionsâ€¦) path patching scaling techinques: attribution patching DAS (distributed alignment search) directional activation patching? ðŸ”­ resources (ROME) Locating and Editing Factual Associations in GPT (ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability (attribution patching) Attribution Patching Outperforms Automated Circuit Discovery Attribution patching: Activation patching at industrial scale AtP*: An efficient and scalable method for localizing llm behaviour to components new Using SAE Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models Automatically Identifying Local and Global Circuits with Linear Computation Graphs Contextual Decomposition Mechanistic Interpretation through Contextual Decomposition in Transformers Edge Pruning ? Finding Transformer Circuits with Edge Pruning Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning Evaluation lack of ground truth\nhuman interpretability\nFor each feature, annotators are presented with the tokens that fire on the feature. automatic faithfulness (see this)\nhow much of the full modelâ€™s performance can a circuit account for. completeness computational efficiency Issues ablation methods: dropout out is also an ablation, so does zero ablation work? superposition, need the help of SAE? Dictionary Learning SAE\nTraining and optimization proper SAE width dead neurons Evaluation human auto ðŸ”­ resources (Anthropic, 2024-8, contrastive eval \u0026 sort eval) Interpretability Evals for Dictionary Learning (RAVEL) RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations Language models can explain neurons in language models Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control Analysis feature splitting Applications SAE + feature discovery ðŸ”­ resources Sparse Autoencoders Find Highly Interpretable Features in Language Models Anthropic research Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet SAE + circuit discovery ðŸ”­ resources Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models SAE + explain model components Sparse Autoencoders Work on Attention Layer Outputs Interpreting Attention Layer Outputs with Sparse Autoencoders SAE + explain model behaviors SAE + model steering Explain Model Components explain neurons, attention heads and circuits\nExplain neurons ðŸ”­ resources Language models can explain neurons in language models Multimodal Neurons in Artificial Neural Networks Finding Safety Neurons in Large Language Models Explain attention heads different heads in one layer/heads in different layer -\u003e grammer/semantic feats\nðŸ”­ resources Copy Suppression: Comprehensively Understanding An Attention Head Explain circuits understand specific circuits on the subspace level\nWhat Do the Circuits Mean? A Knowledge Edit View (DAS) Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations LLM Circuit Analyses Are Consistent Across Training and Scale Explain layernorm On the Nonlinearity of Layer Normalization Others The Quantization Model of Neural Scaling Explain Model Behaviors Feature representations linear representations theory Linear Explanations for Individual Neurons multilingual representations Emerging Cross-lingual Structure in Pretrained Language Models Probing the Emergence of Cross-lingual Alignment during LLM Training Exploring Alignment in Shared Cross-lingual Spaces mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models? Probing LLMs for Joint Encoding of Linguistic Categories Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure multimodal representations safety reprs ðŸ”­ resources Linear Representations of sentiment in large language models nonlinear representations Not All Language Model Features Are Linear Model capabilities training dynamics\nhow post-training affect model representations and mechanisms\nfine-tuning\nMechanistically analyzing the effects of fine-tuning on procedurally defined tasks Understanding Catastrophic Forgetting in Language Models via Implicit Inference Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking in-context learning\nbasic In-context Learning and Induction Heads What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation bad in-context learning (learn wrong things) Overthinking The Truth: Understanding How language Models Process False Demonstrations chain of thought (COT)\nhow and why step by step?\nzero-shot COT ???\nanalyses Iteration Head: A Mechanistic Study of Chain-of-Thought How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation Do Large Language Models Latently Perform Multi-Hop Reasoning? unfaithful COT Language Models Donâ€™t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting Does the model already know the answer while reasoning, or the model really has a goal? instruction following\nhow does reinforcement learning change the inside of a model? understand RL at mechanistic level high efficient RLxF memorization\nplanning\nEvaluating Cognitive Maps and Planning in Large Language Models with CogEval duplication\nself-repair\nThe Hydra Effect: Emergent Self-repair in Language Model Computations Explorations of Self-Repair in Language Models grokking\nProgress Measures For Grokking Via Mechanistic Interpretability Towards Understanding Grokking: AnEffective Theory of Representation Learning Narrow tasks counting greater-than How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model Indirect Object Indentification (IOI) Interpretability in The Wild: A Circuit For Indirect Object Identification in GPT-2 Small Application Interpretable model structure also called intrinsic interpretability\n(SoLU) Softmax Linear Units Steering vectors perturbations\nbackdoor detection, red-teaming, capability discovery\nActivation Addition: Steering Language Models Without Optimization (lesswrong) Steering GPT-2-XL by adding an activation vector Steering Llama 2 via Contrastive Activation Addition Mechanistically Eliciting Latent Behaviors in Language Models AI alignment 3H (helpful, honest, harmless) Avoid bias and harmful behaviors\nconcept-based interpretability representation-based interpretability\nred-teaming\nanomaly detection\nidentify when AI models generate outputs for atypical reasons.\nbackdoor detection Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks ðŸ”­ resources Mechanistic anomaly detection and ELK A gentle introduction to mechanistic anomaly detection Concrete empirical research projects in mechanistic anomaly detection refuse to request \u0026 jailbreak circuit; SAE; steering vector (anti-refusal)\nanalyses Jailbroken: How Does LLM Safety Training Fail? Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs **Improving Alignment and Robustness with Circuit Breakers **(steering) Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models *Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To! (vlm) Visual Adversarial Examples Jailbreak Aligned Large Language Models (vlm) Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models (vlm) Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything evaluation \u0026 benchmark Jailbreak prompts finding on Twitter JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models power-seeking\nParametrically Retargetable Decision-Makers Tend To Seek Power social injustice prejudice, gender bias: doctor \u0026 nurse, discrimination\ntraining dynamics; dataset; gradient descent; SAE circuits\nPrejudice and Volatility: A Statistical Framework for Measuring Social Discrimination in Large Language Models Unveiling Gender Bias in Large Language Models: Using Teacherâ€™s Evaluation in Higher Education As an Example deception\ndishonesty\nLanguage Models Learn To Mislead Humans Via RLHF How do Large Language Models Navigate Conflicts between Honesty and Helpfulness? measurement tampering\nThe AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome.\nBenchmarks for Detecting Measurement Tampering persona drift\nMeasuring and Controlling Persona Drift in Language Model Dialogs other human values\nReducing sycophancy and improving honesty via activation steering Modulating sycophancy in an RLHF model via activation steering ðŸ”­ resources AI Alignment: A Comprehensive Survey Representation Engineering: A Top-Down Approach to AI Transparency Mechanistic Interpretability for AI Safety A Review Alignmnet theory Both alignment and interpretability are related to AI safety, so the mech interp tools are widely used in alignment research. Iâ€™ll put some good resources of alignment work here.\nqualitative work (findings, analyses, concepts, â€¦) *\nalignment representation\ninstrumental convergence\nshard theory\nProposed by Alex Turner (TurnTrout) and Quintin Pope\nThe Shard Theory of Human Values (lesswrong) The Shard Theory of Human Values Agency Understanding and Controlling a Maze-Solving Policy Network Parametrically Retargetable Decision-Makers Tend To Seek Power Avoiding Side Effects in Complex Environments Research Limitations Current work mainly focuses on Transformer-based models. Is transformer a inevitable model structure for generative language models? How can we use post-hoc methods as a guide for training a more interpretable and controllable model? Other Interpretability Fields Neural network interpretability Not necessarily a transformer-based model, maybe an lstm or simply a toy model\nTheories for DL foocker/deeplearningtheory Feature Learning Huang weiâ€™s repo game (chess) Evidence of Learned Look-Ahead in a Chess-Playing Neural Network (Sokoban, planning) Planning behavior in a recurrent neural network that plays Sokoban geometry Reasoning in Large Language Models: A Geometric Perspective Other Surveys A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models Towards Uncovering How Large Language Model Works: An Explainability Perspective ","wordCount":"1477","inLanguage":"en","image":"https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E","datePublished":"2024-09-06T22:52:16+08:00","dateModified":"2024-09-06T22:52:16+08:00","author":{"@type":"Person","name":"Sirius"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://Siriuslala.github.io/posts/mech_interp_research/"},"publisher":{"@type":"Organization","name":"Siriuslala's Blog!","logo":{"@type":"ImageObject","url":"https://Siriuslala.github.io/pig.svg"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Siriuslala.github.io/ accesskey=h title="Home (Alt + H)"><img src=https://Siriuslala.github.io/pig.svg alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://Siriuslala.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://Siriuslala.github.io/about/ title=About><span>About</span></a></li><li><a href=https://Siriuslala.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://Siriuslala.github.io/faq/ title=FAQ><span>FAQ</span></a></li><li><a href=https://Siriuslala.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://Siriuslala.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://Siriuslala.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://Siriuslala.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Possible Research Areas in Mechanistic Interpretability</h1><div class=post-meta><span title='2024-09-06 22:52:16 +0800 CST'>September 6, 2024</span>&nbsp;Â·&nbsp;7 min&nbsp;Â·&nbsp;1477 words&nbsp;Â·&nbsp;Sirius</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#the-purpose-i-write-this-blog>The Purpose I Write This Blog</a></li><li><a href=#circuits-discovery>Circuits Discovery</a><ul><li><a href=#methods>Methods</a></li><li><a href=#evaluation>Evaluation</a></li><li><a href=#issues>Issues</a></li></ul></li><li><a href=#dictionary-learning>Dictionary Learning</a><ul><li><a href=#training-and-optimization>Training and optimization</a></li><li><a href=#evaluation-1>Evaluation</a></li><li><a href=#analysis>Analysis</a></li><li><a href=#applications>Applications</a></li></ul></li><li><a href=#explain-model-components>Explain Model Components</a><ul><li><a href=#explain-neurons>Explain neurons</a></li><li><a href=#explain-attention-heads>Explain attention heads</a></li><li><a href=#explain-circuits>Explain circuits</a></li><li><a href=#explain-layernorm>Explain layernorm</a></li><li><a href=#others>Others</a></li></ul></li><li><a href=#explain-model-behaviors>Explain Model Behaviors</a><ul><li><a href=#feature-representations>Feature representations</a></li><li><a href=#model-capabilities>Model capabilities</a></li><li><a href=#narrow-tasks>Narrow tasks</a></li></ul></li><li><a href=#application>Application</a><ul><li><a href=#interpretable-model-structure>Interpretable model structure</a></li><li><a href=#steering-vectors>Steering vectors</a></li><li><a href=#ai-alignment>AI alignment</a></li><li><a href=#alignmnet-theory>Alignmnet theory</a></li><li><a href=#agency>Agency</a></li></ul></li><li><a href=#research-limitations>Research Limitations</a></li><li><a href=#other-interpretability-fields>Other Interpretability Fields</a><ul><li><a href=#neural-network-interpretability>Neural network interpretability</a></li></ul></li><li><a href=#other-surveys>Other Surveys</a></li></ul></nav></div></details></div><div class=post-content><h2 id=the-purpose-i-write-this-blog>The Purpose I Write This Blog<a hidden class=anchor aria-hidden=true href=#the-purpose-i-write-this-blog>#</a></h2><p>â€‚ â€‚To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.</p><h2 id=circuits-discovery>Circuits Discovery<a hidden class=anchor aria-hidden=true href=#circuits-discovery>#</a></h2><h3 id=methods>Methods<a hidden class=anchor aria-hidden=true href=#methods>#</a></h3><ul><li><strong>basic</strong><ul><li>activation patching (causal mediation/interchange interventions&mldr;)</li><li>path patching</li><li>scaling techinques: attribution patching</li><li>DAS (distributed alignment search) directional activation patching?</li></ul><h4 id=telescope-resources>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources>#</a></h4><ul><li><a href=https://arxiv.org/abs/2202.05262>(ROME) Locating and Editing Factual Associations in GPT</a></li><li><a href=https://arxiv.org/abs/2304.14997>(ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability</a></li><li><a href=https://arxiv.org/abs/2310.10348>(attribution patching) Attribution Patching Outperforms Automated Circuit Discovery</a><ul><li><a href=https://www.neelnanda.io/mechanistic-interpretability/attribution-patching>Attribution patching: Activation patching at industrial scale</a></li></ul></li><li><a href=https://arxiv.org/pdf/2403.00745>AtP*: An efficient and scalable method for localizing llm behaviour to components</a></li><li><a href></a></li></ul></li><li><strong>new</strong><ul><li>Using SAE<ul><li><a href=https://arxiv.org/abs/2403.19647>Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models</a></li><li><a href=https://arxiv.org/pdf/2405.13868>Automatically Identifying Local and Global Circuits with Linear Computation Graphs</a></li></ul></li><li>Contextual Decomposition<ul><li><a href=https://arxiv.org/abs/2407.00886>Mechanistic Interpretation through Contextual Decomposition in Transformers</a></li></ul></li><li>Edge Pruning ?<ul><li><a href=https://arxiv.org/pdf/2406.16778>Finding Transformer Circuits with Edge Pruning</a></li></ul></li><li><a href=https://arxiv.org/abs/2407.03779>Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning</a></li><li><a href></a></li></ul></li></ul><h3 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h3><p>lack of ground truth</p><ul><li><strong>human</strong><ul><li>interpretability<br>For each feature, annotators are presented with the tokens that fire on the feature.</li></ul></li><li><strong>automatic</strong><ul><li>faithfulness (see <a href=https://arxiv.org/pdf/2403.19647>this</a>)<br>how much of the full modelâ€™s performance can a circuit account for.</li><li>completeness</li><li>computational efficiency</li></ul></li></ul><h3 id=issues>Issues<a hidden class=anchor aria-hidden=true href=#issues>#</a></h3><ul><li>ablation methods: dropout out is also an ablation, so does zero ablation work?</li><li>superposition, need the help of SAE?</li></ul><h2 id=dictionary-learning>Dictionary Learning<a hidden class=anchor aria-hidden=true href=#dictionary-learning>#</a></h2><p>SAE</p><h3 id=training-and-optimization>Training and optimization<a hidden class=anchor aria-hidden=true href=#training-and-optimization>#</a></h3><ul><li>proper SAE width</li><li>dead neurons</li></ul><h3 id=evaluation-1>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation-1>#</a></h3><ul><li><strong>human</strong></li><li><strong>auto</strong></li></ul><h4 id=telescope-resources-1>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources-1>#</a></h4><ul><li><a href=https://transformer-circuits.pub/2024/august-update/index.html>(Anthropic, 2024-8, contrastive eval & sort eval) Interpretability Evals for Dictionary Learning</a></li><li><a href>(RAVEL) RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations</a></li><li><a href=https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html>Language models can explain neurons in language models</a></li><li><a href=https://arxiv.org/abs/2405.08366>Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control</a></li><li><a href></a></li></ul><h3 id=analysis>Analysis<a hidden class=anchor aria-hidden=true href=#analysis>#</a></h3><ul><li>feature splitting</li></ul><h3 id=applications>Applications<a hidden class=anchor aria-hidden=true href=#applications>#</a></h3><ul><li><strong>SAE + feature discovery</strong><h4 id=telescope-resources-2>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources-2>#</a></h4><ul><li><a href>Sparse Autoencoders Find Highly Interpretable Features in Language Models</a></li><li>Anthropic research<ul><li><a href=https://transformer-circuits.pub/2023/monosemantic-features/index.html>Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</a></li><li><a href=https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html>Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet</a></li><li><a href></a></li></ul></li></ul></li><li><strong>SAE + circuit discovery</strong><h4 id=telescope-resources-3>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources-3>#</a></h4><ul><li><a href>Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models</a></li><li><a href></a></li><li><a href></a></li></ul></li><li><strong>SAE + explain model components</strong><ul><li><a href=https://www.lesswrong.com/posts/DtdzGwFh9dCfsekZZ/sparse-autoencoders-work-on-attention-layer-outputs>Sparse Autoencoders Work on Attention Layer Outputs</a></li><li><a href=https://arxiv.org/abs/2406.17759>Interpreting Attention Layer Outputs with Sparse Autoencoders</a></li><li><a href></a></li></ul></li><li><strong>SAE + explain model behaviors</strong></li><li><strong>SAE + model steering</strong></li></ul><h2 id=explain-model-components>Explain Model Components<a hidden class=anchor aria-hidden=true href=#explain-model-components>#</a></h2><p>explain neurons, attention heads and circuits</p><h3 id=explain-neurons>Explain neurons<a hidden class=anchor aria-hidden=true href=#explain-neurons>#</a></h3><h4 id=telescope-resources-4>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources-4>#</a></h4><ul><li><a href=https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html>Language models can explain neurons in language models</a></li><li><a href=https://distill.pub/2021/multimodal-neurons/>Multimodal Neurons in Artificial Neural Networks</a></li><li><a href>Finding Safety Neurons in Large Language Models</a></li><li><a href></a></li></ul><h3 id=explain-attention-heads>Explain attention heads<a hidden class=anchor aria-hidden=true href=#explain-attention-heads>#</a></h3><p>different heads in one layer/heads in different layer -> grammer/semantic feats</p><h4 id=telescope-resources-5>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources-5>#</a></h4><ul><li><a href=https://arxiv.org/abs/2310.04625>Copy Suppression: Comprehensively Understanding An Attention Head</a></li><li><a href></a></li></ul><h3 id=explain-circuits>Explain circuits<a hidden class=anchor aria-hidden=true href=#explain-circuits>#</a></h3><p>understand specific circuits on the subspace level</p><ul><li><a href=https://arxiv.org/pdf/2406.17241>What Do the Circuits Mean? A Knowledge Edit View</a></li><li><a href>(DAS) Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations</a></li><li><a href=https://arxiv.org/pdf/2407.10827>LLM Circuit Analyses Are Consistent Across Training and Scale</a></li></ul><h3 id=explain-layernorm>Explain layernorm<a hidden class=anchor aria-hidden=true href=#explain-layernorm>#</a></h3><ul><li><a href="https://openreview.net/pdf?id=18f6iPn0zq">On the Nonlinearity of Layer Normalization</a></li></ul><h3 id=others>Others<a hidden class=anchor aria-hidden=true href=#others>#</a></h3><ul><li><a href=https://arxiv.org/abs/2303.13506>The Quantization Model of Neural Scaling</a></li></ul><h2 id=explain-model-behaviors>Explain Model Behaviors<a hidden class=anchor aria-hidden=true href=#explain-model-behaviors>#</a></h2><h3 id=feature-representations>Feature representations<a hidden class=anchor aria-hidden=true href=#feature-representations>#</a></h3><ul><li><strong>linear representations</strong><ul><li>theory<ul><li><a href="https://openreview.net/pdf?id=WIbntm28cM">Linear Explanations for Individual Neurons</a></li></ul></li><li><font color=green>multilingual representations</font><ul><li><a href=https://arxiv.org/pdf/1911.01464>Emerging Cross-lingual Structure in Pretrained Language Models</a></li><li><a href=https://arxiv.org/pdf/2406.13229>Probing the Emergence of Cross-lingual Alignment during LLM Training</a></li><li><a href=https://arxiv.org/pdf/2405.14535>Exploring Alignment in Shared Cross-lingual Spaces</a></li><li><a href=https://arxiv.org/pdf/2404.12444>mOthello: When Do Cross-Lingual Representation Alignment and Cross-Lingual Transfer Emerge in Multilingual Models?</a></li><li><a href=https://arxiv.org/pdf/2310.18696>Probing LLMs for Joint Encoding of Linguistic Categories</a></li><li><a href=https://arxiv.org/pdf/2203.08430>Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure</a></li><li><a href></a></li><li><a href></a></li></ul></li><li><font color=green>multimodal representations</font></li><li><font color=green>safety reprs</font></li></ul><h4 id=telescope-resources-6>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources-6>#</a></h4><ul><li><a href=https://arxiv.org/abs/2310.15154>Linear Representations of sentiment in large language models</a></li></ul></li><li><strong>nonlinear representations</strong><ul><li><a href>Not All Language Model Features Are Linear</a></li></ul></li></ul><h3 id=model-capabilities>Model capabilities<a hidden class=anchor aria-hidden=true href=#model-capabilities>#</a></h3><p>training dynamics<br>how post-training affect model representations and mechanisms</p><ul><li><p><strong>fine-tuning</strong></p><ul><li><a href=https://arxiv.org/pdf/2311.12786>Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks</a></li><li><a href=https://arxiv.org/pdf/2309.10105>Understanding Catastrophic Forgetting in Language Models via Implicit Inference</a></li><li><a href=https://arxiv.org/pdf/2402.14811>Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking</a></li></ul></li><li><p><strong>in-context learning</strong></p><ul><li>basic<ul><li><a href=https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html>In-context Learning and Induction Heads</a></li><li><a href="https://openreview.net/pdf?id=O8rrXl71D5">What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation</a></li></ul></li><li>bad in-context learning (learn wrong things)<ul><li><a href=https://arxiv.org/pdf/2307.09476>Overthinking The Truth: Understanding How language Models Process False Demonstrations</a></li></ul></li></ul></li><li><p><strong>chain of thought (COT)</strong><br>how and why step by step?<br>zero-shot COT ???</p><ul><li>analyses<ul><li><a href=https://arxiv.org/pdf/2406.02128>Iteration Head: A Mechanistic Study of Chain-of-Thought</a></li><li><a href=https://arxiv.org/pdf/2402.18312>How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning</a></li><li><a href=https://arxiv.org/pdf/2406.12288>An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs</a></li><li><a href=https://arxiv.org/pdf/2402.18344>Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning</a></li><li><a href=https://arxiv.org/pdf/2406.12255>A Hopfieldian View-based Interpretation for Chain-of-Thought Reasoning</a></li><li><a href=https://arxiv.org/pdf/2405.15302>Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation</a></li><li><a href=https://arxiv.org/pdf/2402.16837>Do Large Language Models Latently Perform Multi-Hop Reasoning?</a></li></ul></li><li>unfaithful COT<ul><li><a href=https://arxiv.org/abs/2305.04388>Language Models Donâ€™t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting</a></li></ul></li><li>Does the model already know the answer while reasoning, or the model really has a goal?</li></ul></li><li><p><strong>instruction following</strong></p><ul><li><font color=green>how does reinforcement learning change the inside of a model?</font><ul><li>understand RL at mechanistic level</li><li>high efficient RLxF</li></ul></li></ul></li><li><p><strong>memorization</strong></p></li><li><p><strong>planning</strong></p><ul><li><a href=https://arxiv.org/abs/2309.15129>Evaluating Cognitive Maps and Planning in Large Language Models with CogEval</a></li></ul></li><li><p><strong>duplication</strong></p></li><li><p><strong>self-repair</strong></p><ul><li><a href=https://arxiv.org/abs/2307.15771>The Hydra Effect: Emergent Self-repair in Language Model Computations</a></li><li><a href=https://arxiv.org/abs/2402.15390v1>Explorations of Self-Repair in Language Models</a></li></ul></li><li><p><strong>grokking</strong></p><ul><li><a href>Progress Measures For Grokking Via Mechanistic Interpretability</a></li><li><a href>Towards Understanding Grokking: AnEffective Theory of Representation Learning</a></li></ul></li></ul><h3 id=narrow-tasks>Narrow tasks<a hidden class=anchor aria-hidden=true href=#narrow-tasks>#</a></h3><ul><li>counting</li><li>greater-than<ul><li><a href=https://arxiv.org/pdf/2305.00586>How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model</a></li></ul></li><li>Indirect Object Indentification<ul><li><a href=https://arxiv.org/abs/2211.00593>(IOI) Interpretability in The Wild: A Circuit For Indirect Object Identification in GPT-2 Small</a></li></ul></li></ul><h2 id=application>Application<a hidden class=anchor aria-hidden=true href=#application>#</a></h2><h3 id=interpretable-model-structure>Interpretable model structure<a hidden class=anchor aria-hidden=true href=#interpretable-model-structure>#</a></h3><p>also called intrinsic interpretability</p><ul><li><a href=https://transformer-circuits.pub/2022/solu/index.html>(SoLU) Softmax Linear Units</a></li></ul><h3 id=steering-vectors>Steering vectors<a hidden class=anchor aria-hidden=true href=#steering-vectors>#</a></h3><p>perturbations<br>backdoor detection, red-teaming, capability discovery</p><ul><li><a href=https://arxiv.org/abs/2308.10248>Activation Addition: Steering Language Models Without Optimization</a><ul><li><a href=https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector>(lesswrong) Steering GPT-2-XL by adding an activation vector</a></li></ul></li><li><a href=https://arxiv.org/abs/2312.06681>Steering Llama 2 via Contrastive Activation Addition</a></li><li><a href=https://www.alignmentforum.org/posts/ioPnHKFyy4Cw2Gr2x/mechanistically-eliciting-latent-behaviors-in-language-1>Mechanistically Eliciting Latent Behaviors in Language Models</a></li><li><a href></a></li></ul><h3 id=ai-alignment>AI alignment<a hidden class=anchor aria-hidden=true href=#ai-alignment>#</a></h3><p>3H (helpful, honest, harmless)
Avoid bias and harmful behaviors<br>concept-based interpretability
representation-based interpretability<br>red-teaming</p><ul><li><p><strong>anomaly detection</strong><br>identify when AI models generate outputs for atypical reasons.</p><ul><li><strong>backdoor detection</strong><ul><li><a href=https://arxiv.org/abs/2401.05566>Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</a></li><li><a href=https://arxiv.org/pdf/2405.11575>SEEP: Training Dynamics Grounds Latent Representation Search for Mitigating Backdoor Poisoning Attacks</a></li></ul></li></ul><h4 id=telescope-resources-7>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources-7>#</a></h4><ul><li><a href=https://www.alignmentforum.org/posts/vwt3wKXWaCvqZyF74/mechanistic-anomaly-detection-and-elk>Mechanistic anomaly detection and ELK</a></li><li><a href=https://www.lesswrong.com/posts/n7DFwtJvCzkuKmtbG/a-gentle-introduction-to-mechanistic-anomaly-detection>A gentle introduction to mechanistic anomaly detection</a></li><li><a href=https://www.lesswrong.com/s/GiZ6puwmHozLuBrph/p/99gWh9jxeumcmuduw>Concrete empirical research projects in mechanistic anomaly detection</a></li></ul></li><li><p><font color=green><b>refuse to request & jailbreak</b></font><br>circuit; SAE; steering vector (anti-refusal)</p><ul><li>analyses<ul><li><a href=https://arxiv.org/abs/2307.02483>Jailbroken: How Does LLM Safety Training Fail?</a></li><li><a href=https://arxiv.org/pdf/2404.14461>Competition Report: Finding Universal Jailbreak Backdoors in Aligned LLMs</a></li><li><a href=https://arxiv.org/pdf/2406.04313>**Improving Alignment and Robustness with Circuit Breakers</a></li><li><a href=https://arxiv.org/pdf/2406.09289>**(steering) Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2402.05162>*Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications</a></li><li><a href=https://arxiv.org/pdf/2404.04392>Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes</a></li><li><a href="https://openreview.net/pdf?id=hTEGyKf0dZ">Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!</a></li><li><a href=https://arxiv.org/pdf/2306.13213>(vlm) Visual Adversarial Examples Jailbreak Aligned Large Language Models</a></li><li><a href="https://openreview.net/pdf?id=plmBsXHxgR">(vlm) Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models</a></li><li><a href=https://arxiv.org/pdf/2407.02534>(vlm) Image-to-Text Logic Jailbreak: Your Imagination can Help You Do Anything</a></li></ul></li><li>evaluation & benchmark<ul><li>Jailbreak prompts finding on Twitter</li><li><a href=https://arxiv.org/pdf/2404.01318>JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models</a></li></ul></li></ul></li><li><p><strong>power-seeking</strong></p><ul><li><a href=https://arxiv.org/abs/2206.13477>Parametrically Retargetable Decision-Makers Tend To Seek Power</a></li></ul></li><li><p><font color=green><b>social injustice</b></font><br>prejudice, gender bias: doctor & nurse, discrimination<br>training dynamics; dataset; gradient descent; SAE circuits</p><ul><li><a href=https://arxiv.org/pdf/2402.15481>Prejudice and Volatility: A Statistical Framework for Measuring Social Discrimination in Large Language Models</a></li><li><a href=https://arxiv.org/pdf/2409.09652>Unveiling Gender Bias in Large Language Models: Using Teacherâ€™s Evaluation in Higher Education As an Example</a></li></ul></li><li><p><strong>deception</strong><br>dishonesty</p><ul><li><a href=https://arxiv.org/abs/2409.12822>Language Models Learn To Mislead Humans Via RLHF</a></li><li><a href="https://openreview.net/pdf?id=685vj0lC9z">How do Large Language Models Navigate Conflicts between Honesty and Helpfulness?</a></li></ul></li><li><p><strong>measurement tampering</strong><br>The AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome.</p><ul><li><a href=https://arxiv.org/pdf/2308.15605>Benchmarks for Detecting Measurement Tampering</a></li></ul></li><li><p><strong>persona drift</strong></p><ul><li><a href=https://arxiv.org/pdf/2402.10962v1>Measuring and Controlling Persona Drift in Language Model Dialogs</a></li></ul></li><li><p><strong>other human values</strong></p><ul><li><a href=https://www.lesswrong.com/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation>Reducing sycophancy and improving honesty via activation steering</a></li><li><a href=https://www.lesswrong.com/posts/raoeNarFYCxxyKAop/modulating-sycophancy-in-an-rlhf-model-via-activation#TruthfulQA_performance>Modulating sycophancy in an RLHF model via activation steering</a></li></ul></li></ul><h4 id=telescope-resources-8>&#x1f52d; resources<a hidden class=anchor aria-hidden=true href=#telescope-resources-8>#</a></h4><ul><li><a href=https://arxiv.org/pdf/2310.19852>AI Alignment: A Comprehensive Survey</a></li><li><a href=https://arxiv.org/abs/2310.01405>Representation Engineering: A Top-Down Approach to AI Transparency</a></li><li><a href=https://arxiv.org/pdf/2404.14082>Mechanistic Interpretability for AI Safety A Review</a></li></ul><h3 id=alignmnet-theory>Alignmnet theory<a hidden class=anchor aria-hidden=true href=#alignmnet-theory>#</a></h3><p>Both alignment and interpretability are related to AI safety, so the mech interp tools are widely used in alignment research. I&rsquo;ll put some good resources of alignment work here.</p><ul><li><p><strong>qualitative work (findings, analyses, concepts, &mldr;)</strong>
*</p></li><li><p><strong>alignment representation</strong></p></li><li><p><strong>instrumental convergence</strong></p></li><li><p><strong>shard theory</strong><br>Proposed by Alex Turner (TurnTrout) and Quintin Pope</p><ul><li><a href=https://turntrout.com/shard-theory>The Shard Theory of Human Values</a><ul><li><a href=https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values>(lesswrong) The Shard Theory of Human Values</a></li></ul></li></ul></li></ul><h3 id=agency>Agency<a hidden class=anchor aria-hidden=true href=#agency>#</a></h3><ul><li><a href=https://arxiv.org/pdf/2310.08043>Understanding and Controlling a Maze-Solving Policy Network</a></li><li><a href=https://arxiv.org/abs/2206.13477>Parametrically Retargetable Decision-Makers Tend To Seek Power</a></li><li><a href=https://arxiv.org/abs/2006.06547>Avoiding Side Effects in Complex Environments</a></li></ul><h2 id=research-limitations>Research Limitations<a hidden class=anchor aria-hidden=true href=#research-limitations>#</a></h2><ul><li>Current work mainly focuses on Transformer-based models. Is transformer a inevitable model structure for generative language models?</li><li>How can we use post-hoc methods as a guide for training a more interpretable and controllable model?</li></ul><h2 id=other-interpretability-fields>Other Interpretability Fields<a hidden class=anchor aria-hidden=true href=#other-interpretability-fields>#</a></h2><h3 id=neural-network-interpretability>Neural network interpretability<a hidden class=anchor aria-hidden=true href=#neural-network-interpretability>#</a></h3><p>Not necessarily a transformer-based model, maybe an lstm or simply a toy model</p><ul><li><strong>Theories for DL</strong><ul><li><a href=https://github.com/foocker/deeplearningtheory>foocker/deeplearningtheory</a></li></ul></li><li><strong>Feature Learning</strong><ul><li><a href=https://github.com/WeiHuang05/Awesome-Feature-Learning-in-Deep-Learning-Thoery>Huang wei&rsquo;s repo</a></li></ul></li><li><strong>game</strong><ul><li><a href>(chess) Evidence of Learned Look-Ahead in a Chess-Playing Neural Network</a></li><li><a href>(Sokoban, planning) Planning behavior in a recurrent neural network that plays Sokoban</a></li></ul></li><li><strong>geometry</strong><ul><li><a href>Reasoning in Large Language Models: A Geometric Perspective</a></li></ul></li></ul><h2 id=other-surveys>Other Surveys<a hidden class=anchor aria-hidden=true href=#other-surveys>#</a></h2><ul><li><a href=https://arxiv.org/pdf/2407.02646>A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models</a></li><li><a href=https://arxiv.org/pdf/2402.10688>Towards Uncovering How Large Language Model Works: An Explainability Perspective</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://Siriuslala.github.io/tags/mechanistic-interpretability/>Mechanistic Interpretability</a></li><li><a href=https://Siriuslala.github.io/tags/machine-learning/>Machine Learning</a></li></ul><nav class=paginav><a class=prev href=https://Siriuslala.github.io/posts/%E4%B8%80%E4%BA%9B%E8%AF%AD%E8%A8%80%E5%AD%A6%E7%9A%84%E6%A2%97%E5%92%8C%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E7%9F%A5%E8%AF%86/><span class=title>Â« Prev</span><br><span>ä¸€äº›è¯­è¨€å­¦çš„æ¢—å’Œæœ‰æ„æ€çš„çŸ¥è¯†</span>
</a><a class=next href=https://Siriuslala.github.io/posts/happy_feats/><span class=title>Next Â»</span><br><span>Exploring Emotional Features in GPT2-Small</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Possible Research Areas in Mechanistic Interpretability on x" href="https://x.com/intent/tweet/?text=Possible%20Research%20Areas%20in%20Mechanistic%20Interpretability&amp;url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f&amp;hashtags=mechanisticinterpretability%2cmachinelearning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Possible Research Areas in Mechanistic Interpretability on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f&amp;title=Possible%20Research%20Areas%20in%20Mechanistic%20Interpretability&amp;summary=Possible%20Research%20Areas%20in%20Mechanistic%20Interpretability&amp;source=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Possible Research Areas in Mechanistic Interpretability on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f&title=Possible%20Research%20Areas%20in%20Mechanistic%20Interpretability"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Possible Research Areas in Mechanistic Interpretability on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Possible Research Areas in Mechanistic Interpretability on whatsapp" href="https://api.whatsapp.com/send?text=Possible%20Research%20Areas%20in%20Mechanistic%20Interpretability%20-%20https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Possible Research Areas in Mechanistic Interpretability on telegram" href="https://telegram.me/share/url?text=Possible%20Research%20Areas%20in%20Mechanistic%20Interpretability&amp;url=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Possible Research Areas in Mechanistic Interpretability on ycombinator" href="https://news.ycombinator.com/submitlink?t=Possible%20Research%20Areas%20in%20Mechanistic%20Interpretability&u=https%3a%2f%2fSiriuslala.github.io%2fposts%2fmech_interp_research%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer><div id=tw-comment></div><script>const getStoredTheme=()=>localStorage.getItem("pref-theme")==="light"?"light":"dark",setGiscusTheme=()=>{const e=e=>{const t=document.querySelector("iframe.giscus-frame");t&&t.contentWindow.postMessage({giscus:e},"https://giscus.app")};e({setConfig:{theme:getStoredTheme()}})};document.addEventListener("DOMContentLoaded",()=>{const s={src:"https://giscus.app/client.js","data-repo":"Siriuslala/siriuslala.github.io","data-repo-id":"R_kgDOMo4X2w","data-category":"Announcements","data-category-id":"DIC_kwDOMo4X284CiI_8","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":getStoredTheme(),"data-lang":"en","data-loading":"lazy",crossorigin:"anonymous"},e=document.createElement("script");Object.entries(s).forEach(([t,n])=>e.setAttribute(t,n)),document.querySelector("#tw-comment").appendChild(e);const t=document.querySelector("#theme-toggle");t&&t.addEventListener("click",setGiscusTheme);const n=document.querySelector("#theme-toggle-float");n&&n.addEventListener("click",setGiscusTheme)})</script></article></main><footer class=footer><span>&copy; 2024 <a href=https://Siriuslala.github.io/>Siriuslala's Blog!</a></span> Â·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>