<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Agent on Siriuslala&#39;s Blog!</title>
    <link>https://Siriuslala.github.io/tags/agent/</link>
    <description>Recent content in Agent on Siriuslala&#39;s Blog!</description>
    <image>
      <title>Siriuslala&#39;s Blog!</title>
      <url>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.147.6</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 28 Jun 2025 13:01:09 +0800</lastBuildDate>
    <atom:link href="https://Siriuslala.github.io/tags/agent/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>LLM Agents</title>
      <link>https://Siriuslala.github.io/posts/llm-agent/</link>
      <pubDate>Sat, 28 Jun 2025 13:01:09 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/llm-agent/</guid>
      <description>&lt;h2 id=&#34;the-purpose-i-write-this-blog&#34;&gt;The Purpose I Write This Blog&lt;/h2&gt;
&lt;p&gt;   LLM-based agent is gonna change the world. Amazing agent systems have been created to change our life. Since I was once in a team that aimed to build advanced agents for the control of digital devices and for which I was impressed, I want to keep to collect information about LLM agents and share my thoughts here.&lt;/p&gt;
&lt;h2 id=&#34;resource&#34;&gt;Resource&lt;/h2&gt;
&lt;h3 id=&#34;gui-agents&#34;&gt;GUI Agents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;survey&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.18279&#34;&gt;Large Language Model-Brained GUI Agents: A Survey&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzkwNjE2ODMxNQ==&amp;amp;mid=2247488335&amp;amp;idx=1&amp;amp;sn=506d56c87a179b3af119d4f70dd549f5&amp;amp;scene=21&amp;amp;poc_token=HIF1X2ijWKfo2MykofnxM8oq-4mrLjJkhL8TdWx4&#34;&gt;GUI Agent综述 : 揭秘GUI智能体的前世今生-1 : 总览篇-启程&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;models&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.12326&#34;&gt;UI-TARS: Pioneering Automated GUI Interaction with Native Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;autoglm
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.14040&#34;&gt;ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Xiao9905/AutoGLM/blob/main/static/papers/mobilerl_0820.pdf&#34;&gt;MobileRL: Advancing Mobile Use Agents With Adaptive Online Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2504.19298&#34;&gt;ANDROIDGEN: Building an Android Language Agent under Data Scarcity&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.00820&#34;&gt;Autoglm: Autonomous foundation agents for guis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.02337&#34;&gt;WebRL:Training llm web agents via self-evolving online curriculum reinforcement learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2410.24024&#34;&gt;AndroidLab: Training and Systematic Benchmarking of Android Autonomous Agents&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2024/file/1704ddd0bb89f159dfe609b32c889995-Paper-Conference.pdf&#34;&gt;DigiRL: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.10935&#34;&gt;SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3706598.3713600&#34;&gt;Appagent: Multimodal agents as smartphone users&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.01614&#34;&gt;(SeeAct) GPT-4V(ision) is a Generalist Web Agent, if Grounded&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.11441&#34;&gt;Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;benchmarks&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;web
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2307.13854&#34;&gt;WebArena: A Realistic Web Environment for Building Autonomous Agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/5950bf290a1570ea401bf98882128160-Paper-Datasets_and_Benchmarks.pdf&#34;&gt;Mind2web: Towards a generalist agent for the web&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2024/file/5d413e48f84dc61244b6be550f1cd8f5-Paper-Datasets_and_Benchmarks_Track.pdf&#34;&gt;Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.mlr.press/v70/shi17a/shi17a.pdf&#34;&gt;(MiniWob) World of Bits: An Open-Domain Platform for Web-Based Agents&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;android
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/bbbb6308b402fe909c39dd29950c32e0-Paper-Datasets_and_Benchmarks.pdf&#34;&gt;Android in the Wild: A Large-Scale Dataset for Android Device Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.06596&#34;&gt;(AndroidArena) Understanding the weakness of large language model agents within a complex android environment&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deepresearch&#34;&gt;DeepResearch&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;survey&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.18096&#34;&gt;Deep Research Agents: A Systematic Examination And Roadmap&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.17188v1&#34;&gt;Towards AI Search Paradigm&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;models&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.05366&#34;&gt;Search-o1: Agentic search-enhanced large reasoning models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2503.09516&#34;&gt;Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.05592&#34;&gt;R1-searcher: Incentivizing the search capability in llms via reinforcement learning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/PeterGriffinJin/Search-R1&#34;&gt;repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jina-ai/node-DeepResearch&#34;&gt;(Jina) node-DeepResearch Public&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://moonshotai.github.io/Kimi-Researcher/&#34;&gt;Kimi-Researcher: End-to-End RL Training for Emerging Agentic Capabilities&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.20249&#34;&gt;Language Modeling by Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;agentic-rl&#34;&gt;Agentic RL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;papers&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
