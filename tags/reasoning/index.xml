<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Reasoning on Siriuslala&#39;s Blog!</title>
    <link>https://Siriuslala.github.io/tags/reasoning/</link>
    <description>Recent content in Reasoning on Siriuslala&#39;s Blog!</description>
    <image>
      <title>Siriuslala&#39;s Blog!</title>
      <url>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.147.6</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Jun 2025 10:50:00 +0800</lastBuildDate>
    <atom:link href="https://Siriuslala.github.io/tags/reasoning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Thinking and Reasoning</title>
      <link>https://Siriuslala.github.io/posts/thinking_and_reasoning/</link>
      <pubDate>Mon, 30 Jun 2025 10:50:00 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/thinking_and_reasoning/</guid>
      <description>&lt;h2 id=&#34;the-purpose-i-write-this-blog&#34;&gt;The Purpose I Write This Blog&lt;/h2&gt;
&lt;p&gt;   Thinking models are crazily popualr nowadays. The first time I delved in this area was in September, 2023. Later I gradually forgetted this area, until Deepseek came to life. I want to keep to collect information about LLM reasoning and share my thoughts here.&lt;/p&gt;
&lt;h2 id=&#34;thinking-models&#34;&gt;Thinking Models&lt;/h2&gt;
&lt;h3 id=&#34;text-based&#34;&gt;text-based&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;explicit reasoning
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.12948?&#34;&gt;DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2504.13914&#34;&gt;Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.12599&#34;&gt;Kimi k1.5: Scaling Reinforcement Learning with LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.06471&#34;&gt;GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.22312&#34;&gt;Skywork Open Reasoner 1 Technical Report&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;implicit reasoning
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.06769&#34;&gt;(Coconut) Training Large Language Models to Reason in a Continuous Latent Space&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;others
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2404.02893&#34;&gt;ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;blogs
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://yuanchaofa.com/post/deepseek-r1-paper-reading-notes.html&#34;&gt;自顶向下方式深度解读 DeepSeek-R1，内含大量细节&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yuanchaofa.com/post/hands-on-deepseek-mla.html&#34;&gt;MLA(1)：从代码角度学习和彻底理解 DeepSeek MLA 算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/20161412399&#34;&gt;从头理解思考模型（LLM based Reasoning Model），O1，DeepSeek R1，Kimi K1.5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;overthinking&#34;&gt;overthinking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;survey
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2503.16419&#34;&gt;Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs?&#34;&gt;(repo) Awesome-Efficient-Reasoning-LLMs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;papers
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.09388&#34;&gt;Qwen3 Technical Report&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.11896&#34;&gt;AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.13417&#34;&gt;AdaptThink: Reasoning Models Can Learn When to Think&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;blogs
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://yuanchaofa.com/post/slow-fast-thinking-from-qwen3-thinking-mixed-to-adacot-to-adathinking.html&#34;&gt;自适应快慢思考推理模型（Adaptive Reasoning Model）：Qwen3混合思考-&amp;gt;字节AdaCoT-&amp;gt;清华AdaptThinking&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;parallel-thinking&#34;&gt;parallel thinking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.15260&#34;&gt;Deep Think with Confidence&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;visual-reasoning&#34;&gt;visual reasoning&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;survey
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.23918&#34;&gt;Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;papers
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2312.14135&#34;&gt;$V^{*}$: Guided Visual Search as a Core Mechanism in Multimodal LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;active perception
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.14362&#34;&gt;DeepEyes: Incentivizing “Thinking with Images” via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.15436&#34;&gt;Chain-of-Focus: Adaptive Visual Search and Zooming for Multimodal Reasoning via RL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.15879&#34;&gt;GRIT: Teaching MLLMs to Think with Images&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;tool use
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.20289&#34;&gt;VisualToolAgent (VisTA): A Reinforcement Learning Framework for Visual Tool Selection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.19255&#34;&gt;VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.20256&#34;&gt;Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.09403&#34;&gt;Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;imagination
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.22525v1&#34;&gt;Thinking with Generated Images&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.11409&#34;&gt;Visual Planning: Let&amp;rsquo;s Think Only with Images&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;blogs
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.xiaohongshu.com/explore/683ccdab000000000303c818?app_platform=android&amp;amp;ignoreEngage=true&amp;amp;app_version=8.91.0&amp;amp;share_from_user_hidden=true&amp;amp;xsec_source=app_share&amp;amp;type=normal&amp;amp;xsec_token=CBrIk8OuUeFJeSobm2LWUAhX-LDwR5xnSiLv1nbJnaGwA=&amp;amp;author_share=1&amp;amp;xhsshare=WeixinSession&amp;amp;shareRedId=N0wyNUk4SUA2NzUyOTgwNjY0OTc3SDhP&amp;amp;apptime=1752769027&amp;amp;share_id=1b8ccda2261146b3bef99ba488c1530d&amp;amp;share_channel=wechat&#34;&gt;Thinking with Images 小结&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;others&#34;&gt;others&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/6hFxdtenAsJ0qU1wEP_XOA&#34;&gt;[蒙特卡洛搜索树] MCT Self-Refine (MCTSr)的算法（包含代码理解）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/27278317894&#34;&gt;聊聊推理模型中的PRMs与MCTS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h2&gt;
&lt;h3 id=&#34;dataset&#34;&gt;dataset&lt;/h3&gt;
&lt;h2 id=&#34;analyses&#34;&gt;Analyses&lt;/h2&gt;
&lt;h3 id=&#34;analyses-1&#34;&gt;analyses&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.08221&#34;&gt;Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;interpretability&#34;&gt;interpretability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.05744&#34;&gt;Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2507.00432&#34;&gt;Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.02867&#34;&gt;Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.19143&#34;&gt;Thought Anchors: Which LLM Reasoning Steps Matter?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.18167&#34;&gt;Understanding Reasoning in Thinking Language Models via Steering Vectors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.alphaxiv.org/abs/2025.02&#34;&gt;Chain-of-Thought Is Not Explainability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.04667&#34;&gt;Unveiling the Mechanisms of Explicit CoT Training: How Chain-of-Thought Enhances Reasoning Generalization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.13913&#34;&gt;How Do LLMs Perform Two-Hop Reasoning in Context?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;theories&#34;&gt;theories&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.12514&#34;&gt;Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;reinforcement-learning&#34;&gt;Reinforcement Learning&lt;/h2&gt;
&lt;h3 id=&#34;rl-algorithms&#34;&gt;RL algorithms&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1506.02438&#34;&gt;(GAE) High-Dimensional Continuous Control Using Generalized Advantage Estimation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf&#34;&gt;(DPO) Direct preference optimization: Your language model is secretly a reward model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2404.12358&#34;&gt;From r to q∗: Your language model is secretly a q-function&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/693746297&#34;&gt;DPO新作Your Language Model is Secretly a Q-Function解读，与OPENAI Q* 的联系？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1707.06347&#34;&gt;(PPO) Proximal Policy Optimization Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.03262&#34;&gt;(REINFORCE++) REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt and Reward Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.12948?&#34;&gt;(GRPO) DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.14476&#34;&gt;(DAPO) DAPO: An Open-Source LLM Reinforcement Learning System at Scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2507.18071&#34;&gt;(GSPO) Group Sequence Policy Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.13585&#34;&gt;(Cispo) MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;blogs&#34;&gt;Blogs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;algorithms
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/7461863937&#34;&gt;人人都能看懂的RL-PPO理论知识&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/25410252053?share_code=2UJFMKKeyurA&amp;amp;utm_psn=1939508205295239578&#34;&gt;Reasoning LLM（三）：LLM+RL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/e6mfNLYwLQV-UKuVo6U6zQ&#34;&gt;RLHF 常见的思维误区&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reward-modeling&#34;&gt;reward modeling&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;text
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2305.20050&#34;&gt;(PRM) Let&amp;rsquo;s verify step by step&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2507.05197&#34;&gt;(POLAR) Pre-Trained Policy Discriminators are General Reward Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;reward model for generative models
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.13918&#34;&gt;Improving Video Generation with Human Feedback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.21059v1&#34;&gt;VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning for Image and Video Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2311.04155&#34;&gt;Black-Box Prompt Optimization: Aligning Large Language Models without Model Training&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;analyses-2&#34;&gt;analyses&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;RL training&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.08221&#34;&gt;Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;entropy&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1917257135730964355&#34;&gt;Reasoning LLM（五）：熵缩过程与能力边界&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1924309705548867391&#34;&gt;LLMxRL】熵坍缩与缓解策略&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.22617&#34;&gt;(clip/kl-cov) The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.01939&#34;&gt;(forking tokens) Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RL v.s. SFT&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/25410252053&#34;&gt;3.2 统一视角理解从 SFT 到 RL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.01067&#34;&gt;All Roads Lead to Likelihood: The Value of Reinforcement Learning in Fine-Tuning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.23235&#34;&gt;Generalist Reward Models: Found Inside Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2508.05629&#34;&gt;(DFT) On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1941156341583418911&#34;&gt;从 SFT 到 RL：一步步看清它们的联系&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.18116&#34;&gt;(NFT) Bridging Supervised Learning and Reinforcement Learning in Math Reasoning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;resource&#34;&gt;Resource&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;RL infra
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2409.19256v2&#34;&gt;(verl) HybridFlow: A Flexible and Efficient RLHF Framework&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://verl.readthedocs.io/en/latest/index.html&#34;&gt;doc&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/volcengine/verl&#34;&gt;repo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/THUDM/slime&#34;&gt;slime&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/UbNhNkBed9QO1VrwyR-Faw&#34;&gt;RL Scaling 时代，我们需要什么样的 RL 框架呢？&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;blogs&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;training
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1936165441090328553&#34;&gt;浅聊RL框架的勃勃生机、万物竞发&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.anthropic.com/engineering/multi-agent-research-system&#34;&gt;How we built our multi-agent research system&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;verl
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/27676081245&#34;&gt;[AI Infra] VeRL 框架入门&amp;amp;代码带读&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/30876678559&#34;&gt;从零开始的verl框架解析&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1907077645071524975&#34;&gt;verl RL支持训练deepseek-v3 671B实习复盘(个人版)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/29149216967&#34;&gt;OpenRLHF&amp;amp;Verl参数转换指南&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/25556718002&#34;&gt;verl小白解读&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/1937826285264011929&#34;&gt;一文深度全面解析大模型分布式并行策略：DP/TP/PP/CP/EP/SP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/650383289&#34;&gt;深入理解 Megatron-LM（2）原理介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.csdn.net/m0_60388871/article/details/148958601&#34;&gt;DeepSpeed zero1，zero2，zero3和FSDP区别详解&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;inference
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/711378550&#34;&gt;SGLang：LLM推理引擎发展新方向&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/669926191&#34;&gt;图解大模型计算加速系列：FlashAttention V1，从硬件到计算逻辑&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
  </channel>
</rss>
