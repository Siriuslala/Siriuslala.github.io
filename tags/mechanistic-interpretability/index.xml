<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Mechanistic Interpretability on Siriuslala&#39;s Blog!</title>
    <link>https://Siriuslala.github.io/tags/mechanistic-interpretability/</link>
    <description>Recent content in Mechanistic Interpretability on Siriuslala&#39;s Blog!</description>
    <image>
      <title>Siriuslala&#39;s Blog!</title>
      <url>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.147.6</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Jan 2026 21:21:39 +0800</lastBuildDate>
    <atom:link href="https://Siriuslala.github.io/tags/mechanistic-interpretability/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A Collection of My Publications</title>
      <link>https://Siriuslala.github.io/posts/publications/</link>
      <pubDate>Fri, 23 Jan 2026 21:21:39 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/publications/</guid>
      <description>&lt;div class=&#34;legend&#34; style=&#34;font-size: 0.9em; color: var(--secondary); margin-bottom: 20px;&#34;&gt;
    &lt;span&gt;&lt;sup&gt;*&lt;/sup&gt; Equal contribution&lt;/span&gt;
    &lt;span style=&#34;margin-left: 15px;&#34;&gt;&lt;sup&gt;✉&lt;/sup&gt; Corresponding author&lt;/span&gt;
&lt;/div&gt;
&lt;div class=&#34;paper-item&#34;&gt;
    &lt;div class=&#34;paper-img-container&#34;&gt;
        
        &lt;img src=&#34;https://Siriuslala.github.io/publications/reading_images_like_texts.png&#34; alt=&#34;Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models&#34; class=&#34;paper-img&#34;&gt;
        
    &lt;/div&gt;
    &lt;div class=&#34;paper-info&#34;&gt;
        &lt;span class=&#34;conf-badge badge-iclr&#34;&gt;ICLR 2026&lt;/span&gt;
        
        
        &lt;a href=&#34;https://arxiv.org/pdf/2509.19191&#34; 
           class=&#34;paper-title&#34; 
           target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;
           Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models
        &lt;/a&gt;
        
        &lt;div class=&#34;author-list&#34;&gt;
            &lt;span class=&#39;me&#39;&gt;Yueyan Li&lt;/span&gt;&lt;sup&gt;*&lt;/sup&gt;, Chenggong Zhao, Zeyuan Zhang, Caixia Yuan, Xiaojie Wang&lt;sup&gt;✉&lt;/sup&gt;
        &lt;/div&gt;
        
        &lt;div class=&#34;venue&#34;&gt;
            International Conference on Learning Representations (ICLR), 2026
        &lt;/div&gt;
        
        &lt;div class=&#34;btn-group&#34;&gt;
            &lt;a href=&#34;https://arxiv.org/pdf/2509.19191&#34; class=&#34;btn&#34;&gt;PDF&lt;/a&gt;
            &lt;a href=&#34;https://github.com/Siriuslala/vlm_interp&#34; class=&#34;btn&#34;&gt;Code&lt;/a&gt;
            
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;span style=&#39;color:#e53935; font-weight:bold;&#39;&gt;(Oral)&lt;/span&gt; --&gt;
&lt;hr&gt;
&lt;div class=&#34;paper-item&#34;&gt;
    &lt;div class=&#34;paper-img-container&#34;&gt;
        
        &lt;img src=&#34;https://Siriuslala.github.io/publications/circuit-tuning.png&#34; alt=&#34;Sparse Model Diffing via Dynamic Circuits&#34; class=&#34;paper-img&#34;&gt;
        
    &lt;/div&gt;
    &lt;div class=&#34;paper-info&#34;&gt;
        &lt;span class=&#34;conf-badge badge-arxiv&#34;&gt;arxiv&lt;/span&gt;
        
        
        &lt;a href=&#34;https://arxiv.org/pdf/2502.06106&#34; 
           class=&#34;paper-title&#34; 
           target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;
           Sparse Model Diffing via Dynamic Circuits
        &lt;/a&gt;
        
        &lt;div class=&#34;author-list&#34;&gt;
            &lt;span class=&#39;me&#39;&gt;Yueyan Li&lt;/span&gt;&lt;sup&gt;*&lt;/sup&gt;, Wenhao Gap, Caixia Yuan, Xiaojie Wang&lt;sup&gt;✉&lt;/sup&gt;
        &lt;/div&gt;
        
        &lt;div class=&#34;venue&#34;&gt;
            ArXiv, 2026
        &lt;/div&gt;
        
        &lt;div class=&#34;btn-group&#34;&gt;
            &lt;a href=&#34;https://arxiv.org/pdf/2502.06106&#34; class=&#34;btn&#34;&gt;PDF&lt;/a&gt;
            &lt;a href=&#34;https://github.com/Siriuslala/circuit-tuning&#34; class=&#34;btn&#34;&gt;Code&lt;/a&gt;
            
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paper-item&#34;&gt;
    &lt;div class=&#34;paper-img-container&#34;&gt;
        
        &lt;img src=&#34;https://Siriuslala.github.io/publications/autoglm.png&#34; alt=&#34;AutoGLM: Autonomous Foundation Agents for GUIs&#34; class=&#34;paper-img&#34;&gt;
        
    &lt;/div&gt;
    &lt;div class=&#34;paper-info&#34;&gt;
        &lt;span class=&#34;conf-badge badge-techReport&#34;&gt;Technical Report&lt;/span&gt;
        
        
        &lt;a href=&#34;https://arxiv.org/pdf/2411.00820&#34; 
           class=&#34;paper-title&#34; 
           target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;
           AutoGLM: Autonomous Foundation Agents for GUIs
        &lt;/a&gt;
        
        &lt;div class=&#34;author-list&#34;&gt;
            Team AutoGLM
        &lt;/div&gt;
        
        &lt;div class=&#34;venue&#34;&gt;
            Arxiv, 2024
        &lt;/div&gt;
        
        &lt;div class=&#34;btn-group&#34;&gt;
            &lt;a href=&#34;https://arxiv.org/pdf/2411.00820&#34; class=&#34;btn&#34;&gt;PDF&lt;/a&gt;
            &lt;a href=&#34;https://github.com/zai-org/Open-AutoGLM&#34; class=&#34;btn&#34;&gt;Code&lt;/a&gt;
            
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paper-item&#34;&gt;
    &lt;div class=&#34;paper-img-container&#34;&gt;
        
        &lt;img src=&#34;https://Siriuslala.github.io/publications/chatglm-math.png&#34; alt=&#34;ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline&#34; class=&#34;paper-img&#34;&gt;
        
    &lt;/div&gt;
    &lt;div class=&#34;paper-info&#34;&gt;
        &lt;span class=&#34;conf-badge badge-emnlp&#34;&gt;EMNLP 2024&lt;/span&gt;
        
        
        &lt;a href=&#34;https://arxiv.org/pdf/2411.00820&#34; 
           class=&#34;paper-title&#34; 
           target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;
           ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline
        &lt;/a&gt;
        
        &lt;div class=&#34;author-list&#34;&gt;
            Yifan Xu&lt;sup&gt;*&lt;/sup&gt;, Xiao Liu, Xinghan Liu, Zhenyu Hou, &lt;span class=&#39;me&#39;&gt;Yueyan Li&lt;/span&gt;, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Zhao Wenyi, Jie Tang, Yuxiao Dong&lt;sup&gt;✉&lt;/sup&gt;
        &lt;/div&gt;
        
        &lt;div class=&#34;venue&#34;&gt;
            Conference on Empirical Methods in Natural Language Processing (EMNLP), 2024
        &lt;/div&gt;
        
        &lt;div class=&#34;btn-group&#34;&gt;
            &lt;a href=&#34;https://aclanthology.org/2024.findings-emnlp.569.pdf&#34; class=&#34;btn&#34;&gt;PDF&lt;/a&gt;
            &lt;a href=&#34;https://github.com/Siriuslala/ChatGLM-Math&#34; class=&#34;btn&#34;&gt;Code&lt;/a&gt;
            
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paper-item&#34;&gt;
    &lt;div class=&#34;paper-img-container&#34;&gt;
        
        &lt;img src=&#34;https://Siriuslala.github.io/publications/chatglm.png&#34; alt=&#34;ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools&#34; class=&#34;paper-img&#34;&gt;
        
    &lt;/div&gt;
    &lt;div class=&#34;paper-info&#34;&gt;
        &lt;span class=&#34;conf-badge badge-techReport&#34;&gt;Technical Report&lt;/span&gt;
        
        
        &lt;a href=&#34;https://arxiv.org/pdf/2406.12793&#34; 
           class=&#34;paper-title&#34; 
           target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;
           ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
        &lt;/a&gt;
        
        &lt;div class=&#34;author-list&#34;&gt;
            Team GLM
        &lt;/div&gt;
        
        &lt;div class=&#34;venue&#34;&gt;
            Arxiv, 2024
        &lt;/div&gt;
        
        &lt;div class=&#34;btn-group&#34;&gt;
            &lt;a href=&#34;https://arxiv.org/pdf/2406.12793&#34; class=&#34;btn&#34;&gt;PDF&lt;/a&gt;
            &lt;a href=&#34;https://github.com/zai-org/GLM-4&#34; class=&#34;btn&#34;&gt;Code&lt;/a&gt;
            
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;
&lt;div class=&#34;paper-item&#34;&gt;
    &lt;div class=&#34;paper-img-container&#34;&gt;
        
        &lt;img src=&#34;https://Siriuslala.github.io/publications/entrokv.png&#34; alt=&#34;EntroKV: An Entropy-aware Memory Manager for KV cache Compression&#34; class=&#34;paper-img&#34;&gt;
        
    &lt;/div&gt;
    &lt;div class=&#34;paper-info&#34;&gt;
        &lt;span class=&#34;conf-badge badge-arxiv&#34;&gt;arxiv&lt;/span&gt;
        
        
        &lt;a href=&#34;https://arxiv.org/pdf/2502.06106&#34; 
           class=&#34;paper-title&#34; 
           target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;
           EntroKV: An Entropy-aware Memory Manager for KV cache Compression
        &lt;/a&gt;
        
        &lt;div class=&#34;author-list&#34;&gt;
            Wenhao Gao&lt;sup&gt;*&lt;/sup&gt;, Haoran Cao, &lt;span class=&#39;me&#39;&gt;Yueyan Li&lt;/span&gt;, Caixia Yuan, Xiaojie Wang&lt;sup&gt;✉&lt;/sup&gt;
        &lt;/div&gt;
        
        &lt;div class=&#34;venue&#34;&gt;
            ArXiv, 2026
        &lt;/div&gt;
        
        &lt;div class=&#34;btn-group&#34;&gt;
            
            &lt;a href=&#34;https://github.com/Siriuslala/wisekv&#34; class=&#34;btn&#34;&gt;Code&lt;/a&gt;
            
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;hr&gt;</description>
    </item>
    <item>
      <title>Interpretability (&amp; other areas) for Multimodal Models</title>
      <link>https://Siriuslala.github.io/posts/mm_interp/</link>
      <pubDate>Tue, 25 Feb 2025 15:08:53 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/mm_interp/</guid>
      <description>&lt;p&gt;&amp;#x1f4a1;
This post is initially focused on interpretability for multimodal models, while later a lot of papers in other fields are included, just for convenience.&lt;/p&gt;
&lt;h2 id=&#34;methods&#34;&gt;Methods&lt;/h2&gt;
&lt;h3 id=&#34;interpretability-for-mllms&#34;&gt;Interpretability for MLLMs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;survey&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.17516&#34;&gt;A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.01048v1&#34;&gt;Sparks of Explainability Recent Advancements in Explaining Large Vision Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/itsqyh/Awesome-LMMs-Mechanistic-Interpretability?tab=readme-ov-file#-blog&#34;&gt;Awesome LMMs Mechanistic Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;probing&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.17304&#34;&gt;Probing Multimodal Large Language Models for Global and Local Semantic Representations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;representation&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://distill.pub/2020/circuits/zoom-in/?ref=cold-takes&#34;&gt;Zoom in: An introduction to circuits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://distill.pub/2021/multimodal-neurons/&#34;&gt;Multimodal Neurons in Artificial Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.05916&#34;&gt;Interpreting CLIP&amp;rsquo;s Image Representation via Text-Based Decomposition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.04341&#34;&gt;Interpreting the Second-Order Effects of Neurons in CLIP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cnblogs.com/LittleHenry/p/18688886&#34;&gt;CLIP不同层&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023W/CLVL/papers/Schwettmann_Multimodal_Neurons_in_Pretrained_Text-Only_Transformers_ICCVW_2023_paper.pdf&#34;&gt;Multimodal Neurons in Pretrained Text-Only Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2510.24709v1&#34;&gt;Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2024/papers/Kowal_Understanding_Video_Transformers_via_Universal_Concept_Discovery_CVPR_2024_paper.pdf&#34;&gt;Understanding Video Transformers via Universal Concept Discovery&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;circuit&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.04236&#34;&gt;**(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2404.14349&#34;&gt;Automatic Discovery of Visual Circuits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023W/CLVL/papers/Palit_Towards_Vision-Language_Mechanistic_Interpretability_A_Causal_Tracing_Tool_for_BLIP_ICCVW_2023_paper.pdf&#34;&gt;Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SAE&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/iYFuZo9BMvr6GgMs5/case-study-interpreting-manipulating-and-controlling-clip&#34;&gt;Case study: Interpreting, manipulating, and controlling clip with sparse autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/bCtbuWraqYTDtuARg/towards-multimodal-interpretability-learning-sparse-2&#34;&gt;Towards multimodal interpretability: Learning sparse interpretable features in vision transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2407.14499&#34;&gt;Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;visualization&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.22399&#34;&gt;VITAL: More Understandable Feature Visualization through Distribution Alignment and Relevant Information Flow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/398408338&#34;&gt;Visualizer！简化你的Vision Transformer可视化！&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.02957&#34;&gt;(DVT) Denoising Vision Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.23270&#34;&gt;Token Activation Map to Visually Explain Multimodal LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2024W/XAI4CV/papers/Stan_LVLM-Intrepret_An_Interpretability_Tool_for_Large_Vision-Language_Models_CVPRW_2024_paper.pdf&#34;&gt;LVLM-Intrepret: An Interpretability Tool for Large Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2012.09838&#34;&gt;Transformer Interpretability Beyond Attention Visualization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;others&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2410.07149&#34;&gt;**Towards interpreting visual information processing in vision-language models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://clementneo.com/llava_logit_lens/&#34;&gt;demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.alignmentforum.org/posts/kobJymvvcvhbjWFKe/laying-the-foundations-for-vision-and-multimodal-mechanistic&#34;&gt;(dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability &amp;amp; Open Problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2203.14680&#34;&gt;Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2509.25584&#34;&gt;Skip-It? Theoretical Conditions for Layer Skipping in Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;tools&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/compling-wat/vlm-lens&#34;&gt;VLM-Lens&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;information flow&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2510.13251&#34;&gt;Map the Flow: Revealing Hidden Pathways of Information in VideoLLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.18620&#34;&gt;**Cross-modal Information Flow in Multimodal Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.06579&#34;&gt;*From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.17491&#34;&gt;*What&amp;rsquo;s in the Image? A Deep-Dive into the Vision of Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.06646&#34;&gt;The Narrow Gate: Localized Image-Text Communication in Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;&#34;&gt;Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.13108&#34;&gt;Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;analyses on MLLMs&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2509.26625&#34;&gt;Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2509.11986&#34;&gt;Lost in Embeddings: Information Loss in Vision–Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.02199&#34;&gt;Words or Vision: Do Vision-Language Models Have Blind Faith in Text?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.15969&#34;&gt;Forgotten Polygons: Multimodal Large Language Models are Shape-Blind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2309.16588&#34;&gt;Vision Transformers Need Registers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2507.03683&#34;&gt;On the rankability of visual embeddings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;interpretability-for-diffusion-models&#34;&gt;Interpretability for Diffusion Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;survey&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/adagorgun/awesome-generative-explainability&#34;&gt;awesome-generative-explainability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;representation&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;general
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2306.00966&#34;&gt;The hidden language of diffusion models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023/papers/Xiang_Denoising_Diffusion_Autoencoders_are_Unified_Self-supervised_Learners_ICCV_2023_paper.pdf&#34;&gt;Denoising diffusion autoencoders are unified self-supervised learners&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/4bfcebedf7a2967c410b64670f27f904-Paper-Conference.pdf&#34;&gt;Understanding the Latent Space of Diffusion Models through the Lens of Riemannian Geometry&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;localization
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.18832&#34;&gt;Localizing Knowledge in Diffusion Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.09935&#34;&gt;Precise Parameter Localization for Textual Generation in Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2025/papers/Avrahami_Stable_Flow_Vital_Layers_for_Training-Free_Image_Editing_CVPR_2025_paper.pdf&#34;&gt;Stable flow: Vital layers for training-free image editing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2601.02211&#34;&gt;Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/pdf?id=2VuPBAH94k&#34;&gt;submission (Revisiting Block-wise Interactions of MMDiT for Training-free Improved Synthesis)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=2VuPBAH94k&#34;&gt;review&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;motion (for video gen models)
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2024/file/8b2fc235787852ead92da2268cd9e90c-Paper-Conference.pdf&#34;&gt;Video Diffusion Models are Training-free Motion Interpreter and Controller&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.17220&#34;&gt;Emergent Temporal Correspondences from Video Diffusion Transformers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;inference
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2512.08486&#34;&gt;Temporal Concept Dynamics in Diffusion Models via Prompt-Conditioned Interventions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2024/file/63e8bc7bbf1cfea36d1d1b6538aecce5-Paper-Conference.pdf&#34;&gt;Towards Understanding the Working Mechanism of Text-to-Image Diffusion Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;modules&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;positional encoding
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.16153&#34;&gt;FreeFlux: Understanding and Exploiting Layer-Specific Roles in RoPE-Based MMDiT for Versatile Image Editing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.09830&#34;&gt;Exploring Position Encoding in Diffusion U-Net for Training-free High-resolution Image Generation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;attention
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2210.04885&#34;&gt;What the DAAM: Interpreting Stable Diffusion Using Cross Attention&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;circuit&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/pdf?id=Qmw9ne6SOQ&#34;&gt;Localizing and editing knowledge in text-to-image generative models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/pdf?id=fsVBsxjRER&#34;&gt;On Mechanistic Knowledge Localization in Text-to-Image Generative Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2601.06338&#34;&gt;Circuit Mechanisms for Spatial Relation Generation in Diffusion Transformers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SAE&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2504.15473&#34;&gt;Emergence and Evolution of Interpretable Concepts in Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2501.18052&#34;&gt;SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.07050&#34;&gt;Tide: Temporal-aware sparse autoencoders for interpretable diffusion transformers in image generation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;steering vector&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.07891&#34;&gt;Video Unlearning via Low-Rank Refusal Vector&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2504.13763&#34;&gt;Decoding Vision Transformers: the Diffusion Steering Lens&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;learning dynamics&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2505.17638&#34;&gt;Why Diffusion Models Don’t Memorize: The Role of Implicit Dynamical Regularization in Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.03206&#34;&gt;An Analytical Theory of Spectral Bias in the Learning Dynamics of Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;visualization&lt;/strong&gt;
*&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;others&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2025.emnlp-main.304.pdf&#34;&gt;VideoEraser: Concept Erasure in Text-to-Video Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.19852&#34;&gt;Radial Attention: O(nlogn) Sparse Attention with Energy Decay for Long Video Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2511.20798&#34;&gt;Physics Steering: Causal Control of Cross-Domain Concepts in a Physics Foundation Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2025/papers/Bahmani_AC3D_Analyzing_and_Improving_3D_Camera_Control_in_Video_Diffusion_CVPR_2025_paper.pdf&#34;&gt;Ac3d: Analyzing and improving 3d camera control in video diffusion transformers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;application-guided&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/0503f5dce343a1d06d16ba103dd52db1-Paper-Conference.pdf&#34;&gt;Emergent Correspondence from Image Diffusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2601.12761&#34;&gt;Moaw: Unleashing Motion Awareness for Video Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.10210&#34;&gt;Make It Count: Text-to-Image Generation with an Accurate Number of Objects&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2312.09608&#34;&gt;Faster Diffusion: Rethinking the Role of the Encoder for Diffusion Model Inference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;other-fields-of-mllms&#34;&gt;Other fields of MLLMs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;visual pretraining&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Possible Research Areas in Mechanistic Interpretability</title>
      <link>https://Siriuslala.github.io/posts/mech_interp_research/</link>
      <pubDate>Fri, 06 Sep 2024 22:52:16 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/mech_interp_research/</guid>
      <description>&lt;p&gt;&amp;#x1f4a1;
This post is mainly focused on text models. For multi-modal models, please refer to &lt;a href=&#34;https://Siriuslala.github.io/posts/mm_interp/&#34;&gt;this post&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-purpose-i-write-this-blog&#34;&gt;The Purpose I Write This Blog&lt;/h2&gt;
&lt;p&gt;   To get started in mech interp research, we need to have a macro understanding of this area. So I write this blog as a summarization of this field to help you and me choose a research topic.&lt;/p&gt;
&lt;h2 id=&#34;circuit-discovery&#34;&gt;Circuit Discovery&lt;/h2&gt;
&lt;h3 id=&#34;methods&#34;&gt;Methods&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;basic&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;activation patching (causal mediation/interchange interventions&amp;hellip;)&lt;/li&gt;
&lt;li&gt;path patching&lt;/li&gt;
&lt;li&gt;scaling techinques: attribution patching&lt;/li&gt;
&lt;li&gt;DAS (distributed alignment search)   directional activation patching?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;telescope-resources&#34;&gt;&amp;#x1f52d; resources&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;inspirition
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1905.09418&#34;&gt;Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;what is circuit discovery?
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2309.16042&#34;&gt;Towards Best Practices of Activation Patching in Language Models: Metrics and Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2404.15255&#34;&gt;How to use and interpret activation patching&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;representative work
&lt;ul&gt;
&lt;li&gt;activation patching
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper/2020/file/92650b2e92217715fe312e6fa7b90d82-Paper.pdf&#34;&gt;Investigating gender bias in language models using causal mediation analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2202.05262&#34;&gt;(ROME) Locating and Editing Factual Associations in GPT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing&#34;&gt;Causal Scrubbing: a method for rigorously testing interpretability hypotheses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.neelnanda.io/mechanistic-interpretability/attribution-patching&#34;&gt;(AtP) Attribution patching: Activation patching at industrial scale&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2403.00745&#34;&gt;AtP*: An efficient and scalable method for localizing llm behaviour to components&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;path patching
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.14997&#34;&gt;(ACDC) Towards Automated Circuit Discovery for Mechanistic Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.10348&#34;&gt;(EAP) Attribution Patching Outperforms Automated Circuit Discovery&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2403.17806&#34;&gt;(EAP-IG) Have faith in faithfulness: Going beyond circuit overlap when finding model mechanisms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2304.05969&#34;&gt;Localizing Model Behavior with Path Patching&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;distributed alignment search
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;&#34;&gt;(DAS) Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2023/file/f6a8b109d4d4fd64c75e94aaf85d9697-Paper-Conference.pdf&#34;&gt;Interpretability at Scale: Identifying Causal Mechanisms in Alpaca&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;new&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Using SAE
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2403.19647&#34;&gt;Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2405.13868&#34;&gt;Automatically Identifying Local and Global Circuits with Linear Computation Graphs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Contextual Decomposition
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.00886&#34;&gt;Mechanistic Interpretation through Contextual Decomposition in Transformers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Edge Pruning ?
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.16778&#34;&gt;Finding Transformer Circuits with Edge Pruning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2407.03779&#34;&gt;Functional Faithfulness in the Wild: Circuit Discovery with Differentiable Computation Graph Pruning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;&#34;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;attribution graph&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;see &lt;strong&gt;Applications&lt;/strong&gt; in the &lt;strong&gt;Dictionary Learning&lt;/strong&gt; section&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation&lt;/h3&gt;
&lt;p&gt;lack of ground truth&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring Emotional Features in GPT2-Small</title>
      <link>https://Siriuslala.github.io/posts/happy_feats/</link>
      <pubDate>Thu, 29 Aug 2024 15:51:59 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/happy_feats/</guid>
      <description>&lt;p&gt;&amp;#x1f3b6;Code in this post can be found at &lt;a href=&#34;https://github.com/Siriuslala/saeExploration/blob/main/multilingual_study.ipynb&#34;&gt;the jupyter notebook in my &amp;ldquo;saeExploration&amp;rdquo; repo&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;find-features-that-reflect-positive-emotions&#34;&gt;Find features that reflect positive emotions&lt;/h2&gt;
&lt;p&gt;To find the features related to a specific emotion, I write five sentences containing the key words for each emotion. For example, for happy emotions I have:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-1&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-1&#34;&gt;1&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-2&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-2&#34;&gt;2&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-3&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-3&#34;&gt;3&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-4&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-4&#34;&gt;4&lt;/a&gt;
&lt;/span&gt;&lt;span class=&#34;lnt&#34; id=&#34;hl-0-5&#34;&gt;&lt;a class=&#34;lnlinks&#34; href=&#34;#hl-0-5&#34;&gt;5&lt;/a&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;prompt_happy&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;ll be on a vacation tomorrow and I&amp;#39;m so happy.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;My mombrings home a new puppy and I&amp;#39;m so happy.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;m so glad I got the job I wanted.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I feel so happy when I&amp;#39;m with my friends.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;I&amp;#39;m so happy I got the promotion I wanted.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;I choose to look for features that reflect happiness and sadness. Apart from that, I also wonder if the feature that reflects excitedness has something to do with the one that reflects happiness (they are alike from the semantic level at least.)&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Brief Introduction to Mechanistic Interpretability Research</title>
      <link>https://Siriuslala.github.io/posts/mech_interp_resource/</link>
      <pubDate>Wed, 28 Aug 2024 13:12:25 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/mech_interp_resource/</guid>
      <description>&lt;p&gt;&amp;#x26a0;&amp;#xfe0f; &lt;font color=&#34;red&#34;&gt;&lt;em&gt;&lt;strong&gt;Warnings&lt;/strong&gt;&lt;/em&gt;&lt;/font&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;This post was written when I first delved into this area, and it hasn&amp;rsquo;t been updated for a long time. Thus there might be a lot of errors.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- * *Now I&#39;ve changed my attitude to this area. The area is not well-defined, and most of the research in this area is of low quality and is not appealing to me. Besides, I think the study of interpretability should be applied to pratical use, though we can also study it for fun.* --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;I&amp;rsquo;m still interested in interpretability and its applications. I&amp;rsquo;ll write something new and interesting later ~&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;#x1f4a1;
This post is accompanied with &lt;a href=&#34;https://Siriuslala.github.io/posts/mech_interp_research/&#34;&gt;another post&lt;/a&gt;, which contains specific content in this area.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
