<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Multimodal on Siriuslala&#39;s Blog!</title>
    <link>https://Siriuslala.github.io/tags/multimodal/</link>
    <description>Recent content in Multimodal on Siriuslala&#39;s Blog!</description>
    <image>
      <title>Siriuslala&#39;s Blog!</title>
      <url>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.147.6</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Feb 2025 15:08:53 +0800</lastBuildDate>
    <atom:link href="https://Siriuslala.github.io/tags/multimodal/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Interpretability for Multimodal Models</title>
      <link>https://Siriuslala.github.io/posts/mm_interp/</link>
      <pubDate>Tue, 25 Feb 2025 15:08:53 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/mm_interp/</guid>
      <description>&lt;p&gt;&amp;#x1f4a1;
This post is initially focused on interpretability for multimodal models, while later a lot of papers in other fields are included, just for convenience.&lt;/p&gt;
&lt;h2 id=&#34;resource&#34;&gt;Resource&lt;/h2&gt;
&lt;h3 id=&#34;interpretability-for-mllms&#34;&gt;Interpretability for MLLMs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;survey&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.17516&#34;&gt;A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.01048v1&#34;&gt;Sparks of Explainability Recent Advancements in Explaining Large Vision Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/itsqyh/Awesome-LMMs-Mechanistic-Interpretability?tab=readme-ov-file#-blog&#34;&gt;Awesome LMMs Mechanistic Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;probing&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.17304&#34;&gt;Probing Multimodal Large Language Models for Global and Local Semantic Representations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CLIP&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.05916&#34;&gt;Interpreting CLIP&amp;rsquo;s Image Representation via Text-Based Decomposition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.04341&#34;&gt;Interpreting the Second-Order Effects of Neurons in CLIP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cnblogs.com/LittleHenry/p/18688886&#34;&gt;CLIP不同层&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;information flow&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.17491&#34;&gt;*What&amp;rsquo;s in the Image? A Deep-Dive into the Vision of Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.06646&#34;&gt;The Narrow Gate: Localized Image-Text Communication in Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;&#34;&gt;Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.13108&#34;&gt;Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.18620&#34;&gt;**Cross-modal Information Flow in Multimodal Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.06579&#34;&gt;*From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;others&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2410.07149&#34;&gt;**Towards interpreting visual information processing in vision-language models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://clementneo.com/llava_logit_lens/&#34;&gt;demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.04236&#34;&gt;**(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.alignmentforum.org/posts/kobJymvvcvhbjWFKe/laying-the-foundations-for-vision-and-multimodal-mechanistic&#34;&gt;(dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability &amp;amp; Open Problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2203.14680&#34;&gt;Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;visualization
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2012.09838&#34;&gt;Transformer Interpretability Beyond Attention Visualization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;general&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1902.09506&#34;&gt;(GQA) GQA:ANewDataset for Real-World Visual Reasoning and Compositional Question Answering&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cs.stanford.edu/people/dorarad/gqa/index.html&#34;&gt;https://cs.stanford.edu/people/dorarad/gqa/index.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;spatial&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2210.01936&#34;&gt;(ARO) When and why vision-language models behave like bags-of-words, and what to do about it?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aclanthology.org/2023.emnlp-main.568.pdf&#34;&gt;(Whatsup) What’s “up” with vision-language models? Investigating their struggle with spatial reasoning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/amitakamath/whatsup_vlms&#34;&gt;https://github.com/amitakamath/whatsup_vlms&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00566/116470&#34;&gt;(VSR) Visual Spatial Reasoning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/cambridgeltl/visual-spatial-reasoning&#34;&gt;https://github.com/cambridgeltl/visual-spatial-reasoning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;other-fields-of-mllms&#34;&gt;Other fields of MLLMs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;spatial&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
