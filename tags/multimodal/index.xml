<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Multimodal on Siriuslala&#39;s Blog!</title>
    <link>https://Siriuslala.github.io/tags/multimodal/</link>
    <description>Recent content in Multimodal on Siriuslala&#39;s Blog!</description>
    <image>
      <title>Siriuslala&#39;s Blog!</title>
      <url>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://Siriuslala.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.147.6</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Feb 2025 15:08:53 +0800</lastBuildDate>
    <atom:link href="https://Siriuslala.github.io/tags/multimodal/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Interpretability (&amp; other areas) for Multimodal Models</title>
      <link>https://Siriuslala.github.io/posts/mm_interp/</link>
      <pubDate>Tue, 25 Feb 2025 15:08:53 +0800</pubDate>
      <guid>https://Siriuslala.github.io/posts/mm_interp/</guid>
      <description>&lt;p&gt;&amp;#x1f4a1;
This post is initially focused on interpretability for multimodal models, while later a lot of papers in other fields are included, just for convenience.&lt;/p&gt;
&lt;h2 id=&#34;resource&#34;&gt;Resource&lt;/h2&gt;
&lt;h3 id=&#34;interpretability-for-mllms&#34;&gt;Interpretability for MLLMs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;survey&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.17516&#34;&gt;A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.01048v1&#34;&gt;Sparks of Explainability Recent Advancements in Explaining Large Vision Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/itsqyh/Awesome-LMMs-Mechanistic-Interpretability?tab=readme-ov-file#-blog&#34;&gt;Awesome LMMs Mechanistic Interpretability&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;probing&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2402.17304&#34;&gt;Probing Multimodal Large Language Models for Global and Local Semantic Representations&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;representation&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://distill.pub/2020/circuits/zoom-in/?ref=cold-takes&#34;&gt;Zoom in: An introduction to circuits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://distill.pub/2021/multimodal-neurons/&#34;&gt;Multimodal Neurons in Artificial Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2310.05916&#34;&gt;Interpreting CLIP&amp;rsquo;s Image Representation via Text-Based Decomposition&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.04341&#34;&gt;Interpreting the Second-Order Effects of Neurons in CLIP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cnblogs.com/LittleHenry/p/18688886&#34;&gt;CLIP不同层&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023W/CLVL/papers/Schwettmann_Multimodal_Neurons_in_Pretrained_Text-Only_Transformers_ICCVW_2023_paper.pdf&#34;&gt;Multimodal Neurons in Pretrained Text-Only Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2510.24709v1&#34;&gt;Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;circuit&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.04236&#34;&gt;**(causal tracing) Understanding Information Storage and Transfer in Multi-modal Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2404.14349&#34;&gt;Automatic Discovery of Visual Circuits&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/ICCV2023W/CLVL/papers/Palit_Towards_Vision-Language_Mechanistic_Interpretability_A_Causal_Tracing_Tool_for_BLIP_ICCVW_2023_paper.pdf&#34;&gt;Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SAE&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/iYFuZo9BMvr6GgMs5/case-study-interpreting-manipulating-and-controlling-clip&#34;&gt;Case study: Interpreting, manipulating, and controlling clip with sparse autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/bCtbuWraqYTDtuARg/towards-multimodal-interpretability-learning-sparse-2&#34;&gt;Towards multimodal interpretability: Learning sparse interpretable features in vision transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2407.14499&#34;&gt;Discover-then-Name: Task-Agnostic Concept Bottlenecks via Automated Concept Discovery&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;visualization&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/398408338&#34;&gt;Visualizer！简化你的Vision Transformer可视化！&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2401.02957&#34;&gt;(DVT) Denoising Vision Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2506.23270&#34;&gt;Token Activation Map to Visually Explain Multimodal LLMs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content/CVPR2024W/XAI4CV/papers/Stan_LVLM-Intrepret_An_Interpretability_Tool_for_Large_Vision-Language_Models_CVPRW_2024_paper.pdf&#34;&gt;LVLM-Intrepret: An Interpretability Tool for Large Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2012.09838&#34;&gt;Transformer Interpretability Beyond Attention Visualization&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;others&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2410.07149&#34;&gt;**Towards interpreting visual information processing in vision-language models&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://clementneo.com/llava_logit_lens/&#34;&gt;demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.alignmentforum.org/posts/kobJymvvcvhbjWFKe/laying-the-foundations-for-vision-and-multimodal-mechanistic&#34;&gt;(dogit lens) Laying the Foundations for Vision and Multimodal Mechanistic Interpretability &amp;amp; Open Problems&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2203.14680&#34;&gt;Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2509.25584&#34;&gt;Skip-It? Theoretical Conditions for Layer Skipping in Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;tools&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/compling-wat/vlm-lens&#34;&gt;VLM-Lens&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;information flow&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.18620&#34;&gt;**Cross-modal Information Flow in Multimodal Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2406.06579&#34;&gt;*From Redundancy to Relevance: Information Flow in LVLMs Across Reasoning Tasks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2411.17491&#34;&gt;*What&amp;rsquo;s in the Image? A Deep-Dive into the Vision of Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2412.06646&#34;&gt;The Narrow Gate: Localized Image-Text Communication in Vision-Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;&#34;&gt;Performance Gap in Entity Knowledge Extraction Across Modalities in Vision Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.13108&#34;&gt;Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;analyses on MLLMs&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2509.26625&#34;&gt;Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2509.11986&#34;&gt;Lost in Embeddings: Information Loss in Vision–Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2503.02199&#34;&gt;Words or Vision: Do Vision-Language Models Have Blind Faith in Text?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2502.15969&#34;&gt;Forgotten Polygons: Multimodal Large Language Models are Shape-Blind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2309.16588&#34;&gt;Vision Transformers Need Registers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2507.03683&#34;&gt;On the rankability of visual embeddings&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;other-fields-of-mllms&#34;&gt;Other fields of MLLMs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;visual pretraining&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
